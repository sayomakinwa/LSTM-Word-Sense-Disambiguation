{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"colab":{"name":"senses_only_model_B.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"D723QuTMua-L","colab_type":"text"},"source":["## Mount Drive and Imports"]},{"cell_type":"code","metadata":{"id":"NKtsqriiKxYd","colab_type":"code","outputId":"6f8771a8-618f-4edb-c6ee-3eef40bc5ccb","executionInfo":{"status":"ok","timestamp":1568233640099,"user_tz":-120,"elapsed":588,"user":{"displayName":"Sayo Makinwa","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB01ONwXql1XhFq6AK_2PjHkUfJA3VDpx1AVsLimg=s64","userId":"02832581640686072550"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nRTrAorEKyu5","colab_type":"code","outputId":"440cdf35-c311-4e4d-aebc-e54077d1cea5","executionInfo":{"status":"ok","timestamp":1568233642587,"user_tz":-120,"elapsed":798,"user":{"displayName":"Sayo Makinwa","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB01ONwXql1XhFq6AK_2PjHkUfJA3VDpx1AVsLimg=s64","userId":"02832581640686072550"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%cd /content/drive/My Drive/Colab Notebooks/nlp_hw3/code"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks/nlp_hw3/code\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OxdkmYCxEybs","colab_type":"code","colab":{}},"source":["import os\n","import numpy as np\n","from typing import Tuple, List, Dict\n","\n","import tensorflow as tf\n","import tensorflow.keras as K\n","from tensorflow.keras.layers import *\n","from tensorflow.keras.models import load_model\n","\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","from tensorflow.keras.callbacks import TensorBoard\n","from tensorflow.python.eager import context\n","import json\n","import pandas as pd\n","\n","# from numpy import array\n","# from numpy import argmax\n","# from sklearn.preprocessing import LabelEncoder\n","# from sklearn.preprocessing import OneHotEncoder\n","\n","import my_utils as utils\n","import corpora"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PeQfr7YJuuy2","colab_type":"text"},"source":["##Preprocessing"]},{"cell_type":"code","metadata":{"id":"042xGwFcEyb1","colab_type":"code","colab":{}},"source":["def load_train_dataset(input_path: str, y_path: str) -> Tuple[List[str], List[str]]:\n","    \"\"\"\n","    :param input_path; Path to the input dataset\n","    :param label_path; Path to the file containing the corresponding labels for the input dataset\n","    :return sentences; List of sentences in input_file\n","    :return labels; List of corresponding word segment codes in label_path. Same len as sentences\n","    \"\"\"\n","    sentences = []\n","    k = 0\n","    with open(input_path, \"r\", encoding=\"utf-8-sig\") as file:\n","        for line in file:\n","            k += 1\n","            sentences.append(line.strip())\n","#             if (k >= 4000):\n","#                 break\n","\n","    y = []\n","    k = 0\n","    with open(y_path, \"r\", encoding=\"utf-8-sig\") as file:\n","        for line in file:\n","            k += 1\n","            y.append(line.strip())\n","#             if (k >= 4000):\n","#                 break\n","\n","    return sentences, y\n","\n","\n","def make_X_vocab(sentences: List[str]) -> Dict[str, int]:\n","    '''\n","    :param sentences; List of input sentences from the dataset\n","    :return unigrams_vocab; Dictionary from unigram to int\n","    :return bigrams_vocab; Dictionary from bigram to int\n","    '''\n","    vocab = {\"UNK\": 0}\n","\n","    for sentence in sentences:\n","        for word in sentence.split():\n","            if word not in vocab:\n","                vocab[word] = len(vocab)\n","    \n","    return vocab\n","\n","\n","def make_Y_vocab(y: List[str]) -> Dict[str, int]:\n","    \"\"\"\n","    :param labels; List of label codes\n","    :return labels_vocab; Dictionary from label code to int \n","    \"\"\"\n","    y_vocab = {\"UNK\": 0, \"OTHERS\": 1}\n","    \n","    for y_line in y:\n","        for y_word in y_line.split():\n","            if y_word not in y_vocab and \"bn:\" in y_word:\n","                y_vocab[y_word] = len(y_vocab)\n","                \n","    return y_vocab\n","\n","# def make_Y_vocab(y: List[str]) -> Dict[str, int], str]:\n","#     \"\"\"\n","#     :param labels; List of label codes\n","#     :return labels_vocab; Dictionary from label code to int \n","#     \"\"\"\n","#     #labels_vocab = {\"UNK\": 0}\n","#     y_vocab = {\"UNK\": 0, \"OTHERS\": 1}\n","#     y_vocab_freq = {\"UNK\": 0}\n","    \n","#     for y_line in y:\n","#         for y_word in y_line.split():\n","#             if y_word not in y_vocab and \"bn:\" in y_word:\n","#                 y_vocab[y_word] = len(y_vocab)\n","#                 y_vocab_freq[y_word] = 1\n","#             elif \"bn:\" in y_word:\n","#                 y_vocab_freq[y_word] += 1\n","                \n","#     if y_test:\n","#         for y_line in y_test:\n","#             for y_word in y_line.split():\n","#                 if y_word not in y_vocab and \"bn:\" in y_word:\n","#                     y_vocab[y_word] = len(y_vocab)\n","#                     y_vocab_freq[y_word] = 1\n","#                 elif \"bn:\" in y_word:\n","#                     y_vocab_freq[y_word] += 1\n","    \n","    \n","#     mfs = \"UNK\"\n","#     mfs_freq = 0\n","#     for k, v in y_vocab_freq.items():\n","#         if \"bn:\" in k and v > mfs_freq:\n","#             mfs = k\n","#             mfs_freq = v\n","\n","#     print(mfs, mfs_freq)\n","#     y_vocab[\"__MFS__\"] = mfs\n","    \n","#     return y_vocab, mfs\n","\n","def make_Y(output: List[str], output_vocab: Dict[str, int]) -> np.ndarray:\n","    \"\"\"\n","    :param labels; List of word segment codes, line by line\n","    :param labels_vocab; Label codes vocab\n","    :return y; Vector of label code indices\n","    \"\"\"\n","    y = []\n","    for output_line in output:\n","        y_temp = []\n","        for single_output in output_line.split():\n","            if single_output in output_vocab:\n","                y_temp.append( output_vocab[single_output])\n","            else:\n","                y_temp.append( output_vocab[\"OTHERS\"])\n","        y.append(np.array(y_temp))\n","    \n","    return np.array(y)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JqBzkWHbvtMg","colab_type":"code","colab":{}},"source":["train_data_path = '../data/Training_Corpora/semcor'\n","test_data_path = '../data/Evaluation_Datasets/senseval3'\n","\n","#Parse training data\n","corpora_xml_path = train_data_path + '/semcor.data.xml'\n","gold_mapping_path = train_data_path + '/semcor.gold.key.txt'\n","resources_path = '../resources/'\n","\n","corpora.extract_training_data(corpora_xml_path, gold_mapping_path, resources_path, train_data_path)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"M-jZUtDDvtDo","colab_type":"code","colab":{}},"source":["#Parse validation data\n","corpora_xml_path = test_data_path + '/senseval3.data.xml'\n","gold_mapping_path = test_data_path + '/senseval3.gold.key.txt'\n","resources_path = '../resources/'\n","\n","corpora.extract_training_data(corpora_xml_path, gold_mapping_path, resources_path, test_data_path)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yVV_9dxlEyb4","colab_type":"code","colab":{}},"source":["sentences, y = load_train_dataset(train_data_path+\"/trainX.txt\", train_data_path+\"/trainy.txt\")\n","test_sentences, test_y = load_train_dataset(test_data_path+\"/trainX.txt\", test_data_path+\"/trainy.txt\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-EJ1fHSzEyb7","colab_type":"code","outputId":"57558c9b-cf1b-4274-8380-a1af3e44fbcd","executionInfo":{"status":"ok","timestamp":1568222213670,"user_tz":-120,"elapsed":1364,"user":{"displayName":"Sayo Makinwa","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB01ONwXql1XhFq6AK_2PjHkUfJA3VDpx1AVsLimg=s64","userId":"02832581640686072550"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["print(len(sentences))\n","print(sentences[0])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["37168\n","How long has it been since you reviewed the objectives of your benefit and service program\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"t1fD1tgDEycC","colab_type":"code","outputId":"65cc2bef-6262-48be-8a0a-85ea21c9f43c","executionInfo":{"status":"ok","timestamp":1568222216826,"user_tz":-120,"elapsed":754,"user":{"displayName":"Sayo Makinwa","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB01ONwXql1XhFq6AK_2PjHkUfJA3VDpx1AVsLimg=s64","userId":"02832581640686072550"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["print(len(y))\n","print(y[0])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["37168\n","how bn:00106124a have it bn:00083181v since you bn:00092618v the bn:00002179n of you bn:00009904n and bn:00070654n bn:00064646n\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HgCzdmbLEycF","colab_type":"code","outputId":"e3734733-477d-43de-abbf-952f9bd53ddf","executionInfo":{"status":"ok","timestamp":1568222218844,"user_tz":-120,"elapsed":763,"user":{"displayName":"Sayo Makinwa","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB01ONwXql1XhFq6AK_2PjHkUfJA3VDpx1AVsLimg=s64","userId":"02832581640686072550"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["vocab = make_X_vocab(sentences + test_sentences)\n","print(len(vocab))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["48989\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LPRR9TA7EycH","colab_type":"code","outputId":"6236c87a-6c4c-4a9f-c90c-7432acb531b3","executionInfo":{"status":"ok","timestamp":1568222219808,"user_tz":-120,"elapsed":823,"user":{"displayName":"Sayo Makinwa","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB01ONwXql1XhFq6AK_2PjHkUfJA3VDpx1AVsLimg=s64","userId":"02832581640686072550"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["y_vocab = make_Y_vocab(y + test_y)\n","print(len(y_vocab))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["26034\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iD6TeqJREycR","colab_type":"code","colab":{}},"source":["X = utils.make_X(sentences, vocab)\n","y_array = make_Y(y, y_vocab)\n","\n","X_test = utils.make_X(test_sentences, vocab)\n","y_test_array = make_Y(test_y, y_vocab)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DF5m68nmEycT","colab_type":"code","outputId":"51417a46-3d89-4894-c53d-24d93a69aa9e","executionInfo":{"status":"ok","timestamp":1568222252959,"user_tz":-120,"elapsed":1341,"user":{"displayName":"Sayo Makinwa","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB01ONwXql1XhFq6AK_2PjHkUfJA3VDpx1AVsLimg=s64","userId":"02832581640686072550"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["print(X.shape)\n","print(y_array.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(37168,)\n","(37168,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"j7fg-usHEycZ","colab_type":"code","colab":{}},"source":["train_x = pad_sequences(X, truncating='pre', padding='post', maxlen=30)\n","train_y = pad_sequences(y_array, truncating='pre', padding='post', maxlen=30)\n","\n","dev_x = pad_sequences(X_test, truncating='pre', padding='post', maxlen=30)\n","dev_y = pad_sequences(y_test_array, truncating='pre', padding='post', maxlen=30)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NmXFcuk4Eycc","colab_type":"code","outputId":"df79544d-0f09-4ae4-8066-6f3f15f8f45f","executionInfo":{"status":"ok","timestamp":1568222255745,"user_tz":-120,"elapsed":1110,"user":{"displayName":"Sayo Makinwa","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB01ONwXql1XhFq6AK_2PjHkUfJA3VDpx1AVsLimg=s64","userId":"02832581640686072550"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["print(train_x.shape)\n","print(train_y.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(37168, 30)\n","(37168, 30)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YIojgrqREycg","colab_type":"code","outputId":"6a8cc70f-7eea-46dd-adbc-eb0b47ea08c6","executionInfo":{"status":"ok","timestamp":1568222257213,"user_tz":-120,"elapsed":963,"user":{"displayName":"Sayo Makinwa","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB01ONwXql1XhFq6AK_2PjHkUfJA3VDpx1AVsLimg=s64","userId":"02832581640686072550"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["train_y = train_y.reshape((*train_y.shape, 1))\n","dev_y = dev_y.reshape((*dev_y.shape, 1))\n","print(train_y.shape)\n","print(dev_y.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(37168, 30, 1)\n","(352, 30, 1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ABjb_KPbEyc1","colab_type":"code","outputId":"f5aaa232-6a5b-4037-c428-a5789bab532e","executionInfo":{"status":"ok","timestamp":1568222258836,"user_tz":-120,"elapsed":572,"user":{"displayName":"Sayo Makinwa","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB01ONwXql1XhFq6AK_2PjHkUfJA3VDpx1AVsLimg=s64","userId":"02832581640686072550"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["print(train_x.shape)\n","print(train_y.shape)\n","print(dev_x.shape)\n","print(dev_y.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(37168, 30)\n","(37168, 30, 1)\n","(352, 30)\n","(352, 30, 1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5GMa61tiwcYX","colab_type":"text"},"source":["##Building the model"]},{"cell_type":"code","metadata":{"id":"gd-bBhRDEyc-","colab_type":"code","colab":{}},"source":["vocab_size = len(vocab)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9uF52K_8EydA","colab_type":"code","colab":{}},"source":["#This class helps with logging\n","class TrainValTensorBoard(TensorBoard):\n","    def __init__(self, log_dir='./logs', **kwargs):\n","        self.val_log_dir = os.path.join(log_dir, 'model_B/validation')\n","        training_log_dir = os.path.join(log_dir, 'model_B/training')\n","        super(TrainValTensorBoard, self).__init__(training_log_dir, **kwargs)\n","\n","    def set_model(self, model):\n","        if context.executing_eagerly():\n","            self.val_writer = tf.contrib.summary.create_file_writer(self.val_log_dir)\n","        else:\n","            self.val_writer = tf.summary.FileWriter(self.val_log_dir)\n","        super(TrainValTensorBoard, self).set_model(model)\n","\n","    def _write_custom_summaries(self, step, logs=None):\n","        logs = logs or {}\n","        val_logs = {k.replace('val_', ''): v for k, v in logs.items() if 'val_' in k}\n","        if context.executing_eagerly():\n","            with self.val_writer.as_default(), tf.contrib.summary.always_record_summaries():\n","                for name, value in val_logs.items():\n","                    tf.contrib.summary.scalar(name, value.item(), step=step)\n","        else:\n","            for name, value in val_logs.items():\n","                summary = tf.Summary()\n","                summary_value = summary.value.add()\n","                summary_value.simple_value = value.item()\n","                summary_value.tag = name\n","                self.val_writer.add_summary(summary, step)\n","        self.val_writer.flush()\n","\n","        logs = {k: v for k, v in logs.items() if not 'val_' in k}\n","        super(TrainValTensorBoard, self)._write_custom_summaries(step, logs)\n","\n","    def on_train_end(self, logs=None):\n","        super(TrainValTensorBoard, self).on_train_end(logs)\n","        self.val_writer.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fXhtH-HhEydE","colab_type":"code","colab":{}},"source":["#Please take note that most of this part was extracted from class exercises, with some additions\n","\n","def create_keras_model(vocab_size, y_size, embedding_size=128, hidden_size=512):\n","    print(\"Creating KERAS model\")\n","    \n","    model = K.models.Sequential()\n","    model.add(Embedding(vocab_size, embedding_size, mask_zero=True))\n","    \n","    model.add(Bidirectional(LSTM(hidden_size, dropout=0.3, recurrent_dropout=0.3, return_sequences=True), merge_mode='concat'))\n","    model.add(Bidirectional(LSTM(hidden_size, dropout=0.3, recurrent_dropout=0.3, return_sequences=True), merge_mode='concat'))\n","    \n","    model.add(TimeDistributed(Dense(y_size, activation='softmax')))\n","    optimizer = K.optimizers.Adam()\n","    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['acc'])\n","\n","    return model\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PMqneG5WEydI","colab_type":"code","colab":{}},"source":["resource_path = \"../resources/\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pFd4-nloEydL","colab_type":"code","outputId":"45f8589b-ba2a-4426-e795-c3c5b6370ca2","executionInfo":{"status":"ok","timestamp":1568232382883,"user_tz":-120,"elapsed":8335406,"user":{"displayName":"Sayo Makinwa","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB01ONwXql1XhFq6AK_2PjHkUfJA3VDpx1AVsLimg=s64","userId":"02832581640686072550"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["batch_size = 64\n","epochs = 20\n","model_name = resource_path+\"model.hdf5\"\n","\n","#checks if the FINAL model was saved and loads it instead of creating a new one\n","if os.path.exists(model_name):\n","    model = load_model(model_name)\n","    print(\"Using a pre-saved model\")\n","    model.summary()\n","    \n","else:\n","    model = create_keras_model(vocab_size, len(y_vocab))\n","    print(\"Training a new model\")\n","    model.summary()\n","    \n","    filepath = resource_path+\"models/model_B.hdf5\"\n","    checkpoint = K.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n","    #callbacks_list = [checkpoint]\n","    tb_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\", histogram_freq=0, write_graph=False, write_images=True)\n","    callbacks_list = [TrainValTensorBoard(write_graph=False), checkpoint]\n","    \n","    print(\"\\nStarting training...\")\n","    model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size,\n","              shuffle=True, validation_data=(dev_x, dev_y), callbacks=callbacks_list) \n","    print(\"Training complete.\\n\")\n","    \n","    #Save the FINAL model for later reuse\n","    model.save(model_name)\n","    print(\"Trained model saved for later use\")\n","\n","    print(\"\\nEvaluating test...\")\n","    loss_acc = model.evaluate(dev_x, dev_y, verbose=0)\n","    print(\"Test data: loss = %0.6f  accuracy = %0.2f%% \" % (loss_acc[0], loss_acc[1]*100))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Creating KERAS model\n","Training a new model\n","Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding (Embedding)        (None, None, 128)         6270592   \n","_________________________________________________________________\n","bidirectional (Bidirectional (None, None, 1024)        2625536   \n","_________________________________________________________________\n","bidirectional_1 (Bidirection (None, None, 1024)        6295552   \n","_________________________________________________________________\n","time_distributed (TimeDistri (None, None, 26034)       26684850  \n","=================================================================\n","Total params: 41,876,530\n","Trainable params: 41,876,530\n","Non-trainable params: 0\n","_________________________________________________________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n","\n","Starting training...\n","Train on 37168 samples, validate on 352 samples\n","Epoch 1/20\n","37168/37168 [==============================] - 504s 14ms/sample - loss: 2.0322 - acc: 0.6738 - val_loss: 1.6974 - val_acc: 0.6231\n","Epoch 2/20\n","37168/37168 [==============================] - 500s 13ms/sample - loss: 1.6553 - acc: 0.6795 - val_loss: 1.5920 - val_acc: 0.5907\n","Epoch 3/20\n","37168/37168 [==============================] - 501s 13ms/sample - loss: 1.4002 - acc: 0.6916 - val_loss: 1.4844 - val_acc: 0.5872\n","Epoch 4/20\n","37168/37168 [==============================] - 501s 13ms/sample - loss: 1.2058 - acc: 0.7034 - val_loss: 1.4453 - val_acc: 0.5762\n","Epoch 5/20\n","37168/37168 [==============================] - 501s 13ms/sample - loss: 1.0493 - acc: 0.7192 - val_loss: 1.3764 - val_acc: 0.5925\n","Epoch 6/20\n","37168/37168 [==============================] - 500s 13ms/sample - loss: 0.8977 - acc: 0.7381 - val_loss: 1.3320 - val_acc: 0.6015\n","Epoch 7/20\n","37168/37168 [==============================] - 503s 14ms/sample - loss: 0.7662 - acc: 0.7568 - val_loss: 1.2900 - val_acc: 0.6044\n","Epoch 8/20\n","37168/37168 [==============================] - 505s 14ms/sample - loss: 0.6470 - acc: 0.7781 - val_loss: 1.2326 - val_acc: 0.6136\n","Epoch 9/20\n","37168/37168 [==============================] - 505s 14ms/sample - loss: 0.5389 - acc: 0.8014 - val_loss: 1.2161 - val_acc: 0.6143\n","Epoch 10/20\n","37168/37168 [==============================] - 504s 14ms/sample - loss: 0.4464 - acc: 0.8255 - val_loss: 1.2060 - val_acc: 0.6162\n","Epoch 11/20\n","37168/37168 [==============================] - 501s 13ms/sample - loss: 0.3676 - acc: 0.8515 - val_loss: 1.2006 - val_acc: 0.6193\n","Epoch 12/20\n","37168/37168 [==============================] - 503s 14ms/sample - loss: 0.3032 - acc: 0.8744 - val_loss: 1.1807 - val_acc: 0.6207\n","Epoch 13/20\n","37168/37168 [==============================] - 501s 13ms/sample - loss: 0.2514 - acc: 0.8931 - val_loss: 1.1725 - val_acc: 0.6196\n","Epoch 14/20\n","37168/37168 [==============================] - 501s 13ms/sample - loss: 0.2112 - acc: 0.9077 - val_loss: 1.2053 - val_acc: 0.6255\n","Epoch 15/20\n","37168/37168 [==============================] - 501s 13ms/sample - loss: 0.1818 - acc: 0.9182 - val_loss: 1.1812 - val_acc: 0.6323\n","Epoch 16/20\n","37168/37168 [==============================] - 501s 13ms/sample - loss: 0.1590 - acc: 0.9271 - val_loss: 1.1816 - val_acc: 0.6268\n","Epoch 17/20\n","37168/37168 [==============================] - 501s 13ms/sample - loss: 0.1414 - acc: 0.9339 - val_loss: 1.1819 - val_acc: 0.6290\n","Epoch 18/20\n","37168/37168 [==============================] - 502s 14ms/sample - loss: 0.1261 - acc: 0.9400 - val_loss: 1.1870 - val_acc: 0.6303\n","Epoch 19/20\n","37168/37168 [==============================] - 501s 13ms/sample - loss: 0.1141 - acc: 0.9452 - val_loss: 1.1976 - val_acc: 0.6330\n","Epoch 20/20\n","37168/37168 [==============================] - 501s 13ms/sample - loss: 0.1050 - acc: 0.9488 - val_loss: 1.1851 - val_acc: 0.6365\n","Training complete.\n","\n","Trained model saved for later use\n","\n","Evaluating test...\n","Test data: loss = 1.185088  accuracy = 63.65% \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ztDgLLxJEydO","colab_type":"code","colab":{}},"source":["#Writing the vocabularies to file\n","\n","with open(resource_path+\"x_vocab.txt\", \"w\") as file:\n","    file.write(json.dumps(vocab))\n","    \n","with open(resource_path+\"y_vocab.txt\", \"w\") as file:\n","    file.write(json.dumps(y_vocab))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"abLRt33OxZmZ","colab_type":"text"},"source":["##Running Predictions"]},{"cell_type":"code","metadata":{"id":"FqZyKhGwEydR","colab_type":"code","colab":{}},"source":["from predict import *\n","from score import *"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"T4DYUwkGnSt-","colab_type":"code","outputId":"cb54ac78-3964-4221-e68d-537a4253ecc9","executionInfo":{"status":"ok","timestamp":1568233653005,"user_tz":-120,"elapsed":727,"user":{"displayName":"Sayo Makinwa","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB01ONwXql1XhFq6AK_2PjHkUfJA3VDpx1AVsLimg=s64","userId":"02832581640686072550"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["import nltk\n","nltk.download(\"wordnet\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"B8zHfVMX9tqC","colab_type":"code","colab":{}},"source":["def run_tests1(score_file):\n","    \n","    resources_path = '../resources'\n","    bn2wn_mapping_file = '../resources/babelnet2wordnet.tsv'\n","    \n","    all_scores = []\n","    if (os.path.isfile(score_file)):\n","        with open(score_file, 'r') as file:\n","            for line in file:\n","                all_scores.append(line.split(\",\"))\n","        \n","    print(\"PREDICTING FOR SE2...\")\n","    se2_scores = ['SE2']\n","\n","    input_path = '../data/Evaluation_Datasets/senseval2/senseval2.data.xml'\n","    output_path = '../data/Evaluation_Datasets/senseval2'\n","    gold_file =  '../data/Evaluation_Datasets/senseval2/senseval2.gold.key.txt'\n","\n","    pred_babelnet = '../data/Evaluation_Datasets/senseval2/pred_babelnet.txt'\n","    pred_lex = '../data/Evaluation_Datasets/senseval2/pred_lex.txt'\n","    pred_domains = '../data/Evaluation_Datasets/senseval2/pred_domains.txt'\n","\n","    predict_babelnet(input_path, output_path, resources_path)\n","    score = score_predict1(pred_babelnet, gold_file, bn2wn_mapping_file)\n","    se2_scores.append(score*100)\n","\n","    predict_lexicographer(input_path, output_path, resources_path)\n","    score = score_predict_lex(pred_lex, gold_file, resources_path)\n","    se2_scores.append(score*100)\n","\n","    predict_wordnet_domains(input_path, output_path, resources_path)\n","    score = score_predict_dom(pred_domains, gold_file, resources_path)\n","    se2_scores.append(score*100)\n","\n","    all_scores.append(se2_scores)\n","    \n","\n","    #########################################################\n","\n","    print(\"\\n\\nPREDICTING FOR SE3...\")\n","    se3_scores = ['SE3_(Dev)']\n","\n","    input_path = '../data/Evaluation_Datasets/senseval3/senseval3.data.xml'\n","    output_path = '../data/Evaluation_Datasets/senseval3'\n","    gold_file =  '../data/Evaluation_Datasets/senseval3/senseval3.gold.key.txt'\n","\n","    pred_babelnet = '../data/Evaluation_Datasets/senseval3/pred_babelnet.txt'\n","    pred_lex = '../data/Evaluation_Datasets/senseval3/pred_lex.txt'\n","    pred_domains = '../data/Evaluation_Datasets/senseval3/pred_domains.txt'\n","\n","    predict_babelnet(input_path, output_path, resources_path)\n","    score = score_predict1(pred_babelnet, gold_file, bn2wn_mapping_file)\n","    se3_scores.append(score*100)\n","\n","    predict_lexicographer(input_path, output_path, resources_path)\n","    score = score_predict_lex(pred_lex, gold_file, resources_path)\n","    se3_scores.append(score*100)\n","\n","    predict_wordnet_domains(input_path, output_path, resources_path)\n","    score = score_predict_dom(pred_domains, gold_file, resources_path)\n","    se3_scores.append(score*100)\n","\n","    all_scores.append(se3_scores)\n","    \n","    \n","    ###########################################################\n","        \n","    with open(score_file, \"w\") as file:\n","        for scores in all_scores:\n","            file.write(\"{},{:.2f},{:.2f},{:.2f}\\n\".format(scores[0], float(scores[1]), float(scores[2]), float(scores[3])))\n","\n","\n","\n","def run_tests2(score_file):\n","    \n","    resources_path = '../resources'\n","    bn2wn_mapping_file = '../resources/babelnet2wordnet.tsv'\n","    \n","    all_scores = []\n","    if (os.path.isfile(score_file)):\n","        with open(score_file, 'r') as file:\n","            for line in file:\n","                all_scores.append(line.split(\",\"))\n","                \n","    \n","    print(\"\\n\\nPREDICTING FOR SE07...\")\n","    se07_scores = ['SE07']\n","    \n","    input_path = '../data/Evaluation_Datasets/semeval2007/semeval2007.data.xml'\n","    output_path = '../data/Evaluation_Datasets/semeval2007'\n","    gold_file =  '../data/Evaluation_Datasets/semeval2007/semeval2007.gold.key.txt'\n","    \n","    pred_babelnet = '../data/Evaluation_Datasets/semeval2007/pred_babelnet.txt'\n","    pred_lex = '../data/Evaluation_Datasets/semeval2007/pred_lex.txt'\n","    pred_domains = '../data/Evaluation_Datasets/semeval2007/pred_domains.txt'\n","\n","    predict_babelnet(input_path, output_path, resources_path)\n","    score = score_predict1(pred_babelnet, gold_file, bn2wn_mapping_file)\n","    se07_scores.append(score*100)\n","    \n","    predict_lexicographer(input_path, output_path, resources_path)\n","    score = score_predict_lex(pred_lex, gold_file, resources_path)\n","    se07_scores.append(score*100)\n","    \n","    predict_wordnet_domains(input_path, output_path, resources_path)\n","    score = score_predict_dom(pred_domains, gold_file, resources_path)\n","    se07_scores.append(score*100)\n","    \n","    all_scores.append(se07_scores)\n","    \n","    #########################################################\n","    \n","    print(\"\\n\\nPREDICTING FOR SE13...\")\n","    se13_scores = ['SE13']\n","    \n","    input_path = '../data/Evaluation_Datasets/semeval2013/semeval2013.data.xml'\n","    output_path = '../data/Evaluation_Datasets/semeval2013'\n","    gold_file =  '../data/Evaluation_Datasets/semeval2013/semeval2013.gold.key.txt'\n","    \n","    pred_babelnet = '../data/Evaluation_Datasets/semeval2013/pred_babelnet.txt'\n","    pred_lex = '../data/Evaluation_Datasets/semeval2013/pred_lex.txt'\n","    pred_domains = '../data/Evaluation_Datasets/semeval2013/pred_domains.txt'\n","\n","    predict_babelnet(input_path, output_path, resources_path)\n","    score = score_predict1(pred_babelnet, gold_file, bn2wn_mapping_file)\n","    se13_scores.append(score*100)\n","    \n","    predict_lexicographer(input_path, output_path, resources_path)\n","    score = score_predict_lex(pred_lex, gold_file, resources_path)\n","    se13_scores.append(score*100)\n","    \n","    predict_wordnet_domains(input_path, output_path, resources_path)\n","    score = score_predict_dom(pred_domains, gold_file, resources_path)\n","    se13_scores.append(score*100)\n","    \n","    all_scores.append(se13_scores)\n","    \n","    #########################################################\n","    \n","    \n","    with open(score_file, \"w\") as file:\n","        for scores in all_scores:\n","            file.write(\"{},{:.2f},{:.2f},{:.2f}\\n\".format(scores[0], float(scores[1]), float(scores[2]), float(scores[3])))\n","            \n","            \n","            \n","    \n","    \n","\n","def run_tests3(score_file):\n","    \n","    resources_path = '../resources'\n","    bn2wn_mapping_file = '../resources/babelnet2wordnet.tsv'\n","    \n","    all_scores = []\n","    if (os.path.isfile(score_file)):\n","        with open(score_file, 'r') as file:\n","            for line in file:\n","                all_scores.append(line.split(\",\"))\n","                \n","    \n","    print(\"\\n\\nPREDICTING FOR SE15...\")\n","    se15_scores = ['SE15']\n","    \n","    input_path = '../data/Evaluation_Datasets/semeval2015/semeval2015.data.xml'\n","    output_path = '../data/Evaluation_Datasets/semeval2015'\n","    gold_file =  '../data/Evaluation_Datasets/semeval2015/semeval2015.gold.key.txt'\n","    \n","    pred_babelnet = '../data/Evaluation_Datasets/semeval2015/pred_babelnet.txt'\n","    pred_lex = '../data/Evaluation_Datasets/semeval2015/pred_lex.txt'\n","    pred_domains = '../data/Evaluation_Datasets/semeval2015/pred_domains.txt'\n","\n","    predict_babelnet(input_path, output_path, resources_path)\n","    score = score_predict1(pred_babelnet, gold_file, bn2wn_mapping_file)\n","    se15_scores.append(score*100)\n","    \n","    predict_lexicographer(input_path, output_path, resources_path)\n","    score = score_predict_lex(pred_lex, gold_file, resources_path)\n","    se15_scores.append(score*100)\n","    \n","    predict_wordnet_domains(input_path, output_path, resources_path)\n","    score = score_predict_dom(pred_domains, gold_file, resources_path)\n","    se15_scores.append(score*100)\n","    \n","    all_scores.append(se15_scores)\n","    \n","    \n","    #####################################################################\n","    \n","    \n","    print(\"\\n\\nPREDICTING FOR ALL...\")\n","    se_ALL_scores = ['ALL']\n","    \n","    input_path = '../data/Evaluation_Datasets/ALL/ALL.data.xml'\n","    output_path = '../data/Evaluation_Datasets/ALL'\n","    gold_file =  '../data/Evaluation_Datasets/ALL/ALL.gold.key.txt'\n","    \n","    pred_babelnet = '../data/Evaluation_Datasets/ALL/pred_babelnet.txt'\n","    pred_lex = '../data/Evaluation_Datasets/ALL/pred_lex.txt'\n","    pred_domains = '../data/Evaluation_Datasets/ALL/pred_domains.txt'\n","\n","    predict_babelnet(input_path, output_path, resources_path)\n","    score = score_predict1(pred_babelnet, gold_file, bn2wn_mapping_file)\n","    se_ALL_scores.append(score*100)\n","    \n","    predict_lexicographer(input_path, output_path, resources_path)\n","    score = score_predict_lex(pred_lex, gold_file, resources_path)\n","    se_ALL_scores.append(score*100)\n","    \n","    predict_wordnet_domains(input_path, output_path, resources_path)\n","    score = score_predict_dom(pred_domains, gold_file, resources_path)\n","    se_ALL_scores.append(score*100)\n","    \n","    all_scores.append(se_ALL_scores)\n","    \n","    \n","    with open(score_file, \"w\") as file:\n","        for scores in all_scores:\n","            file.write(\"{},{:.2f},{:.2f},{:.2f}\\n\".format(scores[0], float(scores[1]), float(scores[2]), float(scores[3])))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"N-zJujtZEydT","colab_type":"code","outputId":"6e454323-2285-4967-bfbe-b742c602e2d3","executionInfo":{"status":"ok","timestamp":1568234094664,"user_tz":-120,"elapsed":418942,"user":{"displayName":"Sayo Makinwa","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB01ONwXql1XhFq6AK_2PjHkUfJA3VDpx1AVsLimg=s64","userId":"02832581640686072550"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["#all_scores_pd = run_tests1(\"../resources/scores.csv\")\n","#all_scores_pd = run_tests2(\"../resources/scores.csv\")\n","all_scores_pd = run_tests3(\"../resources/scores.csv\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","\n","PREDICTING FOR SE15...\n","LOADING RESOURCES...\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py:3794: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","PREPARING EVALUATION DATA FOR PREDICTION...\n","138 sentences extracted...\n","Evaluation data extraction completed\n","Predicting (line by line) and writing to file... This may take a little while...\n","100/138 lines done... A little moment more and everything will be done! :)\n","Prediction complete!\n","LOADING RESOURCES...\n","PREPARING EVALUATION DATA FOR PREDICTION...\n","138 sentences extracted...\n","Evaluation data extraction completed\n","Predicting (line by line) and writing to file... This may take a little while...\n","100/138 lines done... A little moment more and everything will be done! :)\n","Prediction complete!\n","LOADING RESOURCES...\n","PREPARING EVALUATION DATA FOR PREDICTION...\n","138 sentences extracted...\n","Evaluation data extraction completed\n","Predicting (line by line) and writing to file... This may take a little while...\n","100/138 lines done... A little moment more and everything will be done! :)\n","Prediction complete!\n","\n","\n","PREDICTING FOR ALL...\n","LOADING RESOURCES...\n","PREPARING EVALUATION DATA FOR PREDICTION...\n","1,173 sentences extracted...\n","Evaluation data extraction completed\n","Predicting (line by line) and writing to file... This may take a little while...\n","100/1173 lines done... A little moment more and everything will be done! :)\n","200/1173 lines done... A little moment more and everything will be done! :)\n","300/1173 lines done... A little moment more and everything will be done! :)\n","400/1173 lines done... A little moment more and everything will be done! :)\n","500/1173 lines done... A little moment more and everything will be done! :)\n","600/1173 lines done... A little moment more and everything will be done! :)\n","700/1173 lines done... A little moment more and everything will be done! :)\n","800/1173 lines done... A little moment more and everything will be done! :)\n","900/1173 lines done... A little moment more and everything will be done! :)\n","1000/1173 lines done... A little moment more and everything will be done! :)\n","1100/1173 lines done... A little moment more and everything will be done! :)\n","Prediction complete!\n","LOADING RESOURCES...\n","PREPARING EVALUATION DATA FOR PREDICTION...\n","1,173 sentences extracted...\n","Evaluation data extraction completed\n","Predicting (line by line) and writing to file... This may take a little while...\n","100/1173 lines done... A little moment more and everything will be done! :)\n","200/1173 lines done... A little moment more and everything will be done! :)\n","300/1173 lines done... A little moment more and everything will be done! :)\n","400/1173 lines done... A little moment more and everything will be done! :)\n","500/1173 lines done... A little moment more and everything will be done! :)\n","600/1173 lines done... A little moment more and everything will be done! :)\n","700/1173 lines done... A little moment more and everything will be done! :)\n","800/1173 lines done... A little moment more and everything will be done! :)\n","900/1173 lines done... A little moment more and everything will be done! :)\n","1000/1173 lines done... A little moment more and everything will be done! :)\n","1100/1173 lines done... A little moment more and everything will be done! :)\n","Prediction complete!\n","LOADING RESOURCES...\n","PREPARING EVALUATION DATA FOR PREDICTION...\n","1,173 sentences extracted...\n","Evaluation data extraction completed\n","Predicting (line by line) and writing to file... This may take a little while...\n","100/1173 lines done... A little moment more and everything will be done! :)\n","200/1173 lines done... A little moment more and everything will be done! :)\n","300/1173 lines done... A little moment more and everything will be done! :)\n","400/1173 lines done... A little moment more and everything will be done! :)\n","500/1173 lines done... A little moment more and everything will be done! :)\n","600/1173 lines done... A little moment more and everything will be done! :)\n","700/1173 lines done... A little moment more and everything will be done! :)\n","800/1173 lines done... A little moment more and everything will be done! :)\n","900/1173 lines done... A little moment more and everything will be done! :)\n","1000/1173 lines done... A little moment more and everything will be done! :)\n","1100/1173 lines done... A little moment more and everything will be done! :)\n","Prediction complete!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"945VsOMqSmA1","colab_type":"text"},"source":["##Results"]},{"cell_type":"code","metadata":{"id":"kViXUjmsEydk","colab_type":"code","outputId":"fe4b4a37-771f-40ab-e6ff-c811eef9f5a8","executionInfo":{"status":"ok","timestamp":1568234139079,"user_tz":-120,"elapsed":863,"user":{"displayName":"Sayo Makinwa","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB01ONwXql1XhFq6AK_2PjHkUfJA3VDpx1AVsLimg=s64","userId":"02832581640686072550"}},"colab":{"base_uri":"https://localhost:8080/","height":235}},"source":["all_scores_pd = pd.read_csv('../resources/scores.csv', names=['Babelnet', 'Lex', 'Domain'])\n","all_scores_pd"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Babelnet</th>\n","      <th>Lex</th>\n","      <th>Domain</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>SE2</th>\n","      <td>60.96</td>\n","      <td>77.78</td>\n","      <td>88.17</td>\n","    </tr>\n","    <tr>\n","      <th>SE3_(Dev)</th>\n","      <td>64.32</td>\n","      <td>77.89</td>\n","      <td>86.65</td>\n","    </tr>\n","    <tr>\n","      <th>SE07</th>\n","      <td>52.97</td>\n","      <td>69.67</td>\n","      <td>85.71</td>\n","    </tr>\n","    <tr>\n","      <th>SE13</th>\n","      <td>57.48</td>\n","      <td>68.92</td>\n","      <td>74.21</td>\n","    </tr>\n","    <tr>\n","      <th>SE15</th>\n","      <td>53.42</td>\n","      <td>70.55</td>\n","      <td>78.77</td>\n","    </tr>\n","    <tr>\n","      <th>ALL</th>\n","      <td>59.47</td>\n","      <td>74.27</td>\n","      <td>83.14</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["           Babelnet    Lex  Domain\n","SE2           60.96  77.78   88.17\n","SE3_(Dev)     64.32  77.89   86.65\n","SE07          52.97  69.67   85.71\n","SE13          57.48  68.92   74.21\n","SE15          53.42  70.55   78.77\n","ALL           59.47  74.27   83.14"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"S-zN_Fb_Celt","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}