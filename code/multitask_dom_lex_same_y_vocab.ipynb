{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"colab":{"name":"multitask_dom_lex_same_y_vocab.ipynb","version":"0.3.2","provenance":[{"file_id":"1f0LKOmNKx6sRcP7dxOwG9azPNDTH7abx","timestamp":1568128407649},{"file_id":"1ztcJt_LgDIH_Umi5lGsDDBDB86ju8P36","timestamp":1567958864120},{"file_id":"1UCBYAjzB3OqMVVwR6-gJ23VgMpVvN6Ql","timestamp":1567817661516}],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"G4oaJRH9gCw5","colab_type":"text"},"source":["## Mount Drive and Imports"]},{"cell_type":"code","metadata":{"id":"NKtsqriiKxYd","colab_type":"code","outputId":"c0d7683a-03d2-49f0-bea9-fdc43d96342a","executionInfo":{"status":"ok","timestamp":1568328850487,"user_tz":-120,"elapsed":1190,"user":{"displayName":"Sayo Makinwa","photoUrl":"","userId":"02017936482338602411"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nRTrAorEKyu5","colab_type":"code","outputId":"6b6984b1-501c-4a01-b6f2-35f127c95282","executionInfo":{"status":"ok","timestamp":1568328852164,"user_tz":-120,"elapsed":972,"user":{"displayName":"Sayo Makinwa","photoUrl":"","userId":"02017936482338602411"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%cd /content/drive/My Drive/nlp_hw3/code"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/nlp_hw3/code\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OxdkmYCxEybs","colab_type":"code","colab":{}},"source":["import os\n","import numpy as np\n","from typing import Tuple, List, Dict\n","\n","import tensorflow as tf\n","import tensorflow.keras as K\n","from tensorflow.keras.layers import *\n","from tensorflow.keras.models import *\n","\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","from tensorflow.keras.callbacks import TensorBoard\n","from tensorflow.python.eager import context\n","import json\n","import pandas as pd\n","from nltk.corpus import stopwords\n","\n","import my_utils as utils\n","import corpora"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NOQLizEYgYAY","colab_type":"text"},"source":["## Preprocessing"]},{"cell_type":"code","metadata":{"id":"042xGwFcEyb1","colab_type":"code","colab":{}},"source":["def load_train_dataset(input_path: str, y_path: str) -> Tuple[List[str], List[str]]:\n","    \"\"\"\n","    :param input_path; Path to the input dataset\n","    :param label_path; Path to the file containing the corresponding labels for the input dataset\n","    :return sentences; List of sentences in input_file\n","    :return labels; List of corresponding word segment codes in label_path. Same len as sentences\n","    \"\"\"\n","    sentences = []\n","    k = 0\n","    with open(input_path, \"r\", encoding=\"utf-8-sig\") as file:\n","        for line in file:\n","            k += 1\n","            sentences.append(line.strip())\n","#             if (k >= 4000):\n","#                 break\n","\n","    y = []\n","    k = 0\n","    with open(y_path, \"r\", encoding=\"utf-8-sig\") as file:\n","        for line in file:\n","            k += 1\n","            y.append(line.strip())\n","#             if (k >= 4000):\n","#                 break\n","\n","    return sentences, y\n","\n","\n","def make_X_vocab(sentences: List[str]) -> Dict[str, int]:\n","    '''\n","    :param sentences; List of input sentences from the dataset\n","    :return unigrams_vocab; Dictionary from unigram to int\n","    :return bigrams_vocab; Dictionary from bigram to int\n","    '''\n","    vocab = {\"UNK\": 0}\n","\n","    for sentence in sentences:\n","        for word in sentence.split():\n","            if word not in vocab:\n","                vocab[word] = len(vocab)\n","\n","    return vocab\n","\n","\n","def make_Y_vocab(y: List[str]) -> Dict[str, int]:\n","    \"\"\"\n","    :param labels; List of label codes\n","    :return labels_vocab; Dictionary from label code to int \n","    \"\"\"\n","    y_vocab = {\"UNK\": 0}\n","    \n","    for y_line in y:\n","        for y_word in y_line.split():\n","            if y_word not in y_vocab:\n","                y_vocab[y_word] = len(y_vocab)\n","                \n","    return y_vocab\n","\n","def make_Y(output: List[str], output_vocab: Dict[str, int]) -> np.ndarray:\n","    \"\"\"\n","    :param labels; List of word segment codes, line by line\n","    :param labels_vocab; Label codes vocab\n","    :return y; Vector of label code indices\n","    \"\"\"\n","    y = []\n","    #one_hot = np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]])\n","    for output_line in output:\n","        y_temp = []\n","        for single_output in output_line.split():\n","            #y_temp.append( one_hot [labels_vocab[label]] )\n","            if single_output in output_vocab:\n","                y_temp.append( output_vocab[single_output])\n","            else:\n","                y_temp.append( output_vocab[\"OTHERS\"])\n","        y.append(np.array(y_temp))\n","    \n","    return np.array(y)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vjHEGJfrggU3","colab_type":"code","colab":{}},"source":["train_data_path = '../data/Training_Corpora/semcor'\n","test_data_path = '../data/Evaluation_Datasets/senseval3'\n","\n","#Parse training data\n","corpora_xml_path = train_data_path + '/semcor.data.xml'\n","gold_mapping_path = train_data_path + '/semcor.gold.key.txt'\n","resources_path = '../resources/'\n","\n","corpora.extract_training_data(corpora_xml_path, gold_mapping_path, resources_path, train_data_path)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"y-mS96uvggIq","colab_type":"code","colab":{}},"source":["#Parse validation data\n","corpora_xml_path = test_data_path + '/senseval3.data.xml'\n","gold_mapping_path = test_data_path + '/senseval3.gold.key.txt'\n","resources_path = '../resources/'\n","\n","corpora.extract_training_data(corpora_xml_path, gold_mapping_path, resources_path, test_data_path)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yVV_9dxlEyb4","colab_type":"code","colab":{}},"source":["sentences, y = load_train_dataset(train_data_path+\"/trainX.txt\", train_data_path+\"/trainy.txt\")\n","_, y_dom = load_train_dataset(train_data_path+\"/trainX.txt\", train_data_path+\"/trainy_dom.txt\")\n","_, y_lex = load_train_dataset(train_data_path+\"/trainX.txt\", train_data_path+\"/trainy_lex.txt\")\n","\n","test_sentences, test_y = load_train_dataset(test_data_path+\"/trainX.txt\", test_data_path+\"/trainy.txt\")\n","_, test_y_dom = load_train_dataset(test_data_path+\"/trainX.txt\", test_data_path+\"/trainy_dom.txt\")\n","_, test_y_lex = load_train_dataset(test_data_path+\"/trainX.txt\", test_data_path+\"/trainy_lex.txt\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-EJ1fHSzEyb7","colab_type":"code","outputId":"26c9e25f-fc6c-47b9-bc1c-52c03848b095","executionInfo":{"status":"ok","timestamp":1568286176299,"user_tz":-120,"elapsed":1867,"user":{"displayName":"Sayo Michael Makinwa","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBCp3x5tBH-cq0DfnQqDJOvAgn1rEOOtZs5RyX9=s64","userId":"08567360008987707591"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["print(len(sentences))\n","print(sentences[0])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["37168\n","How long has it been since you reviewed the objectives of your benefit and service program\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"t1fD1tgDEycC","colab_type":"code","outputId":"ddc9a3a3-ef6b-4cd8-daa6-50485914c536","executionInfo":{"status":"ok","timestamp":1568286176302,"user_tz":-120,"elapsed":453,"user":{"displayName":"Sayo Michael Makinwa","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBCp3x5tBH-cq0DfnQqDJOvAgn1rEOOtZs5RyX9=s64","userId":"08567360008987707591"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["print(len(y))\n","print(y[0])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["37168\n","how bn:00106124a have it bn:00083181v since you bn:00092618v the bn:00002179n of you bn:00009904n and bn:00070654n bn:00064646n\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HgCzdmbLEycF","colab_type":"code","outputId":"2aa14ea8-0b7a-4ace-c361-646da35930c5","executionInfo":{"status":"ok","timestamp":1568286177866,"user_tz":-120,"elapsed":912,"user":{"displayName":"Sayo Michael Makinwa","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBCp3x5tBH-cq0DfnQqDJOvAgn1rEOOtZs5RyX9=s64","userId":"08567360008987707591"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["vocab = make_X_vocab(sentences+test_sentences)\n","print(len(vocab))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["48989\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LPRR9TA7EycH","colab_type":"code","outputId":"3b0e27a0-6543-42d4-b58d-f97459ecfc6c","executionInfo":{"status":"ok","timestamp":1568286178278,"user_tz":-120,"elapsed":799,"user":{"displayName":"Sayo Michael Makinwa","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBCp3x5tBH-cq0DfnQqDJOvAgn1rEOOtZs5RyX9=s64","userId":"08567360008987707591"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["y_vocab = make_Y_vocab(y + test_y + y_dom + test_y_dom + y_lex + test_y_lex)\n","print(len(y_vocab))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["49496\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iD6TeqJREycR","colab_type":"code","colab":{}},"source":["X = utils.make_X(sentences, vocab)\n","y_array = make_Y(y, y_vocab)\n","y_dom_array = make_Y(y_dom, y_vocab)\n","y_lex_array = make_Y(y_lex, y_vocab)\n","\n","X_test = utils.make_X(test_sentences, vocab)\n","y_test_array = make_Y(test_y, y_vocab)\n","y_dom_test_array = make_Y(test_y_dom, y_vocab)\n","y_lex_test_array = make_Y(test_y_lex, y_vocab)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DF5m68nmEycT","colab_type":"code","outputId":"40a825d0-5b4c-458a-cfb5-477eb9eb7126","executionInfo":{"status":"ok","timestamp":1568286180312,"user_tz":-120,"elapsed":1866,"user":{"displayName":"Sayo Michael Makinwa","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBCp3x5tBH-cq0DfnQqDJOvAgn1rEOOtZs5RyX9=s64","userId":"08567360008987707591"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["X.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(37168,)"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"j7fg-usHEycZ","colab_type":"code","colab":{}},"source":["train_x2 = pad_sequences(X, truncating='pre', padding='post', maxlen=30)\n","train_y2 = pad_sequences(y_array, truncating='pre', padding='post', maxlen=30)\n","train_y_dom = pad_sequences(y_dom_array, truncating='pre', padding='post', maxlen=30)\n","train_y_lex = pad_sequences(y_lex_array, truncating='pre', padding='post', maxlen=30)\n","\n","dev_x2 = pad_sequences(X_test, truncating='pre', padding='post', maxlen=30)\n","dev_y2 = pad_sequences(y_test_array, truncating='pre', padding='post', maxlen=30)\n","dev_y_dom = pad_sequences(y_dom_test_array, truncating='pre', padding='post', maxlen=30)\n","dev_y_lex = pad_sequences(y_lex_test_array, truncating='pre', padding='post', maxlen=30)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NmXFcuk4Eycc","colab_type":"code","outputId":"e815a10a-dd0b-4475-f004-00121fff652d","executionInfo":{"status":"ok","timestamp":1568286180319,"user_tz":-120,"elapsed":699,"user":{"displayName":"Sayo Michael Makinwa","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBCp3x5tBH-cq0DfnQqDJOvAgn1rEOOtZs5RyX9=s64","userId":"08567360008987707591"}},"colab":{"base_uri":"https://localhost:8080/","height":90}},"source":["print(train_x2.shape)\n","print(train_y2.shape)\n","print(train_y_dom.shape)\n","print(train_y_lex.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(37168, 30)\n","(37168, 30)\n","(37168, 30)\n","(37168, 30)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YIojgrqREycg","colab_type":"code","outputId":"68fedcdf-cdb0-44e9-9554-f6bacf9e5541","executionInfo":{"status":"ok","timestamp":1568286181548,"user_tz":-120,"elapsed":1367,"user":{"displayName":"Sayo Michael Makinwa","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBCp3x5tBH-cq0DfnQqDJOvAgn1rEOOtZs5RyX9=s64","userId":"08567360008987707591"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["train_y2 = train_y2.reshape((*train_y2.shape, 1))\n","dev_y2 = dev_y2.reshape((*dev_y2.shape, 1))\n","\n","train_y_dom = train_y_dom.reshape((*train_y_dom.shape, 1))\n","dev_y_dom = dev_y_dom.reshape((*dev_y_dom.shape, 1))\n","\n","train_y_lex = train_y_lex.reshape((*train_y_lex.shape, 1))\n","dev_y_lex = dev_y_lex.reshape((*dev_y_lex.shape, 1))\n","\n","print(train_y2.shape)\n","print(dev_y2.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(37168, 30, 1)\n","(352, 30, 1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ABjb_KPbEyc1","colab_type":"code","outputId":"dbb727e4-d523-442e-fc51-34685a7d8879","executionInfo":{"status":"ok","timestamp":1568286181553,"user_tz":-120,"elapsed":890,"user":{"displayName":"Sayo Michael Makinwa","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBCp3x5tBH-cq0DfnQqDJOvAgn1rEOOtZs5RyX9=s64","userId":"08567360008987707591"}},"colab":{"base_uri":"https://localhost:8080/","height":90}},"source":["print(train_x2.shape)\n","print(train_y2.shape)\n","print(dev_x2.shape)\n","print(dev_y2.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(37168, 30)\n","(37168, 30, 1)\n","(352, 30)\n","(352, 30, 1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uG7owux2hRbC","colab_type":"text"},"source":["## Building the model"]},{"cell_type":"code","metadata":{"id":"gd-bBhRDEyc-","colab_type":"code","colab":{}},"source":["vocab_size = len(vocab)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9uF52K_8EydA","colab_type":"code","colab":{}},"source":["#This class helps with logging\n","class TrainValTensorBoard(TensorBoard):\n","    def __init__(self, log_dir='./logs', **kwargs):\n","        self.val_log_dir = os.path.join(log_dir, 'multitask_wsd_1vocab/validation')\n","        training_log_dir = os.path.join(log_dir, 'multitask_wsd_1vocab/training')\n","        super(TrainValTensorBoard, self).__init__(training_log_dir, **kwargs)\n","\n","    def set_model(self, model):\n","        if context.executing_eagerly():\n","            self.val_writer = tf.contrib.summary.create_file_writer(self.val_log_dir)\n","        else:\n","            self.val_writer = tf.summary.FileWriter(self.val_log_dir)\n","        super(TrainValTensorBoard, self).set_model(model)\n","\n","    def _write_custom_summaries(self, step, logs=None):\n","        logs = logs or {}\n","        val_logs = {k.replace('val_', ''): v for k, v in logs.items() if 'val_' in k}\n","        if context.executing_eagerly():\n","            with self.val_writer.as_default(), tf.contrib.summary.always_record_summaries():\n","                for name, value in val_logs.items():\n","                    tf.contrib.summary.scalar(name, value.item(), step=step)\n","        else:\n","            for name, value in val_logs.items():\n","                summary = tf.Summary()\n","                summary_value = summary.value.add()\n","                summary_value.simple_value = value.item()\n","                summary_value.tag = name\n","                self.val_writer.add_summary(summary, step)\n","        self.val_writer.flush()\n","\n","        logs = {k: v for k, v in logs.items() if not 'val_' in k}\n","        super(TrainValTensorBoard, self)._write_custom_summaries(step, logs)\n","\n","    def on_train_end(self, logs=None):\n","        super(TrainValTensorBoard, self).on_train_end(logs)\n","        self.val_writer.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fXhtH-HhEydE","colab_type":"code","colab":{}},"source":["#Please take note that most of this part was extracted from class exercises, with some additions\n","\n","def create_keras_model(vocab_size, y_size, embedding_size=128, hidden_size=512):\n","    print(\"Creating KERAS model\")\n","\n","\n","    model_input = Input(shape=(None,))\n","    embedding = Embedding(vocab_size, embedding_size, mask_zero=True)(model_input)\n","    lstm1 = Bidirectional(LSTM(hidden_size, dropout=0.2, recurrent_dropout=0.2, return_sequences=True), merge_mode='concat')(embedding)\n","    lstm2 = Bidirectional(LSTM(hidden_size, dropout=0.2, recurrent_dropout=0.2, return_sequences=True), merge_mode='concat')(lstm1)\n","\n","    output1 = TimeDistributed(Dense(y_size, activation='softmax'))(lstm2)\n","    output2 = TimeDistributed(Dense(y_size, activation='softmax'))(lstm2)\n","    output3 = TimeDistributed(Dense(y_size, activation='softmax'))(lstm2)\n","\n","    model = Model(inputs=model_input, outputs=[output1, output2, output3])\n","\n","\n","    #     model = K.models.Sequential()\n","    #     model.add(Embedding(vocab_size, embedding_size, mask_zero=True))\n","\n","    #     model.add(Bidirectional(LSTM(hidden_size, dropout=0.2, recurrent_dropout=0.2, return_sequences=True), merge_mode='concat'))\n","    #     model.add(Bidirectional(LSTM(hidden_size, dropout=0.2, recurrent_dropout=0.2, return_sequences=True), merge_mode='concat'))\n","    #     #model.add(Bidirectional(LSTM(hidden_size, dropout=0.2, recurrent_dropout=0.2, return_sequences=True), merge_mode='concat'))\n","\n","    #     #Multi-task learning outputs\n","    #     output1 = TimeDistributed(Dense(y_size, activation='softmax'))\n","    #     output2 = TimeDistributed(Dense(y_size, activation='softmax'))\n","    #     output3 = TimeDistributed(Dense(y_size, activation='softmax'))\n","\n","    #     model.add([output1, output2, output3])\n","    optimizer = K.optimizers.Adam()\n","\n","    model.compile(loss=['sparse_categorical_crossentropy', 'sparse_categorical_crossentropy', 'sparse_categorical_crossentropy'], optimizer=optimizer, metrics=['acc'])\n","\n","    return model\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PMqneG5WEydI","colab_type":"code","colab":{}},"source":["resource_path = \"../resources/\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pFd4-nloEydL","colab_type":"code","outputId":"4201b6dd-511b-4944-b31e-7fe0d09cb6f9","executionInfo":{"status":"ok","timestamp":1568299040862,"user_tz":-120,"elapsed":9098002,"user":{"displayName":"Sayo Michael Makinwa","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBCp3x5tBH-cq0DfnQqDJOvAgn1rEOOtZs5RyX9=s64","userId":"08567360008987707591"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["batch_size = 64\n","epochs = 20\n","model_name = resource_path+\"mult1_model.hdf5\"\n","\n","#checks if the FINAL model was saved and loads it instead of creating a new one\n","if os.path.exists(model_name):\n","    model = load_model(model_name)\n","    print(\"Using a pre-saved model\")\n","    model.summary()\n","    \n","else:\n","    model = create_keras_model(vocab_size, len(y_vocab))\n","    print(\"Training a new model\")\n","    model.summary()\n","    \n","    #filepath = resource_path+\"models/model-{epoch:02d}.hdf5\"\n","    filepath = resource_path+\"models/model_multitask1.hdf5\"\n","    checkpoint = K.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n","    #callbacks_list = [checkpoint]\n","    tb_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\", histogram_freq=0, write_graph=False, write_images=True)\n","    callbacks_list = [TrainValTensorBoard(write_graph=False), checkpoint]\n","    \n","    print(\"\\nStarting training...\")\n","    model.fit(train_x2, [train_y2, train_y_dom, train_y_lex], epochs=epochs, batch_size=batch_size,\n","              shuffle=True, validation_data=(dev_x2, [dev_y2, dev_y_dom, dev_y_lex]), callbacks=callbacks_list) \n","    print(\"Training complete.\\n\")\n","    \n","    #Save the FINAL model for later reuse\n","    model.save(model_name)\n","    print(\"Trained model saved for later use\")\n","\n","    "],"execution_count":0,"outputs":[{"output_type":"stream","text":["Creating KERAS model\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py:3794: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","Training a new model\n","Model: \"model\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            [(None, None)]       0                                            \n","__________________________________________________________________________________________________\n","embedding (Embedding)           (None, None, 128)    6270592     input_1[0][0]                    \n","__________________________________________________________________________________________________\n","bidirectional (Bidirectional)   (None, None, 1024)   2625536     embedding[0][0]                  \n","__________________________________________________________________________________________________\n","bidirectional_1 (Bidirectional) (None, None, 1024)   6295552     bidirectional[0][0]              \n","__________________________________________________________________________________________________\n","time_distributed (TimeDistribut (None, None, 49496)  50733400    bidirectional_1[0][0]            \n","__________________________________________________________________________________________________\n","time_distributed_1 (TimeDistrib (None, None, 49496)  50733400    bidirectional_1[0][0]            \n","__________________________________________________________________________________________________\n","time_distributed_2 (TimeDistrib (None, None, 49496)  50733400    bidirectional_1[0][0]            \n","==================================================================================================\n","Total params: 167,391,880\n","Trainable params: 167,391,880\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n","\n","Starting training...\n","Train on 37168 samples, validate on 352 samples\n","Epoch 1/20\n","37168/37168 [==============================] - 636s 17ms/sample - loss: 11.2300 - time_distributed_loss: 4.5551 - time_distributed_1_loss: 3.2410 - time_distributed_2_loss: 3.4331 - time_distributed_acc: 0.0736 - time_distributed_1_acc: 0.1687 - time_distributed_2_acc: 0.0828 - val_loss: 7.2612 - val_time_distributed_loss: 3.2192 - val_time_distributed_1_loss: 1.9287 - val_time_distributed_2_loss: 2.0665 - val_time_distributed_acc: 0.1211 - val_time_distributed_1_acc: 0.2660 - val_time_distributed_2_acc: 0.1546\n","Epoch 2/20\n","37168/37168 [==============================] - 640s 17ms/sample - loss: 7.4663 - time_distributed_loss: 3.2251 - time_distributed_1_loss: 2.0598 - time_distributed_2_loss: 2.1809 - time_distributed_acc: 0.3171 - time_distributed_1_acc: 0.4739 - time_distributed_2_acc: 0.3833 - val_loss: 4.5635 - val_time_distributed_loss: 2.1616 - val_time_distributed_1_loss: 1.1555 - val_time_distributed_2_loss: 1.2258 - val_time_distributed_acc: 0.4199 - val_time_distributed_1_acc: 0.6262 - val_time_distributed_2_acc: 0.5317\n","Epoch 3/20\n","37168/37168 [==============================] - 639s 17ms/sample - loss: 4.8829 - time_distributed_loss: 2.1891 - time_distributed_1_loss: 1.3125 - time_distributed_2_loss: 1.3811 - time_distributed_acc: 0.4973 - time_distributed_1_acc: 0.6545 - time_distributed_2_acc: 0.5887 - val_loss: 3.8939 - val_time_distributed_loss: 1.8513 - val_time_distributed_1_loss: 1.0084 - val_time_distributed_2_loss: 1.0454 - val_time_distributed_acc: 0.4952 - val_time_distributed_1_acc: 0.6759 - val_time_distributed_2_acc: 0.6204\n","Epoch 4/20\n","37168/37168 [==============================] - 639s 17ms/sample - loss: 3.6112 - time_distributed_loss: 1.6281 - time_distributed_1_loss: 0.9686 - time_distributed_2_loss: 1.0143 - time_distributed_acc: 0.5733 - time_distributed_1_acc: 0.7026 - time_distributed_2_acc: 0.6623 - val_loss: 3.5815 - val_time_distributed_loss: 1.6360 - val_time_distributed_1_loss: 0.9561 - val_time_distributed_2_loss: 0.9906 - val_time_distributed_acc: 0.5326 - val_time_distributed_1_acc: 0.6779 - val_time_distributed_2_acc: 0.6341\n","Epoch 5/20\n","37168/37168 [==============================] - 639s 17ms/sample - loss: 2.7509 - time_distributed_loss: 1.2300 - time_distributed_1_loss: 0.7418 - time_distributed_2_loss: 0.7791 - time_distributed_acc: 0.6382 - time_distributed_1_acc: 0.7459 - time_distributed_2_acc: 0.7152 - val_loss: 3.4820 - val_time_distributed_loss: 1.5641 - val_time_distributed_1_loss: 0.9637 - val_time_distributed_2_loss: 1.0035 - val_time_distributed_acc: 0.5513 - val_time_distributed_1_acc: 0.6691 - val_time_distributed_2_acc: 0.6308\n","Epoch 6/20\n","37168/37168 [==============================] - 641s 17ms/sample - loss: 2.1348 - time_distributed_loss: 0.9380 - time_distributed_1_loss: 0.5810 - time_distributed_2_loss: 0.6156 - time_distributed_acc: 0.6945 - time_distributed_1_acc: 0.7853 - time_distributed_2_acc: 0.7577 - val_loss: 3.3611 - val_time_distributed_loss: 1.4578 - val_time_distributed_1_loss: 0.9121 - val_time_distributed_2_loss: 0.9483 - val_time_distributed_acc: 0.5674 - val_time_distributed_1_acc: 0.6748 - val_time_distributed_2_acc: 0.6433\n","Epoch 7/20\n","37168/37168 [==============================] - 639s 17ms/sample - loss: 1.6693 - time_distributed_loss: 0.7147 - time_distributed_1_loss: 0.4605 - time_distributed_2_loss: 0.4940 - time_distributed_acc: 0.7469 - time_distributed_1_acc: 0.8205 - time_distributed_2_acc: 0.7948 - val_loss: 3.4286 - val_time_distributed_loss: 1.4353 - val_time_distributed_1_loss: 0.9587 - val_time_distributed_2_loss: 0.9935 - val_time_distributed_acc: 0.5724 - val_time_distributed_1_acc: 0.6653 - val_time_distributed_2_acc: 0.6354\n","Epoch 8/20\n","37168/37168 [==============================] - 640s 17ms/sample - loss: 1.3141 - time_distributed_loss: 0.5425 - time_distributed_1_loss: 0.3694 - time_distributed_2_loss: 0.4022 - time_distributed_acc: 0.7987 - time_distributed_1_acc: 0.8532 - time_distributed_2_acc: 0.8281 - val_loss: 3.4697 - val_time_distributed_loss: 1.4436 - val_time_distributed_1_loss: 0.9746 - val_time_distributed_2_loss: 1.0148 - val_time_distributed_acc: 0.5722 - val_time_distributed_1_acc: 0.6585 - val_time_distributed_2_acc: 0.6325\n","Epoch 9/20\n","37168/37168 [==============================] - 640s 17ms/sample - loss: 1.0393 - time_distributed_loss: 0.4091 - time_distributed_1_loss: 0.2992 - time_distributed_2_loss: 0.3310 - time_distributed_acc: 0.8461 - time_distributed_1_acc: 0.8804 - time_distributed_2_acc: 0.8563 - val_loss: 3.4207 - val_time_distributed_loss: 1.4623 - val_time_distributed_1_loss: 0.9865 - val_time_distributed_2_loss: 1.0309 - val_time_distributed_acc: 0.5812 - val_time_distributed_1_acc: 0.6675 - val_time_distributed_2_acc: 0.6438\n","Epoch 10/20\n","37168/37168 [==============================] - 641s 17ms/sample - loss: 0.8496 - time_distributed_loss: 0.3170 - time_distributed_1_loss: 0.2507 - time_distributed_2_loss: 0.2818 - time_distributed_acc: 0.8779 - time_distributed_1_acc: 0.8984 - time_distributed_2_acc: 0.8755 - val_loss: 3.3892 - val_time_distributed_loss: 1.4481 - val_time_distributed_1_loss: 0.9799 - val_time_distributed_2_loss: 1.0163 - val_time_distributed_acc: 0.5867 - val_time_distributed_1_acc: 0.6667 - val_time_distributed_2_acc: 0.6486\n","Epoch 11/20\n","37168/37168 [==============================] - 644s 17ms/sample - loss: 0.7183 - time_distributed_loss: 0.2543 - time_distributed_1_loss: 0.2165 - time_distributed_2_loss: 0.2474 - time_distributed_acc: 0.8988 - time_distributed_1_acc: 0.9108 - time_distributed_2_acc: 0.8886 - val_loss: 3.3764 - val_time_distributed_loss: 1.4250 - val_time_distributed_1_loss: 0.9434 - val_time_distributed_2_loss: 0.9880 - val_time_distributed_acc: 0.5914 - val_time_distributed_1_acc: 0.6788 - val_time_distributed_2_acc: 0.6519\n","Epoch 12/20\n","37168/37168 [==============================] - 648s 17ms/sample - loss: 0.6324 - time_distributed_loss: 0.2133 - time_distributed_1_loss: 0.1938 - time_distributed_2_loss: 0.2253 - time_distributed_acc: 0.9129 - time_distributed_1_acc: 0.9186 - time_distributed_2_acc: 0.8966 - val_loss: 3.4592 - val_time_distributed_loss: 1.4725 - val_time_distributed_1_loss: 1.0038 - val_time_distributed_2_loss: 1.0462 - val_time_distributed_acc: 0.5911 - val_time_distributed_1_acc: 0.6759 - val_time_distributed_2_acc: 0.6532\n","Epoch 13/20\n","37168/37168 [==============================] - 644s 17ms/sample - loss: 0.5706 - time_distributed_loss: 0.1851 - time_distributed_1_loss: 0.1774 - time_distributed_2_loss: 0.2082 - time_distributed_acc: 0.9224 - time_distributed_1_acc: 0.9248 - time_distributed_2_acc: 0.9031 - val_loss: 3.4862 - val_time_distributed_loss: 1.4757 - val_time_distributed_1_loss: 1.0039 - val_time_distributed_2_loss: 1.0410 - val_time_distributed_acc: 0.5896 - val_time_distributed_1_acc: 0.6691 - val_time_distributed_2_acc: 0.6517\n","Epoch 14/20\n","37168/37168 [==============================] - 642s 17ms/sample - loss: 0.5196 - time_distributed_loss: 0.1629 - time_distributed_1_loss: 0.1629 - time_distributed_2_loss: 0.1938 - time_distributed_acc: 0.9306 - time_distributed_1_acc: 0.9303 - time_distributed_2_acc: 0.9088 - val_loss: 3.4414 - val_time_distributed_loss: 1.4451 - val_time_distributed_1_loss: 0.9666 - val_time_distributed_2_loss: 1.0066 - val_time_distributed_acc: 0.5995 - val_time_distributed_1_acc: 0.6827 - val_time_distributed_2_acc: 0.6629\n","Epoch 15/20\n","37168/37168 [==============================] - 642s 17ms/sample - loss: 0.4830 - time_distributed_loss: 0.1471 - time_distributed_1_loss: 0.1526 - time_distributed_2_loss: 0.1834 - time_distributed_acc: 0.9364 - time_distributed_1_acc: 0.9341 - time_distributed_2_acc: 0.9129 - val_loss: 3.5748 - val_time_distributed_loss: 1.4838 - val_time_distributed_1_loss: 0.9973 - val_time_distributed_2_loss: 1.0425 - val_time_distributed_acc: 0.5889 - val_time_distributed_1_acc: 0.6779 - val_time_distributed_2_acc: 0.6519\n","Epoch 16/20\n","37168/37168 [==============================] - 642s 17ms/sample - loss: 0.4436 - time_distributed_loss: 0.1313 - time_distributed_1_loss: 0.1409 - time_distributed_2_loss: 0.1713 - time_distributed_acc: 0.9429 - time_distributed_1_acc: 0.9391 - time_distributed_2_acc: 0.9183 - val_loss: 3.5366 - val_time_distributed_loss: 1.5177 - val_time_distributed_1_loss: 1.0100 - val_time_distributed_2_loss: 1.0527 - val_time_distributed_acc: 0.5962 - val_time_distributed_1_acc: 0.6863 - val_time_distributed_2_acc: 0.6598\n","Epoch 17/20\n","37168/37168 [==============================] - 640s 17ms/sample - loss: 0.4156 - time_distributed_loss: 0.1206 - time_distributed_1_loss: 0.1324 - time_distributed_2_loss: 0.1627 - time_distributed_acc: 0.9473 - time_distributed_1_acc: 0.9421 - time_distributed_2_acc: 0.9217 - val_loss: 3.5961 - val_time_distributed_loss: 1.5348 - val_time_distributed_1_loss: 1.0364 - val_time_distributed_2_loss: 1.0778 - val_time_distributed_acc: 0.5887 - val_time_distributed_1_acc: 0.6715 - val_time_distributed_2_acc: 0.6524\n","Epoch 18/20\n","37168/37168 [==============================] - 644s 17ms/sample - loss: 0.3930 - time_distributed_loss: 0.1123 - time_distributed_1_loss: 0.1253 - time_distributed_2_loss: 0.1556 - time_distributed_acc: 0.9506 - time_distributed_1_acc: 0.9449 - time_distributed_2_acc: 0.9250 - val_loss: 3.5478 - val_time_distributed_loss: 1.5279 - val_time_distributed_1_loss: 1.0201 - val_time_distributed_2_loss: 1.0627 - val_time_distributed_acc: 0.5951 - val_time_distributed_1_acc: 0.6876 - val_time_distributed_2_acc: 0.6636\n","Epoch 19/20\n","37168/37168 [==============================] - 642s 17ms/sample - loss: 0.3698 - time_distributed_loss: 0.1040 - time_distributed_1_loss: 0.1178 - time_distributed_2_loss: 0.1481 - time_distributed_acc: 0.9545 - time_distributed_1_acc: 0.9481 - time_distributed_2_acc: 0.9284 - val_loss: 3.6102 - val_time_distributed_loss: 1.5159 - val_time_distributed_1_loss: 1.0055 - val_time_distributed_2_loss: 1.0557 - val_time_distributed_acc: 0.5920 - val_time_distributed_1_acc: 0.6812 - val_time_distributed_2_acc: 0.6616\n","Epoch 20/20\n","37168/37168 [==============================] - 641s 17ms/sample - loss: 0.3492 - time_distributed_loss: 0.0965 - time_distributed_1_loss: 0.1115 - time_distributed_2_loss: 0.1412 - time_distributed_acc: 0.9576 - time_distributed_1_acc: 0.9504 - time_distributed_2_acc: 0.9310 - val_loss: 3.6089 - val_time_distributed_loss: 1.5520 - val_time_distributed_1_loss: 1.0230 - val_time_distributed_2_loss: 1.0719 - val_time_distributed_acc: 0.5909 - val_time_distributed_1_acc: 0.6834 - val_time_distributed_2_acc: 0.6590\n","Training complete.\n","\n","Trained model saved for later use\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ztDgLLxJEydO","colab_type":"code","colab":{}},"source":["#Writing the vocabularies to file\n","\n","with open(resource_path+\"mult1_x_vocab.txt\", \"w\") as file:\n","    file.write(json.dumps(vocab))\n","    \n","with open(resource_path+\"mult1_y_vocab.txt\", \"w\") as file:\n","    file.write(json.dumps(y_vocab))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2aBvf6VBh09e","colab_type":"text"},"source":["## Running Predictions"]},{"cell_type":"code","metadata":{"id":"FqZyKhGwEydR","colab_type":"code","colab":{}},"source":["from score import *"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"T4DYUwkGnSt-","colab_type":"code","outputId":"f89a51c3-7886-4d13-d250-86a05e1c1e45","executionInfo":{"status":"ok","timestamp":1568328869480,"user_tz":-120,"elapsed":1105,"user":{"displayName":"Sayo Makinwa","photoUrl":"","userId":"02017936482338602411"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["import nltk\n","nltk.download(\"wordnet\")\n","\n","\n","from nltk.corpus import wordnet as wn\n","from tensorflow.keras.models import *\n","import os\n","import json\n","\n","from corpora import extract_eval_data\n","from my_utils import *"],"execution_count":5,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"N-zJujtZEydT","colab_type":"code","colab":{}},"source":["# def predict_babelnet(input_path : str, output_path : str, resources_path : str) -> None:\n","#     \"\"\"\n","#     DO NOT MODIFY THE SIGNATURE!\n","#     This is the skeleton of the prediction function.\n","#     The predict function will build your model, load the weights from the checkpoint and write a new file (output_path)\n","#     with your predictions in the \"<id> <BABELSynset>\" format (e.g. \"d000.s000.t000 bn:01234567n\").\n","    \n","#     The resources folder should contain everything you need to make the predictions. It is the \"resources\" folder in your submission.\n","    \n","#     N.B. DO NOT HARD CODE PATHS IN HERE. Use resource_path instead, otherwise we will not be able to run the code.\n","#     If you don't know what HARD CODING means see: https://en.wikipedia.org/wiki/Hard_coding\n","\n","#     :param input_path: the path of the input file to predict in the same format as Raganato's framework (XML files you downloaded).\n","#     :param output_path: the path of the output file (where you save your predictions)\n","#     :param resources_path: the path of the resources folder containing your model and stuff you might need.\n","#     :return: None\n","#     \"\"\"\n","#     if (not resources_path.endswith(\"/\")):\n","#         resources_path = resources_path+\"/\"\n","\n","#     input_folder_path = input_path\n","#     corpora_xml_path = input_path\n","    \n","#     if (input_path.endswith(\".xml\")):\n","#         input_folder_path = \"/\".join(input_path.split(\"/\")[0:-1])\n","\n","#     if (not input_folder_path.endswith(\"/\")):\n","#         input_folder_path = input_folder_path+\"/\"\n","\n","#     if (os.path.isfile(output_path)):\n","#         pred_file = output_path\n","#         output_folder_path = \"/\".join(output_path.split(\"/\")[0:-1])+\"/\"\n","#     elif (os.path.isdir(output_path)):\n","#         if (not output_path.endswith(\"/\")):\n","#             output_folder_path = output_path+\"/\"\n","#         pred_file = output_folder_path+\"pred_babelnet.txt\"\n","\n","#     model_name = resources_path+\"mult1_model.hdf5\"\n","#     print(\"LOADING RESOURCES...\")\n","#     model = load_model(model_name)\n","\n","#     #load the saved vocabularies\n","#     with open(resources_path+\"mult1_x_vocab.txt\", 'r') as file:\n","#         x_vocab = file.read()\n","#     x_vocab = json.loads(x_vocab)\n","\n","#     with open(resources_path+\"mult1_y_vocab.txt\", 'r') as file:\n","#         y_vocab = file.read()\n","#     y_vocab = json.loads(y_vocab)\n","#     id_to_words = {v:k for k, v in y_vocab.items()}\n","\n","#     bn2wn_mapping = load_bn2wn_mapping(resources_path+\"babelnet2wordnet.tsv\", True)\n","\n","#     #preparing the input data for prediction\n","#     print(\"PREPARING EVALUATION DATA FOR PREDICTION...\")\n","#     extract_eval_data(corpora_xml_path, resources_path)\n","\n","#     sentences = load_test_dataset(input_folder_path+\"sentences.txt\")\n","#     X_ = make_X(sentences, x_vocab)\n","\n","#     sentences_instances = load_sentence_instances(input_folder_path+\"inst_temp_file.txt\")\n","\n","#     #predicting and writing to file\n","#     print(\"Predicting (line by line) and writing to file... This may take a little while...\")\n","#     k = 0\n","#     inst_index = 0\n","#     x_len = X_.shape\n","#     with open(pred_file, \"w\") as file:\n","#         for x in X_:\n","#             if x.size != 0:\n","#                 x__ = np.expand_dims(x, axis=0)\n","#                 y_pred = model.predict(x__)\n","                \n","#                 y_pred = y_pred[0]\n","\n","#                 # This loop is meant to handle one instance at a time, saved temporarily in the inst_temp_file.txt file,\n","#                 # loaded into the sentences_instances variable, till there's no instance left\n","#                 while True:\n","#                     assoc_bn_synsets_vocab_pos = []\n","#                     if inst_index not in sentences_instances:\n","#                         break\n","\n","#                     inst = sentences_instances[inst_index]\n","#                     if (int(inst[2]) != k):\n","#                         break\n","#                     else:\n","#                         inst_index += 1\n","#                         inst_pos_in_sent = int(inst[3])\n","#                         inst_id = inst[1]\n","\n","#                         # Getting associated senses to the lemma of the instance selected\n","#                         inst_synsets = wn.synsets(inst[0])\n","#                         for wn_synset in inst_synsets:\n","#                             wn_synset_id = \"wn:\" + str(wn_synset.offset()).zfill(8) + wn_synset.pos()\n","#                             if wn_synset_id in bn2wn_mapping and bn2wn_mapping[wn_synset_id] in y_vocab:\n","#                                 assoc_bn_synsets_vocab_pos.append(y_vocab[bn2wn_mapping[wn_synset_id]])\n","                        \n","#                         # Finding argmax over all associated synsets, and defaulting to MFS (pre saved to the vocab) where there's none\n","#                         if assoc_bn_synsets_vocab_pos:\n","#                             pred_word = y_pred[0, inst_pos_in_sent]\n","#                             synset_probs = []\n","#                             for pos in assoc_bn_synsets_vocab_pos:\n","#                                 synset_probs.append(pred_word[pos])\n","\n","#                             pred_sense = id_to_words[assoc_bn_synsets_vocab_pos[np.argmax(synset_probs)]]\n","#                         else:\n","#                             #MFS word = inst[0]\n","#                             pred_sense = bn2wn_mapping[wn_mfs(inst[0])]\n","\n","#                         file.write(\"{} {}\\n\".format(inst_id, pred_sense))\n","            \n","#             k = k+1\n","#             if k % 100 < 1:\n","#                 print (\"%d/%d lines done... A little moment more and everything will be done! :)\" % (k,x_len[0]));\n","\n","#     del model, x_vocab, y_vocab, id_to_words, bn2wn_mapping, sentences, sentences_instances, X_, y_pred\n","#     print(\"Prediction complete!\")\n","\n","\n","# def predict_wordnet_domains(input_path : str, output_path : str, resources_path : str) -> None:\n","#     \"\"\"\n","#     DO NOT MODIFY THE SIGNATURE!\n","#     This is the skeleton of the prediction function.\n","#     The predict function will build your model, load the weights from the checkpoint and write a new file (output_path)\n","#     with your predictions in the \"<id> <wordnetDomain>\" format (e.g. \"d000.s000.t000 sport\").\n","\n","#     The resources folder should contain everything you need to make the predictions. It is the \"resources\" folder in your submission.\n","\n","#     N.B. DO NOT HARD CODE PATHS IN HERE. Use resource_path instead, otherwise we will not be able to run the code.\n","#     If you don't know what HARD CODING means see: https://en.wikipedia.org/wiki/Hard_coding\n","\n","#     :param input_path: the path of the input file to predict in the same format as Raganato's framework (XML files you downloaded).\n","#     :param output_path: the path of the output file (where you save your predictions)\n","#     :param resources_path: the path of the resources folder containing your model and stuff you might need.\n","#     :return: None\n","#     \"\"\"\n","#     if (not resources_path.endswith(\"/\")):\n","#         resources_path = resources_path+\"/\"\n","\n","#     input_folder_path = input_path\n","#     corpora_xml_path = input_path\n","    \n","#     if (input_path.endswith(\".xml\")):\n","#         input_folder_path = \"/\".join(input_path.split(\"/\")[0:-1])\n","\n","#     if (not input_folder_path.endswith(\"/\")):\n","#         input_folder_path = input_folder_path+\"/\"\n","\n","#     if (os.path.isfile(output_path)):\n","#         pred_file = output_path\n","#         output_folder_path = \"/\".join(output_path.split(\"/\")[0:-1])+\"/\"\n","#     elif (os.path.isdir(output_path)):\n","#         if (not output_path.endswith(\"/\")):\n","#             output_folder_path = output_path+\"/\"\n","#         pred_file = output_folder_path+\"pred_domains.txt\"\n","\n","#     model_name = resources_path+\"mult1_model.hdf5\"\n","#     print(\"LOADING RESOURCES...\")\n","#     model = load_model(model_name)\n","\n","#     #load the saved vocabularies\n","#     with open(resources_path+\"mult1_x_vocab.txt\", 'r') as file:\n","#         x_vocab = file.read()\n","#     x_vocab = json.loads(x_vocab)\n","\n","#     with open(resources_path+\"mult1_y_vocab.txt\", 'r') as file:\n","#         y_vocab = file.read()\n","#     y_vocab = json.loads(y_vocab)\n","#     id_to_words = {v:k for k, v in y_vocab.items()}\n","\n","#     bn2wn_mapping = load_bn2wn_mapping(resources_path+\"babelnet2wordnet.tsv\", True)\n","#     bn2dom_mapping = load_bn2wn_mapping(resources_path+\"babelnet2wndomains.tsv\")\n","\n","#     #preparing the input data for prediction\n","#     print(\"PREPARING EVALUATION DATA FOR PREDICTION...\")\n","#     extract_eval_data(corpora_xml_path, resources_path)\n","\n","#     sentences = load_test_dataset(input_folder_path+\"sentences.txt\")\n","#     X_ = make_X(sentences, x_vocab)\n","\n","#     sentences_instances = load_sentence_instances(input_folder_path+\"inst_temp_file.txt\")\n","\n","#     #predicting and writing to file\n","#     print(\"Predicting (line by line) and writing to file... This may take a little while...\")\n","#     k = 0\n","#     inst_index = 0\n","#     x_len = X_.shape\n","#     with open(pred_file, \"w\") as file:\n","#         for x in X_:\n","#             if x.size != 0:\n","#                 x__ = np.expand_dims(x, axis=0)\n","#                 y_pred = model.predict(x__)\n","#                 y_pred = y_pred[1]\n","                \n","#                 # This loop is meant to handle one instance at a time, saved temporarily in the inst_temp_file.txt file,\n","#                 # loaded into the sentences_instances variable, till there's no instance left\n","#                 while True:\n","#                     assoc_bn_synsets_vocab_pos = []\n","#                     if inst_index not in sentences_instances:\n","#                         break\n","\n","#                     inst = sentences_instances[inst_index]\n","#                     if (int(inst[2]) != k):\n","#                         break\n","#                     else:\n","#                         inst_index += 1\n","#                         inst_pos_in_sent = int(inst[3])\n","#                         inst_id = inst[1]\n","\n","#                         # Getting associated senses to the lemma of the instance selected\n","#                         inst_synsets = wn.synsets(inst[0])\n","#                         for wn_synset in inst_synsets:\n","#                             wn_synset_id = \"wn:\" + str(wn_synset.offset()).zfill(8) + wn_synset.pos()\n","                            \n","#                             if wn_synset_id in bn2wn_mapping:\n","#                                 bn_id = bn2wn_mapping[wn_synset_id]\n","#                                 if bn_id in bn2dom_mapping:\n","#                                     dom_name = \"dom:\"+bn2dom_mapping[bn_id]\n","                                \n","#                                     if dom_name in y_vocab:\n","#                                         assoc_bn_synsets_vocab_pos.append(y_vocab[dom_name])\n","                        \n","#                         # Finding argmax over all associated synsets, and defaulting to MFS (pre saved to the vocab) where there's none\n","#                         if assoc_bn_synsets_vocab_pos:\n","#                             pred_word = y_pred[0, inst_pos_in_sent]\n","#                             synset_probs = []\n","#                             for pos in assoc_bn_synsets_vocab_pos:\n","#                                 synset_probs.append(pred_word[pos])\n","\n","#                             pred_dom = id_to_words[assoc_bn_synsets_vocab_pos[np.argmax(synset_probs)]]\n","#                         else:\n","#                             #MFS word = inst[0]\n","#                             pred_sense = bn2wn_mapping[wn_mfs(inst[0])]\n","#                             if pred_sense in bn2dom_mapping:\n","#                                 pred_dom = bn2dom_mapping[pred_sense]\n","#                             else:\n","#                                 pred_dom = \"factotum\"\n","\n","#                         file.write(\"{} {}\\n\".format(inst_id, pred_dom[4:]))\n","            \n","#             k = k+1\n","#             if k % 100 < 1:\n","#                 print (\"%d/%d lines done... A little moment more and everything will be done! :)\" % (k,x_len[0]));\n","\n","#     del model, x_vocab, y_vocab, id_to_words, bn2wn_mapping, sentences, sentences_instances, X_, y_pred\n","#     print(\"Prediction complete!\")\n","\n","\n","# def predict_lexicographer(input_path : str, output_path : str, resources_path : str) -> None:\n","#     \"\"\"\n","#     DO NOT MODIFY THE SIGNATURE!\n","#     This is the skeleton of the prediction function.\n","#     The predict function will build your model, load the weights from the checkpoint and write a new file (output_path)\n","#     with your predictions in the \"<id> <lexicographerId>\" format (e.g. \"d000.s000.t000 noun.animal\").\n","\n","#     The resources folder should contain everything you need to make the predictions. It is the \"resources\" folder in your submission.\n","\n","#     N.B. DO NOT HARD CODE PATHS IN HERE. Use resource_path instead, otherwise we will not be able to run the code.\n","#     If you don't know what HARD CODING means see: https://en.wikipedia.org/wiki/Hard_coding\n","\n","#     :param input_path: the path of the input file to predict in the same format as Raganato's framework (XML files you downloaded).\n","#     :param output_path: the path of the output file (where you save your predictions)\n","#     :param resources_path: the path of the resources folder containing your model and stuff you might need.\n","#     :return: None\n","#     \"\"\"\n","#     if (not resources_path.endswith(\"/\")):\n","#         resources_path = resources_path+\"/\"\n","\n","#     input_folder_path = input_path\n","#     corpora_xml_path = input_path\n","    \n","#     if (input_path.endswith(\".xml\")):\n","#         input_folder_path = \"/\".join(input_path.split(\"/\")[0:-1])\n","\n","#     if (not input_folder_path.endswith(\"/\")):\n","#         input_folder_path = input_folder_path+\"/\"\n","\n","#     if (os.path.isfile(output_path)):\n","#         pred_file = output_path\n","#         output_folder_path = \"/\".join(output_path.split(\"/\")[0:-1])+\"/\"\n","#     elif (os.path.isdir(output_path)):\n","#         if (not output_path.endswith(\"/\")):\n","#             output_folder_path = output_path+\"/\"\n","#         pred_file = output_folder_path+\"pred_lex.txt\"\n","\n","#     model_name = resources_path+\"mult1_model.hdf5\"\n","#     print(\"LOADING RESOURCES...\")\n","#     model = load_model(model_name)\n","\n","#     #load the saved vocabularies\n","#     with open(resources_path+\"mult1_x_vocab.txt\", 'r') as file:\n","#         x_vocab = file.read()\n","#     x_vocab = json.loads(x_vocab)\n","\n","#     with open(resources_path+\"mult1_y_vocab.txt\", 'r') as file:\n","#         y_vocab = file.read()\n","#     y_vocab = json.loads(y_vocab)\n","#     id_to_words = {v:k for k, v in y_vocab.items()}\n","\n","#     bn2wn_mapping = load_bn2wn_mapping(resources_path+\"babelnet2wordnet.tsv\", True)\n","#     bn2lex_mapping = load_bn2wn_mapping(resources_path+\"babelnet2lexnames.tsv\")\n","\n","#     #preparing the input data for prediction\n","#     print(\"PREPARING EVALUATION DATA FOR PREDICTION...\")\n","#     extract_eval_data(corpora_xml_path, resources_path)\n","\n","#     sentences = load_test_dataset(input_folder_path+\"sentences.txt\")\n","#     X_ = make_X(sentences, x_vocab)\n","\n","#     sentences_instances = load_sentence_instances(input_folder_path+\"inst_temp_file.txt\")\n","\n","#     #predicting and writing to file\n","#     print(\"Predicting (line by line) and writing to file... This may take a little while...\")\n","#     k = 0\n","#     inst_index = 0\n","#     x_len = X_.shape\n","#     with open(pred_file, \"w\") as file:\n","#         for x in X_:\n","#             if x.size != 0:\n","#                 x__ = np.expand_dims(x, axis=0)\n","#                 y_pred = model.predict(x__)\n","#                 y_pred = y_pred[2]\n","\n","#                 # This loop is meant to handle one instance at a time, saved temporarily in the inst_temp_file.txt file,\n","#                 # loaded into the sentences_instances variable, till there's no instance left\n","#                 while True:\n","#                     assoc_bn_synsets_vocab_pos = []\n","#                     if inst_index not in sentences_instances:\n","#                         break\n","\n","#                     inst = sentences_instances[inst_index]\n","#                     if (int(inst[2]) != k):\n","#                         break\n","#                     else:\n","#                         inst_index += 1\n","#                         inst_pos_in_sent = int(inst[3])\n","#                         inst_id = inst[1]\n","\n","#                         # Getting associated senses to the lemma of the instance selected\n","#                         inst_synsets = wn.synsets(inst[0])\n","#                         for wn_synset in inst_synsets:\n","#                             wn_synset_id = \"wn:\" + str(wn_synset.offset()).zfill(8) + wn_synset.pos()\n","                            \n","#                             if wn_synset_id in bn2wn_mapping:\n","#                                 bn_id = bn2wn_mapping[wn_synset_id]\n","#                                 if bn_id in bn2lex_mapping:\n","#                                     lex_name = \"lex:\"+bn2lex_mapping[bn_id]\n","                                \n","#                                     if lex_name in y_vocab:\n","#                                         assoc_bn_synsets_vocab_pos.append(y_vocab[lex_name])\n","                                      \n","# #                             if wn_synset_id in bn2wn_mapping and bn2wn_mapping[wn_synset_id] in y_vocab:\n","# #                                 assoc_bn_synsets_vocab_pos.append(y_vocab[bn2wn_mapping[wn_synset_id]])\n","                        \n","#                         # Finding argmax over all associated synsets, and defaulting to MFS (pre saved to the vocab) where there's none\n","#                         if assoc_bn_synsets_vocab_pos:\n","#                             pred_word = y_pred[0, inst_pos_in_sent]\n","#                             synset_probs = []\n","#                             for pos in assoc_bn_synsets_vocab_pos:\n","#                                 synset_probs.append(pred_word[pos])\n","\n","#                             pred_lex = id_to_words[assoc_bn_synsets_vocab_pos[np.argmax(synset_probs)]]\n","                            \n","#                         else:\n","#                             #MFS word = inst[0]\n","#                             pred_sense = bn2wn_mapping[wn_mfs(inst[0])]\n","#                             if pred_sense in bn2lex_mapping:\n","#                                 pred_lex = bn2lex_mapping[pred_sense]\n","#                             else:\n","#                                 pred_lex = \"adj.all\"\n","\n","#                         file.write(\"{} {}\\n\".format(inst_id, pred_lex[4:]))\n","            \n","#             k = k+1\n","#             if k % 100 < 1:\n","#                 print (\"%d/%d lines done... A little moment more and everything will be done! :)\" % (k,x_len[0]));\n","\n","#     del model, x_vocab, y_vocab, id_to_words, bn2wn_mapping, sentences, sentences_instances, X_, y_pred\n","#     print(\"Prediction complete!\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ANBxNBoflMWi","colab_type":"code","colab":{}},"source":["def predict_babelnet(input_path : str, output_path : str, resources_path : str) -> None:\n","    \"\"\"\n","    DO NOT MODIFY THE SIGNATURE!\n","    This is the skeleton of the prediction function.\n","    The predict function will build your model, load the weights from the checkpoint and write a new file (output_path)\n","    with your predictions in the \"<id> <BABELSynset>\" format (e.g. \"d000.s000.t000 bn:01234567n\").\n","    \n","    The resources folder should contain everything you need to make the predictions. It is the \"resources\" folder in your submission.\n","    \n","    N.B. DO NOT HARD CODE PATHS IN HERE. Use resource_path instead, otherwise we will not be able to run the code.\n","    If you don't know what HARD CODING means see: https://en.wikipedia.org/wiki/Hard_coding\n","\n","    :param input_path: the path of the input file to predict in the same format as Raganato's framework (XML files you downloaded).\n","    :param output_path: the path of the output file (where you save your predictions)\n","    :param resources_path: the path of the resources folder containing your model and stuff you might need.\n","    :return: None\n","    \"\"\"\n","    if (not resources_path.endswith(\"/\")):\n","        resources_path = resources_path+\"/\"\n","\n","    input_folder_path = input_path\n","    corpora_xml_path = input_path\n","    \n","    if (input_path.endswith(\".xml\")):\n","        input_folder_path = \"/\".join(input_path.split(\"/\")[0:-1])\n","\n","    if (not input_folder_path.endswith(\"/\")):\n","        input_folder_path = input_folder_path+\"/\"\n","\n","    if (os.path.isfile(output_path)):\n","        pred_file = output_path\n","        output_folder_path = \"/\".join(output_path.split(\"/\")[0:-1])+\"/\"\n","    elif (os.path.isdir(output_path)):\n","        if (not output_path.endswith(\"/\")):\n","            output_folder_path = output_path+\"/\"\n","        pred_file = output_folder_path+\"pred_babelnet.txt\"\n","\n","    model_name = resources_path+\"mult1_model.hdf5\"\n","    print(\"LOADING RESOURCES...\")\n","    model = load_model(model_name)\n","\n","    #load the saved vocabularies\n","    with open(resources_path+\"mult1_x_vocab.txt\", 'r') as file:\n","        x_vocab = file.read()\n","    x_vocab = json.loads(x_vocab)\n","\n","    with open(resources_path+\"mult1_y_vocab.txt\", 'r') as file:\n","        y_vocab = file.read()\n","    y_vocab = json.loads(y_vocab)\n","    id_to_words = {v:k for k, v in y_vocab.items()}\n","\n","    bn2wn_mapping = load_bn2wn_mapping(resources_path+\"babelnet2wordnet.tsv\", True)\n","\n","    #preparing the input data for prediction\n","    print(\"PREPARING EVALUATION DATA FOR PREDICTION...\")\n","    extract_eval_data(corpora_xml_path, resources_path)\n","\n","    sentences = load_test_dataset(input_folder_path+\"sentences.txt\")\n","    X_ = make_X(sentences, x_vocab)\n","\n","    sentences_instances = load_sentence_instances(input_folder_path+\"inst_temp_file.txt\")\n","\n","    #predicting and writing to file\n","    print(\"Predicting (line by line) and writing to file... This may take a little while...\")\n","    k = 0\n","    inst_index = 0\n","    x_len = X_.shape\n","    with open(pred_file, \"w\") as file:\n","        for x in X_:\n","            if x.size != 0:\n","                x__ = np.expand_dims(x, axis=0)\n","                y_pred = model.predict(x__)\n","                \n","                y_pred = y_pred[0]\n","\n","                # This loop is meant to handle one instance at a time, saved temporarily in the inst_temp_file.txt file,\n","                # loaded into the sentences_instances variable, till there's no instance left\n","                while True:\n","                    assoc_bn_synsets_vocab_pos = []\n","                    if inst_index not in sentences_instances:\n","                        break\n","\n","                    inst = sentences_instances[inst_index]\n","                    if (int(inst[2]) != k):\n","                        break\n","                    else:\n","                        inst_index += 1\n","                        inst_pos_in_sent = int(inst[3])\n","                        inst_id = inst[1]\n","\n","                        # Getting associated senses to the lemma of the instance selected\n","                        inst_synsets = wn.synsets(inst[0])\n","                        for wn_synset in inst_synsets:\n","                            wn_synset_id = \"wn:\" + str(wn_synset.offset()).zfill(8) + wn_synset.pos()\n","                            if wn_synset_id in bn2wn_mapping and bn2wn_mapping[wn_synset_id] in y_vocab:\n","                                assoc_bn_synsets_vocab_pos.append(y_vocab[bn2wn_mapping[wn_synset_id]])\n","                        \n","                        # Finding argmax over all associated synsets, and defaulting to MFS (pre saved to the vocab) where there's none\n","                        if assoc_bn_synsets_vocab_pos:\n","                            pred_word = y_pred[0, inst_pos_in_sent]\n","                            synset_probs = []\n","                            for pos in assoc_bn_synsets_vocab_pos:\n","                                synset_probs.append(pred_word[pos])\n","\n","                            pred_sense = id_to_words[assoc_bn_synsets_vocab_pos[np.argmax(synset_probs)]]\n","                        else:\n","                            #MFS word = inst[0]\n","                            pred_sense = bn2wn_mapping[wn_mfs(inst[0])]\n","\n","                        file.write(\"{} {}\\n\".format(inst_id, pred_sense))\n","            \n","            k = k+1\n","            if k % 100 < 1:\n","                print (\"%d/%d lines done... A little moment more and everything will be done! :)\" % (k,x_len[0]));\n","\n","    del model, x_vocab, y_vocab, id_to_words, bn2wn_mapping, sentences, sentences_instances, X_, y_pred\n","    print(\"Prediction complete!\")\n","\n","\n","def predict_wordnet_domains(input_path : str, output_path : str, resources_path : str) -> None:\n","    \"\"\"\n","    DO NOT MODIFY THE SIGNATURE!\n","    This is the skeleton of the prediction function.\n","    The predict function will build your model, load the weights from the checkpoint and write a new file (output_path)\n","    with your predictions in the \"<id> <wordnetDomain>\" format (e.g. \"d000.s000.t000 sport\").\n","\n","    The resources folder should contain everything you need to make the predictions. It is the \"resources\" folder in your submission.\n","\n","    N.B. DO NOT HARD CODE PATHS IN HERE. Use resource_path instead, otherwise we will not be able to run the code.\n","    If you don't know what HARD CODING means see: https://en.wikipedia.org/wiki/Hard_coding\n","\n","    :param input_path: the path of the input file to predict in the same format as Raganato's framework (XML files you downloaded).\n","    :param output_path: the path of the output file (where you save your predictions)\n","    :param resources_path: the path of the resources folder containing your model and stuff you might need.\n","    :return: None\n","    \"\"\"\n","    if (not resources_path.endswith(\"/\")):\n","        resources_path = resources_path+\"/\"\n","\n","    input_folder_path = input_path\n","    corpora_xml_path = input_path\n","    \n","    if (input_path.endswith(\".xml\")):\n","        input_folder_path = \"/\".join(input_path.split(\"/\")[0:-1])\n","\n","    if (not input_folder_path.endswith(\"/\")):\n","        input_folder_path = input_folder_path+\"/\"\n","\n","    if (os.path.isfile(output_path)):\n","        pred_file = output_path\n","        output_folder_path = \"/\".join(output_path.split(\"/\")[0:-1])+\"/\"\n","    elif (os.path.isdir(output_path)):\n","        if (not output_path.endswith(\"/\")):\n","            output_folder_path = output_path+\"/\"\n","        pred_file = output_folder_path+\"pred_domains.txt\"\n","\n","    model_name = resources_path+\"mult1_model.hdf5\"\n","    print(\"LOADING RESOURCES...\")\n","    model = load_model(model_name)\n","\n","    #load the saved vocabularies\n","    with open(resources_path+\"mult1_x_vocab.txt\", 'r') as file:\n","        x_vocab = file.read()\n","    x_vocab = json.loads(x_vocab)\n","\n","    with open(resources_path+\"mult1_y_vocab.txt\", 'r') as file:\n","        y_vocab = file.read()\n","    y_vocab = json.loads(y_vocab)\n","    id_to_words = {v:k for k, v in y_vocab.items()}\n","\n","    bn2wn_mapping = load_bn2wn_mapping(resources_path+\"babelnet2wordnet.tsv\", True)\n","    bn2dom_mapping = load_bn2wn_mapping(resources_path+\"babelnet2wndomains.tsv\")\n","\n","    #preparing the input data for prediction\n","    print(\"PREPARING EVALUATION DATA FOR PREDICTION...\")\n","    extract_eval_data(corpora_xml_path, resources_path)\n","\n","    sentences = load_test_dataset(input_folder_path+\"sentences.txt\")\n","    X_ = make_X(sentences, x_vocab)\n","\n","    sentences_instances = load_sentence_instances(input_folder_path+\"inst_temp_file.txt\")\n","\n","    #predicting and writing to file\n","    print(\"Predicting (line by line) and writing to file... This may take a little while...\")\n","    k = 0\n","    inst_index = 0\n","    x_len = X_.shape\n","    with open(pred_file, \"w\") as file:\n","        for x in X_:\n","            if x.size != 0:\n","                x__ = np.expand_dims(x, axis=0)\n","                y_pred = model.predict(x__)\n","                y_pred = y_pred[0]\n","\n","                # This loop is meant to handle one instance at a time, saved temporarily in the inst_temp_file.txt file,\n","                # loaded into the sentences_instances variable, till there's no instance left\n","                while True:\n","                    assoc_bn_synsets_vocab_pos = []\n","                    if inst_index not in sentences_instances:\n","                        break\n","\n","                    inst = sentences_instances[inst_index]\n","                    if (int(inst[2]) != k):\n","                        break\n","                    else:\n","                        inst_index += 1\n","                        inst_pos_in_sent = int(inst[3])\n","                        inst_id = inst[1]\n","\n","                        # Getting associated senses to the lemma of the instance selected\n","                        inst_synsets = wn.synsets(inst[0])\n","                        for wn_synset in inst_synsets:\n","                            wn_synset_id = \"wn:\" + str(wn_synset.offset()).zfill(8) + wn_synset.pos()\n","                            if wn_synset_id in bn2wn_mapping and bn2wn_mapping[wn_synset_id] in y_vocab:\n","                                assoc_bn_synsets_vocab_pos.append(y_vocab[bn2wn_mapping[wn_synset_id]])\n","                        \n","                        # Finding argmax over all associated synsets, and defaulting to MFS (pre saved to the vocab) where there's none\n","                        if assoc_bn_synsets_vocab_pos:\n","                            pred_word = y_pred[0, inst_pos_in_sent]\n","                            synset_probs = []\n","                            for pos in assoc_bn_synsets_vocab_pos:\n","                                synset_probs.append(pred_word[pos])\n","\n","                            pred_sense = id_to_words[assoc_bn_synsets_vocab_pos[np.argmax(synset_probs)]]\n","                            if pred_sense in bn2dom_mapping:\n","                                pred_dom = bn2dom_mapping[pred_sense]\n","                            else:\n","                                pred_dom = \"factotum\"\n","                        else:\n","                            #MFS word = inst[0]\n","                            pred_sense = bn2wn_mapping[wn_mfs(inst[0])]\n","                            if pred_sense in bn2dom_mapping:\n","                                pred_dom = bn2dom_mapping[pred_sense]\n","                            else:\n","                                pred_dom = \"factotum\"\n","\n","                        file.write(\"{} {}\\n\".format(inst_id, pred_dom))\n","            \n","            k = k+1\n","            if k % 100 < 1:\n","                print (\"%d/%d lines done... A little moment more and everything will be done! :)\" % (k,x_len[0]));\n","\n","    del model, x_vocab, y_vocab, id_to_words, bn2wn_mapping, sentences, sentences_instances, X_, y_pred\n","    print(\"Prediction complete!\")\n","\n","\n","def predict_lexicographer(input_path : str, output_path : str, resources_path : str) -> None:\n","    \"\"\"\n","    DO NOT MODIFY THE SIGNATURE!\n","    This is the skeleton of the prediction function.\n","    The predict function will build your model, load the weights from the checkpoint and write a new file (output_path)\n","    with your predictions in the \"<id> <lexicographerId>\" format (e.g. \"d000.s000.t000 noun.animal\").\n","\n","    The resources folder should contain everything you need to make the predictions. It is the \"resources\" folder in your submission.\n","\n","    N.B. DO NOT HARD CODE PATHS IN HERE. Use resource_path instead, otherwise we will not be able to run the code.\n","    If you don't know what HARD CODING means see: https://en.wikipedia.org/wiki/Hard_coding\n","\n","    :param input_path: the path of the input file to predict in the same format as Raganato's framework (XML files you downloaded).\n","    :param output_path: the path of the output file (where you save your predictions)\n","    :param resources_path: the path of the resources folder containing your model and stuff you might need.\n","    :return: None\n","    \"\"\"\n","    if (not resources_path.endswith(\"/\")):\n","        resources_path = resources_path+\"/\"\n","\n","    input_folder_path = input_path\n","    corpora_xml_path = input_path\n","    \n","    if (input_path.endswith(\".xml\")):\n","        input_folder_path = \"/\".join(input_path.split(\"/\")[0:-1])\n","\n","    if (not input_folder_path.endswith(\"/\")):\n","        input_folder_path = input_folder_path+\"/\"\n","\n","    if (os.path.isfile(output_path)):\n","        pred_file = output_path\n","        output_folder_path = \"/\".join(output_path.split(\"/\")[0:-1])+\"/\"\n","    elif (os.path.isdir(output_path)):\n","        if (not output_path.endswith(\"/\")):\n","            output_folder_path = output_path+\"/\"\n","        pred_file = output_folder_path+\"pred_lex.txt\"\n","\n","    model_name = resources_path+\"mult1_model.hdf5\"\n","    print(\"LOADING RESOURCES...\")\n","    model = load_model(model_name)\n","\n","    #load the saved vocabularies\n","    with open(resources_path+\"mult1_x_vocab.txt\", 'r') as file:\n","        x_vocab = file.read()\n","    x_vocab = json.loads(x_vocab)\n","\n","    with open(resources_path+\"mult1_y_vocab.txt\", 'r') as file:\n","        y_vocab = file.read()\n","    y_vocab = json.loads(y_vocab)\n","    id_to_words = {v:k for k, v in y_vocab.items()}\n","\n","    bn2wn_mapping = load_bn2wn_mapping(resources_path+\"babelnet2wordnet.tsv\", True)\n","    bn2lex_mapping = load_bn2wn_mapping(resources_path+\"babelnet2lexnames.tsv\")\n","\n","    #preparing the input data for prediction\n","    print(\"PREPARING EVALUATION DATA FOR PREDICTION...\")\n","    extract_eval_data(corpora_xml_path, resources_path)\n","\n","    sentences = load_test_dataset(input_folder_path+\"sentences.txt\")\n","    X_ = make_X(sentences, x_vocab)\n","\n","    sentences_instances = load_sentence_instances(input_folder_path+\"inst_temp_file.txt\")\n","\n","    #predicting and writing to file\n","    print(\"Predicting (line by line) and writing to file... This may take a little while...\")\n","    k = 0\n","    inst_index = 0\n","    x_len = X_.shape\n","    with open(pred_file, \"w\") as file:\n","        for x in X_:\n","            if x.size != 0:\n","                x__ = np.expand_dims(x, axis=0)\n","                y_pred = model.predict(x__)\n","                y_pred = y_pred[0]\n","\n","                # This loop is meant to handle one instance at a time, saved temporarily in the inst_temp_file.txt file,\n","                # loaded into the sentences_instances variable, till there's no instance left\n","                while True:\n","                    assoc_bn_synsets_vocab_pos = []\n","                    if inst_index not in sentences_instances:\n","                        break\n","\n","                    inst = sentences_instances[inst_index]\n","                    if (int(inst[2]) != k):\n","                        break\n","                    else:\n","                        inst_index += 1\n","                        inst_pos_in_sent = int(inst[3])\n","                        inst_id = inst[1]\n","\n","                        # Getting associated senses to the lemma of the instance selected\n","                        inst_synsets = wn.synsets(inst[0])\n","                        for wn_synset in inst_synsets:\n","                            wn_synset_id = \"wn:\" + str(wn_synset.offset()).zfill(8) + wn_synset.pos()\n","                            if wn_synset_id in bn2wn_mapping and bn2wn_mapping[wn_synset_id] in y_vocab:\n","                                assoc_bn_synsets_vocab_pos.append(y_vocab[bn2wn_mapping[wn_synset_id]])\n","                        \n","                        # Finding argmax over all associated synsets, and defaulting to MFS (pre saved to the vocab) where there's none\n","                        if assoc_bn_synsets_vocab_pos:\n","                            pred_word = y_pred[0, inst_pos_in_sent]\n","                            synset_probs = []\n","                            for pos in assoc_bn_synsets_vocab_pos:\n","                                synset_probs.append(pred_word[pos])\n","\n","                            pred_sense = id_to_words[assoc_bn_synsets_vocab_pos[np.argmax(synset_probs)]]\n","                            if pred_sense in bn2lex_mapping:\n","                                pred_lex = bn2lex_mapping[pred_sense]\n","                            else:\n","                                pred_lex = \"adj.all\"\n","                        else:\n","                            #MFS word = inst[0]\n","                            pred_sense = bn2wn_mapping[wn_mfs(inst[0])]\n","                            if pred_sense in bn2lex_mapping:\n","                                pred_lex = bn2lex_mapping[pred_sense]\n","                            else:\n","                                pred_lex = \"adj.all\"\n","\n","                        file.write(\"{} {}\\n\".format(inst_id, pred_lex))\n","            \n","            k = k+1\n","            if k % 100 < 1:\n","                print (\"%d/%d lines done... A little moment more and everything will be done! :)\" % (k,x_len[0]));\n","\n","    del model, x_vocab, y_vocab, id_to_words, bn2wn_mapping, sentences, sentences_instances, X_, y_pred\n","    print(\"Prediction complete!\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kViXUjmsEydk","colab_type":"code","colab":{}},"source":["def run_tests1(score_file):\n","    \n","    resources_path = '../resources'\n","    bn2wn_mapping_file = '../resources/babelnet2wordnet.tsv'\n","    \n","    all_scores = []\n","    if (os.path.isfile(score_file)):\n","        with open(score_file, 'r') as file:\n","            for line in file:\n","                all_scores.append(line.split(\",\"))\n","        \n","    print(\"PREDICTING FOR SE2...\")\n","    se2_scores = ['SE2']\n","\n","    input_path = '../data/Evaluation_Datasets/senseval2/senseval2.data.xml'\n","    output_path = '../data/Evaluation_Datasets/senseval2'\n","    gold_file =  '../data/Evaluation_Datasets/senseval2/senseval2.gold.key.txt'\n","\n","    pred_babelnet = '../data/Evaluation_Datasets/senseval2/pred_babelnet.txt'\n","    pred_lex = '../data/Evaluation_Datasets/senseval2/pred_lex.txt'\n","    pred_domains = '../data/Evaluation_Datasets/senseval2/pred_domains.txt'\n","\n","    predict_babelnet(input_path, output_path, resources_path)\n","    score = score_predict1(pred_babelnet, gold_file, bn2wn_mapping_file)\n","    se2_scores.append(score*100)\n","\n","    predict_lexicographer(input_path, output_path, resources_path)\n","    score = score_predict_lex(pred_lex, gold_file, resources_path)\n","    se2_scores.append(score*100)\n","\n","    predict_wordnet_domains(input_path, output_path, resources_path)\n","    score = score_predict_dom(pred_domains, gold_file, resources_path)\n","    se2_scores.append(score*100)\n","\n","    all_scores.append(se2_scores)\n","    \n","\n","        \n","    with open(score_file, \"w\") as file:\n","        for scores in all_scores:\n","            file.write(\"{},{:.2f},{:.2f},{:.2f}\\n\".format(scores[0], float(scores[1]), float(scores[2]), float(scores[3])))\n","\n","\n","\n","def run_tests2(score_file):\n","    \n","    resources_path = '../resources'\n","    bn2wn_mapping_file = '../resources/babelnet2wordnet.tsv'\n","    \n","    all_scores = []\n","    if (os.path.isfile(score_file)):\n","        with open(score_file, 'r') as file:\n","            for line in file:\n","                all_scores.append(line.split(\",\"))\n","    \n","\n","    print(\"\\n\\nPREDICTING FOR SE3...\")\n","    se3_scores = ['SE3_(Dev)']\n","\n","    input_path = '../data/Evaluation_Datasets/senseval3/senseval3.data.xml'\n","    output_path = '../data/Evaluation_Datasets/senseval3'\n","    gold_file =  '../data/Evaluation_Datasets/senseval3/senseval3.gold.key.txt'\n","\n","    pred_babelnet = '../data/Evaluation_Datasets/senseval3/pred_babelnet.txt'\n","    pred_lex = '../data/Evaluation_Datasets/senseval3/pred_lex.txt'\n","    pred_domains = '../data/Evaluation_Datasets/senseval3/pred_domains.txt'\n","\n","    predict_babelnet(input_path, output_path, resources_path)\n","    score = score_predict1(pred_babelnet, gold_file, bn2wn_mapping_file)\n","    se3_scores.append(score*100)\n","\n","    predict_lexicographer(input_path, output_path, resources_path)\n","    score = score_predict_lex(pred_lex, gold_file, resources_path)\n","    se3_scores.append(score*100)\n","\n","    predict_wordnet_domains(input_path, output_path, resources_path)\n","    score = score_predict_dom(pred_domains, gold_file, resources_path)\n","    se3_scores.append(score*100)\n","\n","    all_scores.append(se3_scores)\n","    \n","    \n","    ###########################################################\n","        \n","    with open(score_file, \"w\") as file:\n","        for scores in all_scores:\n","            file.write(\"{},{:.2f},{:.2f},{:.2f}\\n\".format(scores[0], float(scores[1]), float(scores[2]), float(scores[3])))\n","\n","\n","\n","def run_tests3(score_file):\n","    \n","    resources_path = '../resources'\n","    bn2wn_mapping_file = '../resources/babelnet2wordnet.tsv'\n","    \n","    all_scores = []\n","    if (os.path.isfile(score_file)):\n","        with open(score_file, 'r') as file:\n","            for line in file:\n","                all_scores.append(line.split(\",\"))\n","                \n","    \n","    print(\"\\n\\nPREDICTING FOR SE07...\")\n","    se07_scores = ['SE07']\n","    \n","    input_path = '../data/Evaluation_Datasets/semeval2007/semeval2007.data.xml'\n","    output_path = '../data/Evaluation_Datasets/semeval2007'\n","    gold_file =  '../data/Evaluation_Datasets/semeval2007/semeval2007.gold.key.txt'\n","    \n","    pred_babelnet = '../data/Evaluation_Datasets/semeval2007/pred_babelnet.txt'\n","    pred_lex = '../data/Evaluation_Datasets/semeval2007/pred_lex.txt'\n","    pred_domains = '../data/Evaluation_Datasets/semeval2007/pred_domains.txt'\n","\n","    predict_babelnet(input_path, output_path, resources_path)\n","    score = score_predict1(pred_babelnet, gold_file, bn2wn_mapping_file)\n","    se07_scores.append(score*100)\n","    \n","    predict_lexicographer(input_path, output_path, resources_path)\n","    score = score_predict_lex(pred_lex, gold_file, resources_path)\n","    se07_scores.append(score*100)\n","    \n","    predict_wordnet_domains(input_path, output_path, resources_path)\n","    score = score_predict_dom(pred_domains, gold_file, resources_path)\n","    se07_scores.append(score*100)\n","    \n","    all_scores.append(se07_scores)\n","    \n","    #########################################################\n","    \n","    with open(score_file, \"w\") as file:\n","        for scores in all_scores:\n","            file.write(\"{},{:.2f},{:.2f},{:.2f}\\n\".format(scores[0], float(scores[1]), float(scores[2]), float(scores[3])))\n","            \n","            \n","\n","            \n","def run_tests4(score_file):\n","    \n","    resources_path = '../resources'\n","    bn2wn_mapping_file = '../resources/babelnet2wordnet.tsv'\n","    \n","    all_scores = []\n","    if (os.path.isfile(score_file)):\n","        with open(score_file, 'r') as file:\n","            for line in file:\n","                all_scores.append(line.split(\",\"))\n","\n","    \n","    print(\"\\n\\nPREDICTING FOR SE13...\")\n","    se13_scores = ['SE13']\n","    \n","    input_path = '../data/Evaluation_Datasets/semeval2013/semeval2013.data.xml'\n","    output_path = '../data/Evaluation_Datasets/semeval2013'\n","    gold_file =  '../data/Evaluation_Datasets/semeval2013/semeval2013.gold.key.txt'\n","    \n","    pred_babelnet = '../data/Evaluation_Datasets/semeval2013/pred_babelnet.txt'\n","    pred_lex = '../data/Evaluation_Datasets/semeval2013/pred_lex.txt'\n","    pred_domains = '../data/Evaluation_Datasets/semeval2013/pred_domains.txt'\n","\n","    predict_babelnet(input_path, output_path, resources_path)\n","    score = score_predict1(pred_babelnet, gold_file, bn2wn_mapping_file)\n","    se13_scores.append(score*100)\n","    \n","    predict_lexicographer(input_path, output_path, resources_path)\n","    score = score_predict_lex(pred_lex, gold_file, resources_path)\n","    se13_scores.append(score*100)\n","    \n","    predict_wordnet_domains(input_path, output_path, resources_path)\n","    score = score_predict_dom(pred_domains, gold_file, resources_path)\n","    se13_scores.append(score*100)\n","    \n","    all_scores.append(se13_scores)\n","    \n","    #########################################################\n","    \n","    \n","    with open(score_file, \"w\") as file:\n","        for scores in all_scores:\n","            file.write(\"{},{:.2f},{:.2f},{:.2f}\\n\".format(scores[0], float(scores[1]), float(scores[2]), float(scores[3])))\n","\n","\n","\n","    \n","    \n","\n","def run_tests5(score_file):\n","    \n","    resources_path = '../resources'\n","    bn2wn_mapping_file = '../resources/babelnet2wordnet.tsv'\n","    \n","    all_scores = []\n","    if (os.path.isfile(score_file)):\n","        with open(score_file, 'r') as file:\n","            for line in file:\n","                all_scores.append(line.split(\",\"))\n","                \n","    \n","    print(\"\\n\\nPREDICTING FOR SE15...\")\n","    se15_scores = ['SE15']\n","    \n","    input_path = '../data/Evaluation_Datasets/semeval2015/semeval2015.data.xml'\n","    output_path = '../data/Evaluation_Datasets/semeval2015'\n","    gold_file =  '../data/Evaluation_Datasets/semeval2015/semeval2015.gold.key.txt'\n","    \n","    pred_babelnet = '../data/Evaluation_Datasets/semeval2015/pred_babelnet.txt'\n","    pred_lex = '../data/Evaluation_Datasets/semeval2015/pred_lex.txt'\n","    pred_domains = '../data/Evaluation_Datasets/semeval2015/pred_domains.txt'\n","\n","    predict_babelnet(input_path, output_path, resources_path)\n","    score = score_predict1(pred_babelnet, gold_file, bn2wn_mapping_file)\n","    se15_scores.append(score*100)\n","    \n","    predict_lexicographer(input_path, output_path, resources_path)\n","    score = score_predict_lex(pred_lex, gold_file, resources_path)\n","    se15_scores.append(score*100)\n","    \n","    predict_wordnet_domains(input_path, output_path, resources_path)\n","    score = score_predict_dom(pred_domains, gold_file, resources_path)\n","    se15_scores.append(score*100)\n","    \n","    all_scores.append(se15_scores)\n","    \n","    \n","    with open(score_file, \"w\") as file:\n","        for scores in all_scores:\n","            file.write(\"{},{:.2f},{:.2f},{:.2f}\\n\".format(scores[0], float(scores[1]), float(scores[2]), float(scores[3])))\n","\n","\n","\n","\n","\n","def run_tests6(score_file):\n","    \n","    resources_path = '../resources'\n","    bn2wn_mapping_file = '../resources/babelnet2wordnet.tsv'\n","    \n","    all_scores = []\n","    if (os.path.isfile(score_file)):\n","        with open(score_file, 'r') as file:\n","            for line in file:\n","                all_scores.append(line.split(\",\"))\n","    \n","    \n","    print(\"\\n\\nPREDICTING FOR ALL...\")\n","    se_ALL_scores = ['ALL']\n","    \n","    input_path = '../data/Evaluation_Datasets/ALL/ALL.data.xml'\n","    output_path = '../data/Evaluation_Datasets/ALL'\n","    gold_file =  '../data/Evaluation_Datasets/ALL/ALL.gold.key.txt'\n","    \n","    pred_babelnet = '../data/Evaluation_Datasets/ALL/pred_babelnet.txt'\n","    pred_lex = '../data/Evaluation_Datasets/ALL/pred_lex.txt'\n","    pred_domains = '../data/Evaluation_Datasets/ALL/pred_domains.txt'\n","\n","    predict_babelnet(input_path, output_path, resources_path)\n","    score = score_predict1(pred_babelnet, gold_file, bn2wn_mapping_file)\n","    se_ALL_scores.append(score*100)\n","    \n","    predict_lexicographer(input_path, output_path, resources_path)\n","    score = score_predict_lex(pred_lex, gold_file, resources_path)\n","    se_ALL_scores.append(score*100)\n","    \n","    predict_wordnet_domains(input_path, output_path, resources_path)\n","    score = score_predict_dom(pred_domains, gold_file, resources_path)\n","    se_ALL_scores.append(score*100)\n","    \n","    all_scores.append(se_ALL_scores)\n","    \n","    \n","    with open(score_file, \"w\") as file:\n","        for scores in all_scores:\n","            file.write(\"{},{:.2f},{:.2f},{:.2f}\\n\".format(scores[0], float(scores[1]), float(scores[2]), float(scores[3])))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xZZxxvNflGq1","colab_type":"text"},"source":["## Some"]},{"cell_type":"code","metadata":{"id":"gFRKAvnUuoNw","colab_type":"code","outputId":"027dd3d1-5aa4-4b21-cfc0-1f5d18ffe2e4","executionInfo":{"status":"ok","timestamp":1568329177083,"user_tz":-120,"elapsed":296770,"user":{"displayName":"Sayo Makinwa","photoUrl":"","userId":"02017936482338602411"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["#run_tests1(\"../resources/scores2.csv\")\n","#run_tests2(\"../resources/scores2.csv\")\n","#run_tests3(\"../resources/scores2.csv\")\n","#run_tests4(\"../resources/scores2.csv\")\n","#run_tests5(\"../resources/scores2.csv\")\n","run_tests6(\"../resources/scores2.csv\")"],"execution_count":9,"outputs":[{"output_type":"stream","text":["\n","\n","PREDICTING FOR ALL...\n","LOADING RESOURCES...\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py:3794: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","PREPARING EVALUATION DATA FOR PREDICTION...\n","1,173 sentences extracted...\n","Evaluation data extraction completed\n","Predicting (line by line) and writing to file... This may take a little while...\n","100/1173 lines done... A little moment more and everything will be done! :)\n","200/1173 lines done... A little moment more and everything will be done! :)\n","300/1173 lines done... A little moment more and everything will be done! :)\n","400/1173 lines done... A little moment more and everything will be done! :)\n","500/1173 lines done... A little moment more and everything will be done! :)\n","600/1173 lines done... A little moment more and everything will be done! :)\n","700/1173 lines done... A little moment more and everything will be done! :)\n","800/1173 lines done... A little moment more and everything will be done! :)\n","900/1173 lines done... A little moment more and everything will be done! :)\n","1000/1173 lines done... A little moment more and everything will be done! :)\n","1100/1173 lines done... A little moment more and everything will be done! :)\n","Prediction complete!\n","LOADING RESOURCES...\n","PREPARING EVALUATION DATA FOR PREDICTION...\n","1,173 sentences extracted...\n","Evaluation data extraction completed\n","Predicting (line by line) and writing to file... This may take a little while...\n","100/1173 lines done... A little moment more and everything will be done! :)\n","200/1173 lines done... A little moment more and everything will be done! :)\n","300/1173 lines done... A little moment more and everything will be done! :)\n","400/1173 lines done... A little moment more and everything will be done! :)\n","500/1173 lines done... A little moment more and everything will be done! :)\n","600/1173 lines done... A little moment more and everything will be done! :)\n","700/1173 lines done... A little moment more and everything will be done! :)\n","800/1173 lines done... A little moment more and everything will be done! :)\n","900/1173 lines done... A little moment more and everything will be done! :)\n","1000/1173 lines done... A little moment more and everything will be done! :)\n","1100/1173 lines done... A little moment more and everything will be done! :)\n","Prediction complete!\n","LOADING RESOURCES...\n","PREPARING EVALUATION DATA FOR PREDICTION...\n","1,173 sentences extracted...\n","Evaluation data extraction completed\n","Predicting (line by line) and writing to file... This may take a little while...\n","100/1173 lines done... A little moment more and everything will be done! :)\n","200/1173 lines done... A little moment more and everything will be done! :)\n","300/1173 lines done... A little moment more and everything will be done! :)\n","400/1173 lines done... A little moment more and everything will be done! :)\n","500/1173 lines done... A little moment more and everything will be done! :)\n","600/1173 lines done... A little moment more and everything will be done! :)\n","700/1173 lines done... A little moment more and everything will be done! :)\n","800/1173 lines done... A little moment more and everything will be done! :)\n","900/1173 lines done... A little moment more and everything will be done! :)\n","1000/1173 lines done... A little moment more and everything will be done! :)\n","1100/1173 lines done... A little moment more and everything will be done! :)\n","Prediction complete!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GcvSomSxuuqx","colab_type":"text"},"source":["##Results"]},{"cell_type":"code","metadata":{"id":"59FcqbZsoSi-","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YZTqQPHeuoIP","colab_type":"code","outputId":"f2640af4-2f3e-49df-da59-815aa1c80f9f","executionInfo":{"status":"ok","timestamp":1568314751084,"user_tz":-120,"elapsed":211882,"user":{"displayName":"Sayo Makinwa","photoUrl":"","userId":"02017936482338602411"}},"colab":{"base_uri":"https://localhost:8080/","height":235}},"source":["all_scores_pd = pd.read_csv('../resources/scores.csv', names=['Babelnet', 'Lex', 'Domain'])\n","all_scores_pd"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Babelnet</th>\n","      <th>Lex</th>\n","      <th>Domain</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>SE2</th>\n","      <td>61.44</td>\n","      <td>79.18</td>\n","      <td>63.72</td>\n","    </tr>\n","    <tr>\n","      <th>SE3_(Dev)</th>\n","      <td>61.89</td>\n","      <td>77.41</td>\n","      <td>70.54</td>\n","    </tr>\n","    <tr>\n","      <th>SE07</th>\n","      <td>54.73</td>\n","      <td>67.69</td>\n","      <td>88.79</td>\n","    </tr>\n","    <tr>\n","      <th>SE13</th>\n","      <td>57.85</td>\n","      <td>70.26</td>\n","      <td>74.39</td>\n","    </tr>\n","    <tr>\n","      <th>SE15</th>\n","      <td>54.50</td>\n","      <td>71.62</td>\n","      <td>63.70</td>\n","    </tr>\n","    <tr>\n","      <th>ALL</th>\n","      <td>59.34</td>\n","      <td>74.92</td>\n","      <td>69.45</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["           Babelnet    Lex  Domain\n","SE2           61.44  79.18   63.72\n","SE3_(Dev)     61.89  77.41   70.54\n","SE07          54.73  67.69   88.79\n","SE13          57.85  70.26   74.39\n","SE15          54.50  71.62   63.70\n","ALL           59.34  74.92   69.45"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"63fZcMDR1Lw1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":235},"outputId":"7ea301d5-a8b5-4f3b-fe4a-cdd389ac7123","executionInfo":{"status":"ok","timestamp":1568329206317,"user_tz":-120,"elapsed":941,"user":{"displayName":"Sayo Makinwa","photoUrl":"","userId":"02017936482338602411"}}},"source":["all_scores_pd = pd.read_csv('../resources/scores2.csv', names=['Babelnet', 'Lex', 'Domain'])\n","all_scores_pd"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Babelnet</th>\n","      <th>Lex</th>\n","      <th>Domain</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>SE2</th>\n","      <td>61.44</td>\n","      <td>78.31</td>\n","      <td>87.20</td>\n","    </tr>\n","    <tr>\n","      <th>SE3_(Dev)</th>\n","      <td>61.89</td>\n","      <td>76.54</td>\n","      <td>85.35</td>\n","    </tr>\n","    <tr>\n","      <th>SE07</th>\n","      <td>54.73</td>\n","      <td>71.21</td>\n","      <td>85.49</td>\n","    </tr>\n","    <tr>\n","      <th>SE13</th>\n","      <td>57.85</td>\n","      <td>69.71</td>\n","      <td>74.03</td>\n","    </tr>\n","    <tr>\n","      <th>SE15</th>\n","      <td>54.50</td>\n","      <td>69.67</td>\n","      <td>78.77</td>\n","    </tr>\n","    <tr>\n","      <th>ALL</th>\n","      <td>59.34</td>\n","      <td>74.25</td>\n","      <td>82.45</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["           Babelnet    Lex  Domain\n","SE2           61.44  78.31   87.20\n","SE3_(Dev)     61.89  76.54   85.35\n","SE07          54.73  71.21   85.49\n","SE13          57.85  69.71   74.03\n","SE15          54.50  69.67   78.77\n","ALL           59.34  74.25   82.45"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"RL1TuqkxtIW6","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}