{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"colab":{"name":"model_C_with_attention_layer.ipynb","version":"0.3.2","provenance":[{"file_id":"1ztcJt_LgDIH_Umi5lGsDDBDB86ju8P36","timestamp":1567821932880},{"file_id":"1UCBYAjzB3OqMVVwR6-gJ23VgMpVvN6Ql","timestamp":1567817661516}],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"xg-wSpUBU7gx","colab_type":"text"},"source":["## Mount Drive and Imports"]},{"cell_type":"code","metadata":{"id":"NKtsqriiKxYd","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nRTrAorEKyu5","colab_type":"code","colab":{}},"source":["%cd /content/drive/My Drive/nlp_hw3/code"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OxdkmYCxEybs","colab_type":"code","colab":{}},"source":["import os\n","import numpy as np\n","from typing import Tuple, List, Dict\n","\n","import tensorflow as tf\n","import keras\n","import keras as K\n","from keras.layers import *\n","from keras.models import *\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.callbacks import TensorBoard\n","from keras.regularizers import l2\n","from keras.optimizers import Adam\n","from tensorflow.python.eager import context\n","\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","import json\n","import pandas as pd\n","\n","import my_utils as utils\n","import corpora"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s2DfqlkrVuTu","colab_type":"text"},"source":["## Preprocessing"]},{"cell_type":"code","metadata":{"id":"042xGwFcEyb1","colab_type":"code","colab":{}},"source":["def load_train_dataset(input_path: str, y_path: str) -> Tuple[List[str], List[str]]:\n","    \"\"\"\n","    :param input_path; Path to the input dataset\n","    :param label_path; Path to the file containing the corresponding labels for the input dataset\n","    :return sentences; List of sentences in input_file\n","    :return labels; List of corresponding word segment codes in label_path. Same len as sentences\n","    \"\"\"\n","    sentences = []\n","    k = 0\n","    with open(input_path, \"r\", encoding=\"utf-8-sig\") as file:\n","        for line in file:\n","            k += 1\n","            sentences.append(line.strip())\n","#             if (k >= 4000):\n","#                 break\n","\n","    y = []\n","    k = 0\n","    with open(y_path, \"r\", encoding=\"utf-8-sig\") as file:\n","        for line in file:\n","            k += 1\n","            y.append(line.strip())\n","#             if (k >= 4000):\n","#                 break\n","\n","    return sentences, y\n","\n","\n","def make_X_vocab(sentences: List[str]) -> Dict[str, int]:\n","    '''\n","    :param sentences; List of input sentences from the dataset\n","    :return unigrams_vocab; Dictionary from unigram to int\n","    :return bigrams_vocab; Dictionary from bigram to int\n","    '''\n","    vocab = {\"UNK\": 0}\n","\n","    for sentence in sentences:\n","        for word in sentence.split():\n","            if word not in vocab:\n","                vocab[word] = len(vocab)\n","    \n","    return vocab\n","\n","\n","def make_Y_vocab(y: List[str], min_count=3) -> Dict[str, int]:\n","    \"\"\"\n","    :param labels; List of label codes\n","    :return labels_vocab; Dictionary from label code to int \n","    \"\"\"\n","    y_vocab = {\"UNK\": 0, \"OTHERS\": 1}\n","    words_freq = {}\n","    \n","    for y_line in y:\n","        for y_word in y_line.split():\n","            if \"bn:\" in y_word:\n","                if y_word not in y_vocab:\n","                    y_vocab[y_word] = len(y_vocab)\n","            else:\n","                if y_word not in words_freq:\n","                    words_freq[y_word] = 1\n","                else:\n","                    words_freq[y_word] += 1\n","    \n","    stpwrds = list(set(stopwords.words('english')))\n","    for k, v in words_freq.items():\n","        if v >= min_count and v not in stpwrds:\n","            y_vocab[k] = len(y_vocab)\n","    \n","    return y_vocab\n","\n","def make_Y(output: List[str], output_vocab: Dict[str, int]) -> np.ndarray:\n","    \"\"\"\n","    :param labels; List of word segment codes, line by line\n","    :param labels_vocab; Label codes vocab\n","    :return y; Vector of label code indices\n","    \"\"\"\n","    y = []\n","    for output_line in output:\n","        y_temp = []\n","        for single_output in output_line.split():\n","            if single_output in output_vocab:\n","                y_temp.append( output_vocab[single_output])\n","            else:\n","                y_temp.append( output_vocab[\"OTHERS\"])\n","        y.append(np.array(y_temp))\n","    \n","    return np.array(y)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mAtVKDjHU2wj","colab_type":"code","colab":{}},"source":["train_data_path = '../data/Training_Corpora/semcor'\n","test_data_path = '../data/Evaluation_Datasets/senseval3'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nDelCEWdWFxz","colab_type":"code","colab":{}},"source":["#Parse training data\n","corpora_xml_path = train_data_path + '/semcor.data.xml'\n","gold_mapping_path = train_data_path + '/semcor.gold.key.txt'\n","resources_path = '../resources/'\n","\n","corpora.extract_training_data(corpora_xml_path, gold_mapping_path, resources_path, train_data_path)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KSpdbW_QWFkd","colab_type":"code","colab":{}},"source":["#Parse validation data\n","corpora_xml_path = test_data_path + '/senseval3.data.xml'\n","gold_mapping_path = test_data_path + '/senseval3.gold.key.txt'\n","resources_path = '../resources/'\n","\n","corpora.extract_training_data(corpora_xml_path, gold_mapping_path, resources_path, test_data_path)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yVV_9dxlEyb4","colab_type":"code","colab":{}},"source":["sentences, y = load_train_dataset(train_data_path+\"/trainX.txt\", train_data_path+\"/trainy.txt\")\n","test_sentences, test_y = load_train_dataset(test_data_path+\"/trainX.txt\", test_data_path+\"/trainy.txt\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-EJ1fHSzEyb7","colab_type":"code","outputId":"f6be0367-13e3-4893-e52a-de13780674ca","executionInfo":{"status":"ok","timestamp":1568337943980,"user_tz":-120,"elapsed":616,"user":{"displayName":"Sayo Makinwa","photoUrl":"","userId":"02017936482338602411"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["print(len(sentences))\n","print(sentences[0])"],"execution_count":7,"outputs":[{"output_type":"stream","text":["37168\n","How long has it been since you reviewed the objectives of your benefit and service program\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"t1fD1tgDEycC","colab_type":"code","outputId":"b2b17e82-a4e6-45c7-d160-03cc1e9ae747","executionInfo":{"status":"ok","timestamp":1568337945959,"user_tz":-120,"elapsed":1728,"user":{"displayName":"Sayo Makinwa","photoUrl":"","userId":"02017936482338602411"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["print(len(y))\n","print(y[0])"],"execution_count":8,"outputs":[{"output_type":"stream","text":["37168\n","how bn:00106124a have it bn:00083181v since you bn:00092618v the bn:00002179n of you bn:00009904n and bn:00070654n bn:00064646n\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HgCzdmbLEycF","colab_type":"code","outputId":"bf4fed90-02e0-4ff4-cf3d-f63b8f6df3a2","executionInfo":{"status":"ok","timestamp":1568337945963,"user_tz":-120,"elapsed":1136,"user":{"displayName":"Sayo Makinwa","photoUrl":"","userId":"02017936482338602411"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["vocab = make_X_vocab(sentences + test_sentences)\n","print(len(vocab))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["48989\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ns1BMFwif7xp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"e4e98f84-a330-4ef3-eaee-66df69dc8920","executionInfo":{"status":"ok","timestamp":1568337947244,"user_tz":-120,"elapsed":1527,"user":{"displayName":"Sayo Makinwa","photoUrl":"","userId":"02017936482338602411"}}},"source":["import nltk\n","nltk.download('stopwords')"],"execution_count":10,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"nvCW3zRXZeFS","colab_type":"code","colab":{}},"source":["from nltk.corpus import stopwords"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LPRR9TA7EycH","colab_type":"code","outputId":"10f66e4c-a756-497f-da51-748bdd3cf7c1","executionInfo":{"status":"ok","timestamp":1568337949315,"user_tz":-120,"elapsed":2331,"user":{"displayName":"Sayo Makinwa","photoUrl":"","userId":"02017936482338602411"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["y_vocab = make_Y_vocab(y + test_y)\n","print(len(y_vocab))"],"execution_count":12,"outputs":[{"output_type":"stream","text":["34505\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iD6TeqJREycR","colab_type":"code","colab":{}},"source":["X = utils.make_X(sentences, vocab)\n","y_array = make_Y(y, y_vocab)\n","\n","X_test = utils.make_X(test_sentences, vocab)\n","y_test_array = make_Y(test_y, y_vocab)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DF5m68nmEycT","colab_type":"code","outputId":"4a8b68c0-24e6-4d9d-a94c-68bb268aa29b","executionInfo":{"status":"ok","timestamp":1568324107994,"user_tz":-120,"elapsed":1297,"user":{"displayName":"Sayo Makinwa","photoUrl":"","userId":"02017936482338602411"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["X.shape"],"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(37168,)"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"id":"j7fg-usHEycZ","colab_type":"code","colab":{}},"source":["train_x = pad_sequences(X, truncating='pre', padding='post', maxlen=30)\n","train_y = pad_sequences(y_array, truncating='pre', padding='post', maxlen=30)\n","\n","dev_x = pad_sequences(X_test, truncating='pre', padding='post', maxlen=30)\n","dev_y = pad_sequences(y_test_array, truncating='pre', padding='post', maxlen=30)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NmXFcuk4Eycc","colab_type":"code","outputId":"31caafdb-0d2f-4e44-e016-f4b1ea96f99a","executionInfo":{"status":"ok","timestamp":1568324109495,"user_tz":-120,"elapsed":1380,"user":{"displayName":"Sayo Makinwa","photoUrl":"","userId":"02017936482338602411"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["print(train_x.shape)\n","print(train_y.shape)"],"execution_count":34,"outputs":[{"output_type":"stream","text":["(37168, 30)\n","(37168, 30)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YIojgrqREycg","colab_type":"code","outputId":"88480336-c430-4ebf-f9e4-ae30a9945780","executionInfo":{"status":"ok","timestamp":1568324110654,"user_tz":-120,"elapsed":1815,"user":{"displayName":"Sayo Makinwa","photoUrl":"","userId":"02017936482338602411"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["train_y = train_y.reshape((*train_y.shape, 1))\n","dev_y = dev_y.reshape((*dev_y.shape, 1))\n","print(train_y.shape)\n","print(dev_y.shape)"],"execution_count":35,"outputs":[{"output_type":"stream","text":["(37168, 30, 1)\n","(352, 30, 1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ABjb_KPbEyc1","colab_type":"code","outputId":"79db0738-848f-41f6-82d5-09d656060671","executionInfo":{"status":"ok","timestamp":1568324110655,"user_tz":-120,"elapsed":792,"user":{"displayName":"Sayo Makinwa","photoUrl":"","userId":"02017936482338602411"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["print(train_x.shape)\n","print(train_y.shape)\n","print(dev_x.shape)\n","print(dev_y.shape)"],"execution_count":36,"outputs":[{"output_type":"stream","text":["(37168, 30)\n","(37168, 30, 1)\n","(352, 30)\n","(352, 30, 1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MkMfJ1HTYD9f","colab_type":"text"},"source":["## Building the model"]},{"cell_type":"code","metadata":{"id":"PCHC0G38U682","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":190},"outputId":"e713b4a9-ea15-4993-e40a-a651a988ee75","executionInfo":{"status":"ok","timestamp":1568324116598,"user_tz":-120,"elapsed":5006,"user":{"displayName":"Sayo Makinwa","photoUrl":"","userId":"02017936482338602411"}}},"source":["pip install keras-self-attention"],"execution_count":37,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: keras-self-attention in /usr/local/lib/python3.6/dist-packages (0.42.0)\n","Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-self-attention) (2.2.5)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-self-attention) (1.16.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (3.13)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (2.8.0)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.3.1)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.12.0)\n","Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.1.0)\n","Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.0.8)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"klbho2v1U-93","colab_type":"code","colab":{}},"source":["from keras_self_attention import SeqSelfAttention"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gd-bBhRDEyc-","colab_type":"code","colab":{}},"source":["vocab_size = len(vocab)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9uF52K_8EydA","colab_type":"code","colab":{}},"source":["#This class helps with logging\n","\n","class TrainValTensorBoard(TensorBoard):\n","    def __init__(self, log_dir='./logs', **kwargs):\n","        self.val_log_dir = os.path.join(log_dir, 'C_attention/validation')\n","        training_log_dir = os.path.join(log_dir, 'C_attention/training')\n","        super(TrainValTensorBoard, self).__init__(training_log_dir, **kwargs)\n","\n","    def set_model(self, model):\n","        if context.executing_eagerly():\n","            self.val_writer = tf.contrib.summary.create_file_writer(self.val_log_dir)\n","        else:\n","            self.val_writer = tf.summary.FileWriter(self.val_log_dir)\n","        super(TrainValTensorBoard, self).set_model(model)\n","\n","    def _write_custom_summaries(self, step, logs=None):\n","        logs = logs or {}\n","        val_logs = {k.replace('val_', ''): v for k, v in logs.items() if 'val_' in k}\n","        if context.executing_eagerly():\n","            with self.val_writer.as_default(), tf.contrib.summary.always_record_summaries():\n","                for name, value in val_logs.items():\n","                    tf.contrib.summary.scalar(name, value.item(), step=step)\n","        else:\n","            for name, value in val_logs.items():\n","                summary = tf.Summary()\n","                summary_value = summary.value.add()\n","                summary_value.simple_value = value.item()\n","                summary_value.tag = name\n","                self.val_writer.add_summary(summary, step)\n","        self.val_writer.flush()\n","\n","        logs = {k: v for k, v in logs.items() if not 'val_' in k}\n","        super(TrainValTensorBoard, self)._write_custom_summaries(step, logs)\n","\n","    def on_train_end(self, logs=None):\n","        super(TrainValTensorBoard, self).on_train_end(logs)\n","        self.val_writer.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fXhtH-HhEydE","colab_type":"code","colab":{}},"source":["#Please take note that most of this part was extracted from class exercises, with some additions\n","      \n","def create_keras_model(vocab_size, y_size, embedding_size=128, hidden_size=512):\n","    print(\"Creating KERAS model\")\n","    \n","    model = Sequential()\n","    model.add(Embedding(vocab_size, embedding_size, mask_zero=True))\n","    \n","    model.add(Bidirectional(LSTM(hidden_size, dropout=0.3, recurrent_dropout=0.3, return_sequences=True), merge_mode='concat'))\n","    model.add(Bidirectional(LSTM(hidden_size, dropout=0.3, recurrent_dropout=0.3, return_sequences=True), merge_mode='concat'))\n","    \n","    model.add(\n","        SeqSelfAttention(\n","            attention_activation='softmax'\n","        )\n","    )\n","    \n","    model.add(TimeDistributed(Dense(y_size, activation='softmax')))\n","    optimizer = Adam()\n","    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['acc'])\n","\n","    return model\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PMqneG5WEydI","colab_type":"code","colab":{}},"source":["resource_path = \"../resources/\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pFd4-nloEydL","colab_type":"code","outputId":"ee7522c1-4a0e-4e04-9833-1a6f4940eee4","executionInfo":{"status":"error","timestamp":1568329160543,"user_tz":-120,"elapsed":5015595,"user":{"displayName":"Sayo Makinwa","photoUrl":"","userId":"02017936482338602411"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["batch_size = 64\n","epochs = 20\n","model_name = resource_path+\"modelC_att.hdf5\"\n","\n","#checks if the FINAL model was saved and loads it instead of creating a new one\n","if os.path.exists(model_name):\n","    model = load_model(model_name)\n","    print(\"Using a pre-saved model\")\n","    model.summary()\n","    \n","else:\n","    model = create_keras_model(vocab_size, len(y_vocab))\n","    print(\"Training a new model\")\n","    model.summary()\n","    \n","    filepath = resource_path+\"models/model_C_attention.hdf5\"\n","    checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n","    #callbacks_list = [checkpoint]\n","    tb_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\", histogram_freq=0, write_graph=False, write_images=True)\n","    callbacks_list = [TrainValTensorBoard(write_graph=False), checkpoint]\n","    \n","    print(\"\\nStarting training...\")\n","    model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size,\n","              shuffle=True, validation_data=(dev_x, dev_y), callbacks=callbacks_list) \n","    print(\"Training complete.\\n\")"],"execution_count":44,"outputs":[{"output_type":"stream","text":["Creating KERAS model\n","Training a new model\n","Model: \"sequential_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_2 (Embedding)      (None, None, 128)         6270592   \n","_________________________________________________________________\n","bidirectional_3 (Bidirection (None, None, 1024)        2625536   \n","_________________________________________________________________\n","bidirectional_4 (Bidirection (None, None, 1024)        6295552   \n","_________________________________________________________________\n","seq_self_attention_2 (SeqSel (None, None, 1024)        65601     \n","_________________________________________________________________\n","time_distributed_2 (TimeDist (None, None, 34505)       35367625  \n","=================================================================\n","Total params: 50,624,906\n","Trainable params: 50,624,906\n","Non-trainable params: 0\n","_________________________________________________________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n","\n","Starting training...\n","Train on 37168 samples, validate on 352 samples\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1122: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1128: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n","\n","Epoch 1/20\n","37168/37168 [==============================] - 254s 7ms/step - loss: 7.2642 - acc: 0.0689 - val_loss: 7.1650 - val_acc: 0.0749\n","Epoch 2/20\n","37168/37168 [==============================] - 250s 7ms/step - loss: 6.5849 - acc: 0.0964 - val_loss: 6.8806 - val_acc: 0.0925\n","Epoch 3/20\n","37168/37168 [==============================] - 248s 7ms/step - loss: 6.3368 - acc: 0.1053 - val_loss: 6.6995 - val_acc: 0.1080\n","Epoch 4/20\n","37168/37168 [==============================] - 248s 7ms/step - loss: 6.0408 - acc: 0.1271 - val_loss: 6.4959 - val_acc: 0.1354\n","Epoch 5/20\n","37168/37168 [==============================] - 248s 7ms/step - loss: 5.6906 - acc: 0.1595 - val_loss: 6.3329 - val_acc: 0.1615\n","Epoch 6/20\n","37168/37168 [==============================] - 249s 7ms/step - loss: 5.3533 - acc: 0.1875 - val_loss: 6.2672 - val_acc: 0.1871\n","Epoch 7/20\n","37168/37168 [==============================] - 249s 7ms/step - loss: 5.0558 - acc: 0.2093 - val_loss: 6.2111 - val_acc: 0.2043\n","Epoch 8/20\n","37168/37168 [==============================] - 249s 7ms/step - loss: 4.7731 - acc: 0.2289 - val_loss: 6.2247 - val_acc: 0.2198\n","Epoch 9/20\n","37168/37168 [==============================] - 248s 7ms/step - loss: 4.4999 - acc: 0.2546 - val_loss: 6.1895 - val_acc: 0.2612\n","Epoch 10/20\n","37168/37168 [==============================] - 247s 7ms/step - loss: 4.2281 - acc: 0.2893 - val_loss: 6.1928 - val_acc: 0.2729\n","Epoch 11/20\n","37168/37168 [==============================] - 247s 7ms/step - loss: 3.9852 - acc: 0.3142 - val_loss: 6.2239 - val_acc: 0.2873\n","Epoch 12/20\n","37168/37168 [==============================] - 247s 7ms/step - loss: 3.7826 - acc: 0.3320 - val_loss: 6.1899 - val_acc: 0.2967\n","Epoch 13/20\n","37168/37168 [==============================] - 246s 7ms/step - loss: 3.6063 - acc: 0.3468 - val_loss: 6.1587 - val_acc: 0.3087\n","Epoch 14/20\n","37168/37168 [==============================] - 247s 7ms/step - loss: 3.4475 - acc: 0.3608 - val_loss: 6.1645 - val_acc: 0.3180\n","Epoch 15/20\n","37168/37168 [==============================] - 247s 7ms/step - loss: 3.3009 - acc: 0.3740 - val_loss: 6.1519 - val_acc: 0.3323\n","Epoch 16/20\n","37168/37168 [==============================] - 247s 7ms/step - loss: 3.1511 - acc: 0.3887 - val_loss: 6.1291 - val_acc: 0.3427\n","Epoch 17/20\n","37168/37168 [==============================] - 248s 7ms/step - loss: 3.0057 - acc: 0.4032 - val_loss: 6.0714 - val_acc: 0.3547\n","Epoch 18/20\n","37168/37168 [==============================] - 247s 7ms/step - loss: 2.8729 - acc: 0.4167 - val_loss: 6.0247 - val_acc: 0.3565\n","Epoch 19/20\n","37168/37168 [==============================] - 247s 7ms/step - loss: 2.7564 - acc: 0.4275 - val_loss: 6.0282 - val_acc: 0.3545\n","Epoch 20/20\n","37168/37168 [==============================] - 246s 7ms/step - loss: 2.6521 - acc: 0.4377 - val_loss: 6.0379 - val_acc: 0.3630\n"],"name":"stdout"},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-44-97acff0b5f56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nStarting training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size,\n\u001b[0;32m---> 24\u001b[0;31m               shuffle=True, validation_data=(dev_x, dev_y), callbacks=callbacks_list) \n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training complete.\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_end_hook\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Helper function for on_{train|test|predict}_end methods.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_TRAIN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_TEST\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_end\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \"\"\"\n\u001b[1;32m    228\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_test_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_end\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    938\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ckpt_saved_epoch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m       \u001b[0;31m# Make `_ckpt_saved_epoch` attribute `None` at the end of training as it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m       \u001b[0;31m# is only used during the training. Currently it is decided not to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute '_ckpt_saved_epoch'"]}]},{"cell_type":"code","metadata":{"id":"o7W6BxfJtn1z","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"93974cea-aee4-44c9-ebf4-08a8b625eeba","executionInfo":{"status":"ok","timestamp":1568329411916,"user_tz":-120,"elapsed":7503,"user":{"displayName":"Sayo Makinwa","photoUrl":"","userId":"02017936482338602411"}}},"source":["#Save the FINAL model for later reuse\n","model.save(model_name)\n","print(\"Trained model saved for later use\")\n","\n","print(\"\\nEvaluating test...\")\n","loss_acc = model.evaluate(dev_x, dev_y, verbose=0)\n","print(\"Test data: loss = %0.6f  accuracy = %0.2f%% \" % (loss_acc[0], loss_acc[1]*100))"],"execution_count":47,"outputs":[{"output_type":"stream","text":["Trained model saved for later use\n","\n","Evaluating test...\n","Test data: loss = 5.960498  accuracy = 36.71% \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ztDgLLxJEydO","colab_type":"code","colab":{}},"source":["#Writing the vocabularies to file\n","\n","with open(resource_path+\"x_vocab.txt\", \"w\") as file:\n","    file.write(json.dumps(vocab))\n","    \n","with open(resource_path+\"y_vocab.txt\", \"w\") as file:\n","    file.write(json.dumps(y_vocab))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oqYFyp-bYJpR","colab_type":"text"},"source":["## Running Predictions"]},{"cell_type":"code","metadata":{"id":"FqZyKhGwEydR","colab_type":"code","colab":{}},"source":["from predict_att import *\n","from score import *"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"l5cAhA48y7Ii","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":190},"outputId":"0b2f2cc7-a867-448e-ba93-fb039d27a2ce","executionInfo":{"status":"ok","timestamp":1568338633425,"user_tz":-120,"elapsed":6388,"user":{"displayName":"Sayo Makinwa","photoUrl":"","userId":"02017936482338602411"}}},"source":["pip install keras-self-attention"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: keras-self-attention in /usr/local/lib/python3.6/dist-packages (0.42.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-self-attention) (1.16.5)\n","Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-self-attention) (2.2.5)\n","Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.0.8)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (3.13)\n","Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.1.0)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.3.1)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.12.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (2.8.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"T4DYUwkGnSt-","colab_type":"code","outputId":"c8ac2b65-cdc5-4647-def8-ee3e09c1e541","executionInfo":{"status":"ok","timestamp":1568338633426,"user_tz":-120,"elapsed":6047,"user":{"displayName":"Sayo Makinwa","photoUrl":"","userId":"02017936482338602411"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["import nltk\n","nltk.download(\"wordnet\")"],"execution_count":6,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"VlpjJp8Oy-Zm","colab_type":"code","colab":{}},"source":["from keras_self_attention import SeqSelfAttention"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"p9G98b-X4Uzz","colab_type":"code","colab":{}},"source":["def run_tests1(score_file):\n","    \n","    resources_path = '../resources'\n","    bn2wn_mapping_file = '../resources/babelnet2wordnet.tsv'\n","    \n","    all_scores = []\n","    if (os.path.isfile(score_file)):\n","        with open(score_file, 'r') as file:\n","            for line in file:\n","                all_scores.append(line.split(\",\"))\n","        \n","    print(\"PREDICTING FOR SE2...\")\n","    se2_scores = ['SE2']\n","\n","    input_path = '../data/Evaluation_Datasets/senseval2/senseval2.data.xml'\n","    output_path = '../data/Evaluation_Datasets/senseval2'\n","    gold_file =  '../data/Evaluation_Datasets/senseval2/senseval2.gold.key.txt'\n","\n","    pred_babelnet = '../data/Evaluation_Datasets/senseval2/pred_babelnet.txt'\n","    pred_lex = '../data/Evaluation_Datasets/senseval2/pred_lex.txt'\n","    pred_domains = '../data/Evaluation_Datasets/senseval2/pred_domains.txt'\n","\n","    predict_babelnet(input_path, output_path, resources_path)\n","    score = score_predict1(pred_babelnet, gold_file, bn2wn_mapping_file)\n","    se2_scores.append(score*100)\n","\n","    predict_lexicographer(input_path, output_path, resources_path)\n","    score = score_predict_lex(pred_lex, gold_file, resources_path)\n","    se2_scores.append(score*100)\n","\n","    predict_wordnet_domains(input_path, output_path, resources_path)\n","    score = score_predict_dom(pred_domains, gold_file, resources_path)\n","    se2_scores.append(score*100)\n","\n","    all_scores.append(se2_scores)\n","    \n","\n","    #########################################################\n","\n","    print(\"\\n\\nPREDICTING FOR SE3...\")\n","    se3_scores = ['SE3_(Dev)']\n","\n","    input_path = '../data/Evaluation_Datasets/senseval3/senseval3.data.xml'\n","    output_path = '../data/Evaluation_Datasets/senseval3'\n","    gold_file =  '../data/Evaluation_Datasets/senseval3/senseval3.gold.key.txt'\n","\n","    pred_babelnet = '../data/Evaluation_Datasets/senseval3/pred_babelnet.txt'\n","    pred_lex = '../data/Evaluation_Datasets/senseval3/pred_lex.txt'\n","    pred_domains = '../data/Evaluation_Datasets/senseval3/pred_domains.txt'\n","\n","    predict_babelnet(input_path, output_path, resources_path)\n","    score = score_predict1(pred_babelnet, gold_file, bn2wn_mapping_file)\n","    se3_scores.append(score*100)\n","\n","    predict_lexicographer(input_path, output_path, resources_path)\n","    score = score_predict_lex(pred_lex, gold_file, resources_path)\n","    se3_scores.append(score*100)\n","\n","    predict_wordnet_domains(input_path, output_path, resources_path)\n","    score = score_predict_dom(pred_domains, gold_file, resources_path)\n","    se3_scores.append(score*100)\n","\n","    all_scores.append(se3_scores)\n","    \n","    \n","    ###########################################################\n","        \n","    with open(score_file, \"w\") as file:\n","        for scores in all_scores:\n","            file.write(\"{},{:.2f},{:.2f},{:.2f}\\n\".format(scores[0], float(scores[1]), float(scores[2]), float(scores[3])))\n","\n","\n","\n","def run_tests2(score_file):\n","    \n","    resources_path = '../resources'\n","    bn2wn_mapping_file = '../resources/babelnet2wordnet.tsv'\n","    \n","    all_scores = []\n","    if (os.path.isfile(score_file)):\n","        with open(score_file, 'r') as file:\n","            for line in file:\n","                all_scores.append(line.split(\",\"))\n","                \n","    \n","    print(\"\\n\\nPREDICTING FOR SE07...\")\n","    se07_scores = ['SE07']\n","    \n","    input_path = '../data/Evaluation_Datasets/semeval2007/semeval2007.data.xml'\n","    output_path = '../data/Evaluation_Datasets/semeval2007'\n","    gold_file =  '../data/Evaluation_Datasets/semeval2007/semeval2007.gold.key.txt'\n","    \n","    pred_babelnet = '../data/Evaluation_Datasets/semeval2007/pred_babelnet.txt'\n","    pred_lex = '../data/Evaluation_Datasets/semeval2007/pred_lex.txt'\n","    pred_domains = '../data/Evaluation_Datasets/semeval2007/pred_domains.txt'\n","\n","    predict_babelnet(input_path, output_path, resources_path)\n","    score = score_predict1(pred_babelnet, gold_file, bn2wn_mapping_file)\n","    se07_scores.append(score*100)\n","    \n","    predict_lexicographer(input_path, output_path, resources_path)\n","    score = score_predict_lex(pred_lex, gold_file, resources_path)\n","    se07_scores.append(score*100)\n","    \n","    predict_wordnet_domains(input_path, output_path, resources_path)\n","    score = score_predict_dom(pred_domains, gold_file, resources_path)\n","    se07_scores.append(score*100)\n","    \n","    all_scores.append(se07_scores)\n","    \n","    #########################################################\n","    \n","    print(\"\\n\\nPREDICTING FOR SE13...\")\n","    se13_scores = ['SE13']\n","    \n","    input_path = '../data/Evaluation_Datasets/semeval2013/semeval2013.data.xml'\n","    output_path = '../data/Evaluation_Datasets/semeval2013'\n","    gold_file =  '../data/Evaluation_Datasets/semeval2013/semeval2013.gold.key.txt'\n","    \n","    pred_babelnet = '../data/Evaluation_Datasets/semeval2013/pred_babelnet.txt'\n","    pred_lex = '../data/Evaluation_Datasets/semeval2013/pred_lex.txt'\n","    pred_domains = '../data/Evaluation_Datasets/semeval2013/pred_domains.txt'\n","\n","    predict_babelnet(input_path, output_path, resources_path)\n","    score = score_predict1(pred_babelnet, gold_file, bn2wn_mapping_file)\n","    se13_scores.append(score*100)\n","    \n","    predict_lexicographer(input_path, output_path, resources_path)\n","    score = score_predict_lex(pred_lex, gold_file, resources_path)\n","    se13_scores.append(score*100)\n","    \n","    predict_wordnet_domains(input_path, output_path, resources_path)\n","    score = score_predict_dom(pred_domains, gold_file, resources_path)\n","    se13_scores.append(score*100)\n","    \n","    all_scores.append(se13_scores)\n","    \n","    #########################################################\n","    \n","    \n","    with open(score_file, \"w\") as file:\n","        for scores in all_scores:\n","            file.write(\"{},{:.2f},{:.2f},{:.2f}\\n\".format(scores[0], float(scores[1]), float(scores[2]), float(scores[3])))\n","            \n","            \n","            \n","    \n","    \n","\n","def run_tests3(score_file):\n","    \n","    resources_path = '../resources'\n","    bn2wn_mapping_file = '../resources/babelnet2wordnet.tsv'\n","    \n","    all_scores = []\n","    if (os.path.isfile(score_file)):\n","        with open(score_file, 'r') as file:\n","            for line in file:\n","                all_scores.append(line.split(\",\"))\n","                \n","    \n","    print(\"\\n\\nPREDICTING FOR SE15...\")\n","    se15_scores = ['SE15']\n","    \n","    input_path = '../data/Evaluation_Datasets/semeval2015/semeval2015.data.xml'\n","    output_path = '../data/Evaluation_Datasets/semeval2015'\n","    gold_file =  '../data/Evaluation_Datasets/semeval2015/semeval2015.gold.key.txt'\n","    \n","    pred_babelnet = '../data/Evaluation_Datasets/semeval2015/pred_babelnet.txt'\n","    pred_lex = '../data/Evaluation_Datasets/semeval2015/pred_lex.txt'\n","    pred_domains = '../data/Evaluation_Datasets/semeval2015/pred_domains.txt'\n","\n","    predict_babelnet(input_path, output_path, resources_path)\n","    score = score_predict1(pred_babelnet, gold_file, bn2wn_mapping_file)\n","    se15_scores.append(score*100)\n","    \n","    predict_lexicographer(input_path, output_path, resources_path)\n","    score = score_predict_lex(pred_lex, gold_file, resources_path)\n","    se15_scores.append(score*100)\n","    \n","    predict_wordnet_domains(input_path, output_path, resources_path)\n","    score = score_predict_dom(pred_domains, gold_file, resources_path)\n","    se15_scores.append(score*100)\n","    \n","    all_scores.append(se15_scores)\n","    \n","    \n","    #####################################################################\n","    \n","    \n","    print(\"\\n\\nPREDICTING FOR ALL...\")\n","    se_ALL_scores = ['ALL']\n","    \n","    input_path = '../data/Evaluation_Datasets/ALL/ALL.data.xml'\n","    output_path = '../data/Evaluation_Datasets/ALL'\n","    gold_file =  '../data/Evaluation_Datasets/ALL/ALL.gold.key.txt'\n","    \n","    pred_babelnet = '../data/Evaluation_Datasets/ALL/pred_babelnet.txt'\n","    pred_lex = '../data/Evaluation_Datasets/ALL/pred_lex.txt'\n","    pred_domains = '../data/Evaluation_Datasets/ALL/pred_domains.txt'\n","\n","    predict_babelnet(input_path, output_path, resources_path)\n","    score = score_predict1(pred_babelnet, gold_file, bn2wn_mapping_file)\n","    se_ALL_scores.append(score*100)\n","    \n","    predict_lexicographer(input_path, output_path, resources_path)\n","    score = score_predict_lex(pred_lex, gold_file, resources_path)\n","    se_ALL_scores.append(score*100)\n","    \n","    predict_wordnet_domains(input_path, output_path, resources_path)\n","    score = score_predict_dom(pred_domains, gold_file, resources_path)\n","    se_ALL_scores.append(score*100)\n","    \n","    all_scores.append(se_ALL_scores)\n","    \n","    \n","    with open(score_file, \"w\") as file:\n","        for scores in all_scores:\n","            file.write(\"{},{:.2f},{:.2f},{:.2f}\\n\".format(scores[0], float(scores[1]), float(scores[2]), float(scores[3])))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NNeyRlWL3gVH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"ba7e856b-bb3a-47a3-86fe-841b44eda075","executionInfo":{"status":"ok","timestamp":1568338944196,"user_tz":-120,"elapsed":307293,"user":{"displayName":"Sayo Makinwa","photoUrl":"","userId":"02017936482338602411"}}},"source":["#all_scores_pd = run_tests1(\"../resources/scores_.csv\")\n","#all_scores_pd = run_tests2(\"../resources/scores_.csv\")\n","all_scores_pd = run_tests3(\"../resources/scores_.csv\")"],"execution_count":9,"outputs":[{"output_type":"stream","text":["\n","\n","PREDICTING FOR SE15...\n","LOADING RESOURCES...\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3239: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","PREPARING EVALUATION DATA FOR PREDICTION...\n","138 sentences extracted...\n","Evaluation data extraction completed\n","Predicting (line by line) and writing to file... This may take a little while...\n","100/138 lines done... A little moment more and everything will be done! :)\n","Prediction complete!\n","LOADING RESOURCES...\n","PREPARING EVALUATION DATA FOR PREDICTION...\n","138 sentences extracted...\n","Evaluation data extraction completed\n","Predicting (line by line) and writing to file... This may take a little while...\n","100/138 lines done... A little moment more and everything will be done! :)\n","Prediction complete!\n","LOADING RESOURCES...\n","PREPARING EVALUATION DATA FOR PREDICTION...\n","138 sentences extracted...\n","Evaluation data extraction completed\n","Predicting (line by line) and writing to file... This may take a little while...\n","100/138 lines done... A little moment more and everything will be done! :)\n","Prediction complete!\n","\n","\n","PREDICTING FOR ALL...\n","LOADING RESOURCES...\n","PREPARING EVALUATION DATA FOR PREDICTION...\n","1,173 sentences extracted...\n","Evaluation data extraction completed\n","Predicting (line by line) and writing to file... This may take a little while...\n","100/1173 lines done... A little moment more and everything will be done! :)\n","200/1173 lines done... A little moment more and everything will be done! :)\n","300/1173 lines done... A little moment more and everything will be done! :)\n","400/1173 lines done... A little moment more and everything will be done! :)\n","500/1173 lines done... A little moment more and everything will be done! :)\n","600/1173 lines done... A little moment more and everything will be done! :)\n","700/1173 lines done... A little moment more and everything will be done! :)\n","800/1173 lines done... A little moment more and everything will be done! :)\n","900/1173 lines done... A little moment more and everything will be done! :)\n","1000/1173 lines done... A little moment more and everything will be done! :)\n","1100/1173 lines done... A little moment more and everything will be done! :)\n","Prediction complete!\n","LOADING RESOURCES...\n","PREPARING EVALUATION DATA FOR PREDICTION...\n","1,173 sentences extracted...\n","Evaluation data extraction completed\n","Predicting (line by line) and writing to file... This may take a little while...\n","100/1173 lines done... A little moment more and everything will be done! :)\n","200/1173 lines done... A little moment more and everything will be done! :)\n","300/1173 lines done... A little moment more and everything will be done! :)\n","400/1173 lines done... A little moment more and everything will be done! :)\n","500/1173 lines done... A little moment more and everything will be done! :)\n","600/1173 lines done... A little moment more and everything will be done! :)\n","700/1173 lines done... A little moment more and everything will be done! :)\n","800/1173 lines done... A little moment more and everything will be done! :)\n","900/1173 lines done... A little moment more and everything will be done! :)\n","1000/1173 lines done... A little moment more and everything will be done! :)\n","1100/1173 lines done... A little moment more and everything will be done! :)\n","Prediction complete!\n","LOADING RESOURCES...\n","PREPARING EVALUATION DATA FOR PREDICTION...\n","1,173 sentences extracted...\n","Evaluation data extraction completed\n","Predicting (line by line) and writing to file... This may take a little while...\n","100/1173 lines done... A little moment more and everything will be done! :)\n","200/1173 lines done... A little moment more and everything will be done! :)\n","300/1173 lines done... A little moment more and everything will be done! :)\n","400/1173 lines done... A little moment more and everything will be done! :)\n","500/1173 lines done... A little moment more and everything will be done! :)\n","600/1173 lines done... A little moment more and everything will be done! :)\n","700/1173 lines done... A little moment more and everything will be done! :)\n","800/1173 lines done... A little moment more and everything will be done! :)\n","900/1173 lines done... A little moment more and everything will be done! :)\n","1000/1173 lines done... A little moment more and everything will be done! :)\n","1100/1173 lines done... A little moment more and everything will be done! :)\n","Prediction complete!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Jd7iZT-GYTCa","colab_type":"text"},"source":["## Results"]},{"cell_type":"code","metadata":{"id":"kViXUjmsEydk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":235},"outputId":"a73b1d24-07c7-41e6-f7da-e6e511fe477d","executionInfo":{"status":"ok","timestamp":1568338944198,"user_tz":-120,"elapsed":302615,"user":{"displayName":"Sayo Makinwa","photoUrl":"","userId":"02017936482338602411"}}},"source":["all_scores_pd = pd.read_csv('../resources/scores_.csv', names=['Babelnet', 'Lex', 'Domain'])\n","all_scores_pd"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Babelnet</th>\n","      <th>Lex</th>\n","      <th>Domain</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>SE2</th>\n","      <td>54.60</td>\n","      <td>72.00</td>\n","      <td>85.32</td>\n","    </tr>\n","    <tr>\n","      <th>SE3_(Dev)</th>\n","      <td>55.95</td>\n","      <td>68.81</td>\n","      <td>82.27</td>\n","    </tr>\n","    <tr>\n","      <th>SE07</th>\n","      <td>43.30</td>\n","      <td>62.20</td>\n","      <td>84.62</td>\n","    </tr>\n","    <tr>\n","      <th>SE13</th>\n","      <td>52.31</td>\n","      <td>63.20</td>\n","      <td>72.08</td>\n","    </tr>\n","    <tr>\n","      <th>SE15</th>\n","      <td>49.80</td>\n","      <td>63.11</td>\n","      <td>76.22</td>\n","    </tr>\n","    <tr>\n","      <th>ALL</th>\n","      <td>53.04</td>\n","      <td>67.32</td>\n","      <td>80.22</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["           Babelnet    Lex  Domain\n","SE2           54.60  72.00   85.32\n","SE3_(Dev)     55.95  68.81   82.27\n","SE07          43.30  62.20   84.62\n","SE13          52.31  63.20   72.08\n","SE15          49.80  63.11   76.22\n","ALL           53.04  67.32   80.22"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"Pfkb11VCBZzp","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}