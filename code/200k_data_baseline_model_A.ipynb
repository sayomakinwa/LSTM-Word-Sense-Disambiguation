{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P00PH5WB6Cdn"
   },
   "source": [
    "## Mount Drive and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 763,
     "status": "ok",
     "timestamp": 1568340147136,
     "user": {
      "displayName": "Sayo Makinwa",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mB01ONwXql1XhFq6AK_2PjHkUfJA3VDpx1AVsLimg=s64",
      "userId": "02832581640686072550"
     },
     "user_tz": -120
    },
    "id": "NKtsqriiKxYd",
    "outputId": "cd4b3df4-7fdb-41cd-f198-6fe4445033d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 518,
     "status": "ok",
     "timestamp": 1568340147152,
     "user": {
      "displayName": "Sayo Makinwa",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mB01ONwXql1XhFq6AK_2PjHkUfJA3VDpx1AVsLimg=s64",
      "userId": "02832581640686072550"
     },
     "user_tz": -120
    },
    "id": "nRTrAorEKyu5",
    "outputId": "8b6add85-d9bb-4641-86b8-4afc5881322b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/Colab Notebooks/nlp_hw3/code\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/My Drive/Colab Notebooks/nlp_hw3/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OxdkmYCxEybs"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from typing import Tuple, List, Dict\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as K\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.python.eager import context\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import my_utils as utils\n",
    "import corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TYLN4Lf-6RO_"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "042xGwFcEyb1"
   },
   "outputs": [],
   "source": [
    "def load_train_dataset(input_path: str, y_path: str) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    :param input_path; Path to the input dataset\n",
    "    :param label_path; Path to the file containing the corresponding labels for the input dataset\n",
    "    :return sentences; List of sentences in input_file\n",
    "    :return labels; List of corresponding word segment codes in label_path. Same len as sentences\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    k = 0\n",
    "    with open(input_path, \"r\", encoding=\"utf-8-sig\") as file:\n",
    "        for line in file:\n",
    "            k += 1\n",
    "            sentences.append(line.strip())\n",
    "            if (k >= 200000):\n",
    "                break\n",
    "\n",
    "    y = []\n",
    "    k = 0\n",
    "    with open(y_path, \"r\", encoding=\"utf-8-sig\") as file:\n",
    "        for line in file:\n",
    "            k += 1\n",
    "            y.append(line.strip())\n",
    "            if (k >= 200000):\n",
    "                break\n",
    "\n",
    "    return sentences, y\n",
    "\n",
    "\n",
    "def make_X_vocab(sentences: List[str]) -> Dict[str, int]:\n",
    "    '''\n",
    "    :param sentences; List of input sentences from the dataset\n",
    "    :return unigrams_vocab; Dictionary from unigram to int\n",
    "    :return bigrams_vocab; Dictionary from bigram to int\n",
    "    '''\n",
    "    vocab = {\"UNK\": 0}\n",
    "\n",
    "    for sentence in sentences:\n",
    "        for word in sentence.split():\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "\n",
    "def make_Y_vocab(y: List[str]) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    :param labels; List of label codes\n",
    "    :return labels_vocab; Dictionary from label code to int \n",
    "    \"\"\"\n",
    "    y_vocab = {\"UNK\": 0}\n",
    "    \n",
    "    for y_line in y:\n",
    "        for y_word in y_line.split():\n",
    "            if y_word not in y_vocab:\n",
    "                y_vocab[y_word] = len(y_vocab)\n",
    "                \n",
    "    return y_vocab\n",
    "\n",
    "def make_Y(output: List[str], output_vocab: Dict[str, int]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    :param labels; List of word segment codes, line by line\n",
    "    :param labels_vocab; Label codes vocab\n",
    "    :return y; Vector of label code indices\n",
    "    \"\"\"\n",
    "    y = []\n",
    "    for output_line in output:\n",
    "        y_temp = []\n",
    "        for single_output in output_line.split():\n",
    "            if single_output in output_vocab:\n",
    "                y_temp.append( output_vocab[single_output])\n",
    "            else:\n",
    "                y_temp.append( output_vocab[\"UNK\"])\n",
    "        y.append(np.array(y_temp))\n",
    "    \n",
    "    return np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pYTDcJyQm1gV"
   },
   "outputs": [],
   "source": [
    "train_data_path = '../data/Training_Corpora/semcor+omsti'\n",
    "test_data_path = '../data/Evaluation_Datasets/senseval3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6kiV4oP3-r-Y"
   },
   "outputs": [],
   "source": [
    "#Parse training data\n",
    "corpora_xml_path = train_data_path + '/semcor+omsti.data.xml'\n",
    "gold_mapping_path = train_data_path + '/semcor.gold.key.txt'\n",
    "resources_path = '../resources/'\n",
    "\n",
    "corpora.extract_training_data(corpora_xml_path, gold_mapping_path, resources_path, train_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eBT3apRV_Bdn"
   },
   "outputs": [],
   "source": [
    "#Parse validation data\n",
    "corpora_xml_path = test_data_path + '/senseval3.data.xml'\n",
    "gold_mapping_path = test_data_path + '/senseval3.gold.key.txt'\n",
    "resources_path = '../resources/'\n",
    "\n",
    "corpora.extract_training_data(corpora_xml_path, gold_mapping_path, resources_path, test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yVV_9dxlEyb4"
   },
   "outputs": [],
   "source": [
    "sentences, y = load_train_dataset(train_data_path+\"/trainX.txt\", train_data_path+\"/trainy.txt\")\n",
    "test_sentences, test_y = load_train_dataset(test_data_path+\"/trainX.txt\", test_data_path+\"/trainy.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 456,
     "status": "ok",
     "timestamp": 1568312298241,
     "user": {
      "displayName": "Sayo Makinwa",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mB01ONwXql1XhFq6AK_2PjHkUfJA3VDpx1AVsLimg=s64",
      "userId": "02832581640686072550"
     },
     "user_tz": -120
    },
    "id": "-EJ1fHSzEyb7",
    "outputId": "bb60cfcb-ce48-4f6b-9ab6-a90e09c92b46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n",
      "How long has it been since you reviewed the objectives of your benefit and service program\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 874,
     "status": "ok",
     "timestamp": 1568312300163,
     "user": {
      "displayName": "Sayo Makinwa",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mB01ONwXql1XhFq6AK_2PjHkUfJA3VDpx1AVsLimg=s64",
      "userId": "02832581640686072550"
     },
     "user_tz": -120
    },
    "id": "t1fD1tgDEycC",
    "outputId": "e9bc4f95-9b7c-425a-9afb-d2a6f7d30016"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n",
      "how bn:00106124a have it bn:00083181v since you bn:00092618v the bn:00002179n of you bn:00009904n and bn:00070654n bn:00064646n\n"
     ]
    }
   ],
   "source": [
    "print(len(y))\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2610,
     "status": "ok",
     "timestamp": 1568312302658,
     "user": {
      "displayName": "Sayo Makinwa",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mB01ONwXql1XhFq6AK_2PjHkUfJA3VDpx1AVsLimg=s64",
      "userId": "02832581640686072550"
     },
     "user_tz": -120
    },
    "id": "HgCzdmbLEycF",
    "outputId": "d0ac0337-6a13-4c3f-ae29-b7b4abef4eb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105417\n"
     ]
    }
   ],
   "source": [
    "vocab = make_X_vocab(sentences + test_sentences)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2404,
     "status": "ok",
     "timestamp": 1568312303317,
     "user": {
      "displayName": "Sayo Makinwa",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mB01ONwXql1XhFq6AK_2PjHkUfJA3VDpx1AVsLimg=s64",
      "userId": "02832581640686072550"
     },
     "user_tz": -120
    },
    "id": "LPRR9TA7EycH",
    "outputId": "4b586c20-89b1-4b6d-d673-e7a74244a02c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104249\n"
     ]
    }
   ],
   "source": [
    "y_vocab = make_Y_vocab(y + test_y)\n",
    "print(len(y_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iD6TeqJREycR"
   },
   "outputs": [],
   "source": [
    "X = utils.make_X(sentences, vocab)\n",
    "y_array = make_Y(y, y_vocab)\n",
    "\n",
    "X_test = utils.make_X(test_sentences, vocab)\n",
    "y_test_array = make_Y(test_y, y_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6792,
     "status": "ok",
     "timestamp": 1568312309046,
     "user": {
      "displayName": "Sayo Makinwa",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mB01ONwXql1XhFq6AK_2PjHkUfJA3VDpx1AVsLimg=s64",
      "userId": "02832581640686072550"
     },
     "user_tz": -120
    },
    "id": "DF5m68nmEycT",
    "outputId": "5b34d843-e2d2-4d86-e725-c8f9a43fe272"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000,)\n",
      "(200000,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j7fg-usHEycZ"
   },
   "outputs": [],
   "source": [
    "train_x = pad_sequences(X, truncating='pre', padding='post', maxlen=30)\n",
    "train_y = pad_sequences(y_array, truncating='pre', padding='post', maxlen=30)\n",
    "\n",
    "dev_x = pad_sequences(X_test, truncating='pre', padding='post', maxlen=30)\n",
    "dev_y = pad_sequences(y_test_array, truncating='pre', padding='post', maxlen=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6919,
     "status": "ok",
     "timestamp": 1568312310470,
     "user": {
      "displayName": "Sayo Makinwa",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mB01ONwXql1XhFq6AK_2PjHkUfJA3VDpx1AVsLimg=s64",
      "userId": "02832581640686072550"
     },
     "user_tz": -120
    },
    "id": "NmXFcuk4Eycc",
    "outputId": "0a8806a8-272a-45f7-c9c8-76956fff7d1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 30)\n",
      "(200000, 30)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6430,
     "status": "ok",
     "timestamp": 1568312310472,
     "user": {
      "displayName": "Sayo Makinwa",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mB01ONwXql1XhFq6AK_2PjHkUfJA3VDpx1AVsLimg=s64",
      "userId": "02832581640686072550"
     },
     "user_tz": -120
    },
    "id": "YIojgrqREycg",
    "outputId": "527c1a06-d5cf-4191-aaf9-6beaabe12b52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 30, 1)\n",
      "(352, 30, 1)\n"
     ]
    }
   ],
   "source": [
    "train_y = train_y.reshape((*train_y.shape, 1))\n",
    "dev_y = dev_y.reshape((*dev_y.shape, 1))\n",
    "print(train_y.shape)\n",
    "print(dev_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5864,
     "status": "ok",
     "timestamp": 1568312310474,
     "user": {
      "displayName": "Sayo Makinwa",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mB01ONwXql1XhFq6AK_2PjHkUfJA3VDpx1AVsLimg=s64",
      "userId": "02832581640686072550"
     },
     "user_tz": -120
    },
    "id": "ABjb_KPbEyc1",
    "outputId": "4ce13863-f4b8-4407-83a0-1ab678a01cf0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 30)\n",
      "(200000, 30, 1)\n",
      "(352, 30)\n",
      "(352, 30, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(dev_x.shape)\n",
    "print(dev_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IwhKREoe_vx_"
   },
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gd-bBhRDEyc-"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9uF52K_8EydA"
   },
   "outputs": [],
   "source": [
    "#This class helps with logging\n",
    "class TrainValTensorBoard(TensorBoard):\n",
    "    def __init__(self, log_dir='./logs', **kwargs):\n",
    "        self.val_log_dir = os.path.join(log_dir, 'model_A_omsti/validation')\n",
    "        training_log_dir = os.path.join(log_dir, 'model_A_omsti/training')\n",
    "        super(TrainValTensorBoard, self).__init__(training_log_dir, **kwargs)\n",
    "\n",
    "    def set_model(self, model):\n",
    "        if context.executing_eagerly():\n",
    "            self.val_writer = tf.contrib.summary.create_file_writer(self.val_log_dir)\n",
    "        else:\n",
    "            self.val_writer = tf.summary.FileWriter(self.val_log_dir)\n",
    "        super(TrainValTensorBoard, self).set_model(model)\n",
    "\n",
    "    def _write_custom_summaries(self, step, logs=None):\n",
    "        logs = logs or {}\n",
    "        val_logs = {k.replace('val_', ''): v for k, v in logs.items() if 'val_' in k}\n",
    "        if context.executing_eagerly():\n",
    "            with self.val_writer.as_default(), tf.contrib.summary.always_record_summaries():\n",
    "                for name, value in val_logs.items():\n",
    "                    tf.contrib.summary.scalar(name, value.item(), step=step)\n",
    "        else:\n",
    "            for name, value in val_logs.items():\n",
    "                summary = tf.Summary()\n",
    "                summary_value = summary.value.add()\n",
    "                summary_value.simple_value = value.item()\n",
    "                summary_value.tag = name\n",
    "                self.val_writer.add_summary(summary, step)\n",
    "        self.val_writer.flush()\n",
    "\n",
    "        logs = {k: v for k, v in logs.items() if not 'val_' in k}\n",
    "        super(TrainValTensorBoard, self)._write_custom_summaries(step, logs)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        super(TrainValTensorBoard, self).on_train_end(logs)\n",
    "        self.val_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fXhtH-HhEydE"
   },
   "outputs": [],
   "source": [
    "#Please take note that most of this part was extracted from class exercises, with some additions\n",
    "\n",
    "def create_keras_model(vocab_size, y_size, embedding_size=128, hidden_size=128):\n",
    "    print(\"Creating KERAS model\")\n",
    "    \n",
    "    model = K.models.Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_size, mask_zero=True))\n",
    "    \n",
    "    model.add(Bidirectional(LSTM(hidden_size, dropout=0.3, recurrent_dropout=0.3, return_sequences=True), merge_mode='concat'))\n",
    "    model.add(Bidirectional(LSTM(hidden_size, dropout=0.3, recurrent_dropout=0.3, return_sequences=True), merge_mode='concat'))\n",
    "    \n",
    "    model.add(TimeDistributed(Dense(y_size, activation='softmax')))\n",
    "    optimizer = K.optimizers.Adam(lr=0.01)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['acc'])\n",
    "\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PMqneG5WEydI"
   },
   "outputs": [],
   "source": [
    "resource_path = \"../resources/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "872HdkkrTJi9"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 989
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 17636442,
     "status": "ok",
     "timestamp": 1568329947525,
     "user": {
      "displayName": "Sayo Makinwa",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mB01ONwXql1XhFq6AK_2PjHkUfJA3VDpx1AVsLimg=s64",
      "userId": "02832581640686072550"
     },
     "user_tz": -120
    },
    "id": "pFd4-nloEydL",
    "outputId": "697b7f7f-6678-4aca-eaac-6da99fef7059"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating KERAS model\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py:3794: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Training a new model\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 128)         13493376  \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, None, 256)         263168    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, None, 256)         394240    \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, None, 104249)      26791993  \n",
      "=================================================================\n",
      "Total params: 40,942,777\n",
      "Trainable params: 40,942,777\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n",
      "\n",
      "Starting training...\n",
      "Train on 200000 samples, validate on 352 samples\n",
      "Epoch 1/5\n",
      "200000/200000 [==============================] - 3487s 17ms/sample - loss: 1.5508 - acc: 0.7559 - val_loss: 1.6280 - val_acc: 0.5826\n",
      "Epoch 2/5\n",
      "200000/200000 [==============================] - 3518s 18ms/sample - loss: 0.6478 - acc: 0.8754 - val_loss: 1.5124 - val_acc: 0.5923\n",
      "Epoch 3/5\n",
      "200000/200000 [==============================] - 3533s 18ms/sample - loss: 0.5596 - acc: 0.8830 - val_loss: 1.4721 - val_acc: 0.6070\n",
      "Epoch 4/5\n",
      "200000/200000 [==============================] - 3541s 18ms/sample - loss: 0.5073 - acc: 0.8876 - val_loss: 1.4922 - val_acc: 0.6057\n",
      "Epoch 5/5\n",
      "200000/200000 [==============================] - 3542s 18ms/sample - loss: 0.4723 - acc: 0.8906 - val_loss: 1.4809 - val_acc: 0.6050\n",
      "Training complete.\n",
      "\n",
      "Trained model saved for later use\n",
      "\n",
      "Evaluating test...\n",
      "Test data: loss = 1.480879  accuracy = 60.50% \n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 5\n",
    "model_name = resource_path+\"model_A_omsti.hdf5\"\n",
    "\n",
    "#checks if the FINAL model was saved and loads it instead of creating a new one\n",
    "if os.path.exists(model_name):\n",
    "    model = load_model(model_name)\n",
    "    print(\"Using a pre-saved model\")\n",
    "    model.summary()\n",
    "    \n",
    "else:\n",
    "    model = create_keras_model(vocab_size, len(y_vocab))\n",
    "    print(\"Training a new model\")\n",
    "    model.summary()\n",
    "    \n",
    "    filepath = resource_path+\"models/model_A_omsti.hdf5\"\n",
    "    checkpoint = K.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "    #callbacks_list = [checkpoint]\n",
    "    tb_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\", histogram_freq=0, write_graph=False, write_images=True)\n",
    "    callbacks_list = [TrainValTensorBoard(write_graph=False), checkpoint]\n",
    "    \n",
    "    print(\"\\nStarting training...\")\n",
    "    model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size,\n",
    "              shuffle=True, validation_data=(dev_x, dev_y), callbacks=callbacks_list) \n",
    "    print(\"Training complete.\\n\")\n",
    "    \n",
    "    #Save the FINAL model for later reuse\n",
    "    model.save(model_name)\n",
    "    print(\"Trained model saved for later use\")\n",
    "\n",
    "    print(\"\\nEvaluating test...\")\n",
    "    loss_acc = model.evaluate(dev_x, dev_y, verbose=0)\n",
    "    print(\"Test data: loss = %0.6f  accuracy = %0.2f%% \" % (loss_acc[0], loss_acc[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ztDgLLxJEydO"
   },
   "outputs": [],
   "source": [
    "del model\n",
    "\n",
    "#Writing the vocabularies to file\n",
    "\n",
    "with open(resource_path+\"200k_A_x_vocab.txt\", \"w\") as file:\n",
    "    file.write(json.dumps(vocab))\n",
    "    \n",
    "with open(resource_path+\"200k_A_y_vocab.txt\", \"w\") as file:\n",
    "    file.write(json.dumps(y_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "viA96Aw5TOF8"
   },
   "source": [
    "## Running Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FqZyKhGwEydR"
   },
   "outputs": [],
   "source": [
    "from predict import *\n",
    "from score import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 648,
     "status": "ok",
     "timestamp": 1568340153592,
     "user": {
      "displayName": "Sayo Makinwa",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mB01ONwXql1XhFq6AK_2PjHkUfJA3VDpx1AVsLimg=s64",
      "userId": "02832581640686072550"
     },
     "user_tz": -120
    },
    "id": "T4DYUwkGnSt-",
    "outputId": "e408f2cc-b733-4795-93f9-0747501f322c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N-zJujtZEydT"
   },
   "outputs": [],
   "source": [
    "def run_tests1(score_file):\n",
    "    \n",
    "    resources_path = '../resources'\n",
    "    bn2wn_mapping_file = '../resources/babelnet2wordnet.tsv'\n",
    "    \n",
    "    all_scores = []\n",
    "    if (os.path.isfile(score_file)):\n",
    "        with open(score_file, 'r') as file:\n",
    "            for line in file:\n",
    "                all_scores.append(line.split(\",\"))\n",
    "        \n",
    "    print(\"PREDICTING FOR SE2...\")\n",
    "    se2_scores = ['SE2']\n",
    "\n",
    "    input_path = '../data/Evaluation_Datasets/senseval2/senseval2.data.xml'\n",
    "    output_path = '../data/Evaluation_Datasets/senseval2'\n",
    "    gold_file =  '../data/Evaluation_Datasets/senseval2/senseval2.gold.key.txt'\n",
    "\n",
    "    pred_babelnet = '../data/Evaluation_Datasets/senseval2/pred_babelnet.txt'\n",
    "    pred_lex = '../data/Evaluation_Datasets/senseval2/pred_lex.txt'\n",
    "    pred_domains = '../data/Evaluation_Datasets/senseval2/pred_domains.txt'\n",
    "\n",
    "    predict_babelnet(input_path, output_path, resources_path)\n",
    "    score = score_predict1(pred_babelnet, gold_file, bn2wn_mapping_file)\n",
    "    se2_scores.append(score*100)\n",
    "\n",
    "    predict_lexicographer(input_path, output_path, resources_path)\n",
    "    score = score_predict_lex(pred_lex, gold_file, resources_path)\n",
    "    se2_scores.append(score*100)\n",
    "\n",
    "    predict_wordnet_domains(input_path, output_path, resources_path)\n",
    "    score = score_predict_dom(pred_domains, gold_file, resources_path)\n",
    "    se2_scores.append(score*100)\n",
    "\n",
    "    all_scores.append(se2_scores)\n",
    "    \n",
    "\n",
    "    #########################################################\n",
    "\n",
    "    print(\"\\n\\nPREDICTING FOR SE3...\")\n",
    "    se3_scores = ['SE3_(Dev)']\n",
    "\n",
    "    input_path = '../data/Evaluation_Datasets/senseval3/senseval3.data.xml'\n",
    "    output_path = '../data/Evaluation_Datasets/senseval3'\n",
    "    gold_file =  '../data/Evaluation_Datasets/senseval3/senseval3.gold.key.txt'\n",
    "\n",
    "    pred_babelnet = '../data/Evaluation_Datasets/senseval3/pred_babelnet.txt'\n",
    "    pred_lex = '../data/Evaluation_Datasets/senseval3/pred_lex.txt'\n",
    "    pred_domains = '../data/Evaluation_Datasets/senseval3/pred_domains.txt'\n",
    "\n",
    "    predict_babelnet(input_path, output_path, resources_path)\n",
    "    score = score_predict1(pred_babelnet, gold_file, bn2wn_mapping_file)\n",
    "    se3_scores.append(score*100)\n",
    "\n",
    "    predict_lexicographer(input_path, output_path, resources_path)\n",
    "    score = score_predict_lex(pred_lex, gold_file, resources_path)\n",
    "    se3_scores.append(score*100)\n",
    "\n",
    "    predict_wordnet_domains(input_path, output_path, resources_path)\n",
    "    score = score_predict_dom(pred_domains, gold_file, resources_path)\n",
    "    se3_scores.append(score*100)\n",
    "\n",
    "    all_scores.append(se3_scores)\n",
    "    \n",
    "    \n",
    "    ###########################################################\n",
    "        \n",
    "    with open(score_file, \"w\") as file:\n",
    "        for scores in all_scores:\n",
    "            file.write(\"{},{:.2f},{:.2f},{:.2f}\\n\".format(scores[0], float(scores[1]), float(scores[2]), float(scores[3])))\n",
    "\n",
    "\n",
    "\n",
    "def run_tests2(score_file):\n",
    "    \n",
    "    resources_path = '../resources'\n",
    "    bn2wn_mapping_file = '../resources/babelnet2wordnet.tsv'\n",
    "    \n",
    "    all_scores = []\n",
    "    if (os.path.isfile(score_file)):\n",
    "        with open(score_file, 'r') as file:\n",
    "            for line in file:\n",
    "                all_scores.append(line.split(\",\"))\n",
    "                \n",
    "    \n",
    "    print(\"\\n\\nPREDICTING FOR SE07...\")\n",
    "    se07_scores = ['SE07']\n",
    "    \n",
    "    input_path = '../data/Evaluation_Datasets/semeval2007/semeval2007.data.xml'\n",
    "    output_path = '../data/Evaluation_Datasets/semeval2007'\n",
    "    gold_file =  '../data/Evaluation_Datasets/semeval2007/semeval2007.gold.key.txt'\n",
    "    \n",
    "    pred_babelnet = '../data/Evaluation_Datasets/semeval2007/pred_babelnet.txt'\n",
    "    pred_lex = '../data/Evaluation_Datasets/semeval2007/pred_lex.txt'\n",
    "    pred_domains = '../data/Evaluation_Datasets/semeval2007/pred_domains.txt'\n",
    "\n",
    "    predict_babelnet(input_path, output_path, resources_path)\n",
    "    score = score_predict1(pred_babelnet, gold_file, bn2wn_mapping_file)\n",
    "    se07_scores.append(score*100)\n",
    "    \n",
    "    predict_lexicographer(input_path, output_path, resources_path)\n",
    "    score = score_predict_lex(pred_lex, gold_file, resources_path)\n",
    "    se07_scores.append(score*100)\n",
    "    \n",
    "    predict_wordnet_domains(input_path, output_path, resources_path)\n",
    "    score = score_predict_dom(pred_domains, gold_file, resources_path)\n",
    "    se07_scores.append(score*100)\n",
    "    \n",
    "    all_scores.append(se07_scores)\n",
    "    \n",
    "    #########################################################\n",
    "    \n",
    "    print(\"\\n\\nPREDICTING FOR SE13...\")\n",
    "    se13_scores = ['SE13']\n",
    "    \n",
    "    input_path = '../data/Evaluation_Datasets/semeval2013/semeval2013.data.xml'\n",
    "    output_path = '../data/Evaluation_Datasets/semeval2013'\n",
    "    gold_file =  '../data/Evaluation_Datasets/semeval2013/semeval2013.gold.key.txt'\n",
    "    \n",
    "    pred_babelnet = '../data/Evaluation_Datasets/semeval2013/pred_babelnet.txt'\n",
    "    pred_lex = '../data/Evaluation_Datasets/semeval2013/pred_lex.txt'\n",
    "    pred_domains = '../data/Evaluation_Datasets/semeval2013/pred_domains.txt'\n",
    "\n",
    "    predict_babelnet(input_path, output_path, resources_path)\n",
    "    score = score_predict1(pred_babelnet, gold_file, bn2wn_mapping_file)\n",
    "    se13_scores.append(score*100)\n",
    "    \n",
    "    predict_lexicographer(input_path, output_path, resources_path)\n",
    "    score = score_predict_lex(pred_lex, gold_file, resources_path)\n",
    "    se13_scores.append(score*100)\n",
    "    \n",
    "    predict_wordnet_domains(input_path, output_path, resources_path)\n",
    "    score = score_predict_dom(pred_domains, gold_file, resources_path)\n",
    "    se13_scores.append(score*100)\n",
    "    \n",
    "    all_scores.append(se13_scores)\n",
    "    \n",
    "    #########################################################\n",
    "    \n",
    "    \n",
    "    with open(score_file, \"w\") as file:\n",
    "        for scores in all_scores:\n",
    "            file.write(\"{},{:.2f},{:.2f},{:.2f}\\n\".format(scores[0], float(scores[1]), float(scores[2]), float(scores[3])))\n",
    "            \n",
    "            \n",
    "            \n",
    "    \n",
    "    \n",
    "\n",
    "def run_tests3(score_file):\n",
    "    \n",
    "    resources_path = '../resources'\n",
    "    bn2wn_mapping_file = '../resources/babelnet2wordnet.tsv'\n",
    "    \n",
    "    all_scores = []\n",
    "    if (os.path.isfile(score_file)):\n",
    "        with open(score_file, 'r') as file:\n",
    "            for line in file:\n",
    "                all_scores.append(line.split(\",\"))\n",
    "                \n",
    "    \n",
    "    print(\"\\n\\nPREDICTING FOR SE15...\")\n",
    "    se15_scores = ['SE15']\n",
    "    \n",
    "    input_path = '../data/Evaluation_Datasets/semeval2015/semeval2015.data.xml'\n",
    "    output_path = '../data/Evaluation_Datasets/semeval2015'\n",
    "    gold_file =  '../data/Evaluation_Datasets/semeval2015/semeval2015.gold.key.txt'\n",
    "    \n",
    "    pred_babelnet = '../data/Evaluation_Datasets/semeval2015/pred_babelnet.txt'\n",
    "    pred_lex = '../data/Evaluation_Datasets/semeval2015/pred_lex.txt'\n",
    "    pred_domains = '../data/Evaluation_Datasets/semeval2015/pred_domains.txt'\n",
    "\n",
    "    predict_babelnet(input_path, output_path, resources_path)\n",
    "    score = score_predict1(pred_babelnet, gold_file, bn2wn_mapping_file)\n",
    "    se15_scores.append(score*100)\n",
    "    \n",
    "    predict_lexicographer(input_path, output_path, resources_path)\n",
    "    score = score_predict_lex(pred_lex, gold_file, resources_path)\n",
    "    se15_scores.append(score*100)\n",
    "    \n",
    "    predict_wordnet_domains(input_path, output_path, resources_path)\n",
    "    score = score_predict_dom(pred_domains, gold_file, resources_path)\n",
    "    se15_scores.append(score*100)\n",
    "    \n",
    "    all_scores.append(se15_scores)\n",
    "    \n",
    "    \n",
    "    #####################################################################\n",
    "    \n",
    "    \n",
    "    print(\"\\n\\nPREDICTING FOR ALL...\")\n",
    "    se_ALL_scores = ['ALL']\n",
    "    \n",
    "    input_path = '../data/Evaluation_Datasets/ALL/ALL.data.xml'\n",
    "    output_path = '../data/Evaluation_Datasets/ALL'\n",
    "    gold_file =  '../data/Evaluation_Datasets/ALL/ALL.gold.key.txt'\n",
    "    \n",
    "    pred_babelnet = '../data/Evaluation_Datasets/ALL/pred_babelnet.txt'\n",
    "    pred_lex = '../data/Evaluation_Datasets/ALL/pred_lex.txt'\n",
    "    pred_domains = '../data/Evaluation_Datasets/ALL/pred_domains.txt'\n",
    "\n",
    "    predict_babelnet(input_path, output_path, resources_path)\n",
    "    score = score_predict1(pred_babelnet, gold_file, bn2wn_mapping_file)\n",
    "    se_ALL_scores.append(score*100)\n",
    "    \n",
    "    predict_lexicographer(input_path, output_path, resources_path)\n",
    "    score = score_predict_lex(pred_lex, gold_file, resources_path)\n",
    "    se_ALL_scores.append(score*100)\n",
    "    \n",
    "    predict_wordnet_domains(input_path, output_path, resources_path)\n",
    "    score = score_predict_dom(pred_domains, gold_file, resources_path)\n",
    "    se_ALL_scores.append(score*100)\n",
    "    \n",
    "    all_scores.append(se_ALL_scores)\n",
    "    \n",
    "    \n",
    "    with open(score_file, \"w\") as file:\n",
    "        for scores in all_scores:\n",
    "            file.write(\"{},{:.2f},{:.2f},{:.2f}\\n\".format(scores[0], float(scores[1]), float(scores[2]), float(scores[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 386619,
     "status": "ok",
     "timestamp": 1568340555744,
     "user": {
      "displayName": "Sayo Makinwa",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mB01ONwXql1XhFq6AK_2PjHkUfJA3VDpx1AVsLimg=s64",
      "userId": "02832581640686072550"
     },
     "user_tz": -120
    },
    "id": "kViXUjmsEydk",
    "outputId": "db46df2a-a649-44b1-debd-759039e09444"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "PREDICTING FOR SE15...\n",
      "LOADING RESOURCES...\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py:3794: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "PREPARING EVALUATION DATA FOR PREDICTION...\n",
      "138 sentences extracted...\n",
      "Evaluation data extraction completed\n",
      "Predicting (line by line) and writing to file... This may take a little while...\n",
      "100/138 lines done... A little moment more and everything will be done! :)\n",
      "Prediction complete!\n",
      "LOADING RESOURCES...\n",
      "PREPARING EVALUATION DATA FOR PREDICTION...\n",
      "138 sentences extracted...\n",
      "Evaluation data extraction completed\n",
      "Predicting (line by line) and writing to file... This may take a little while...\n",
      "100/138 lines done... A little moment more and everything will be done! :)\n",
      "Prediction complete!\n",
      "LOADING RESOURCES...\n",
      "PREPARING EVALUATION DATA FOR PREDICTION...\n",
      "138 sentences extracted...\n",
      "Evaluation data extraction completed\n",
      "Predicting (line by line) and writing to file... This may take a little while...\n",
      "100/138 lines done... A little moment more and everything will be done! :)\n",
      "Prediction complete!\n",
      "\n",
      "\n",
      "PREDICTING FOR ALL...\n",
      "LOADING RESOURCES...\n",
      "PREPARING EVALUATION DATA FOR PREDICTION...\n",
      "1,173 sentences extracted...\n",
      "Evaluation data extraction completed\n",
      "Predicting (line by line) and writing to file... This may take a little while...\n",
      "100/1173 lines done... A little moment more and everything will be done! :)\n",
      "200/1173 lines done... A little moment more and everything will be done! :)\n",
      "300/1173 lines done... A little moment more and everything will be done! :)\n",
      "400/1173 lines done... A little moment more and everything will be done! :)\n",
      "500/1173 lines done... A little moment more and everything will be done! :)\n",
      "600/1173 lines done... A little moment more and everything will be done! :)\n",
      "700/1173 lines done... A little moment more and everything will be done! :)\n",
      "800/1173 lines done... A little moment more and everything will be done! :)\n",
      "900/1173 lines done... A little moment more and everything will be done! :)\n",
      "1000/1173 lines done... A little moment more and everything will be done! :)\n",
      "1100/1173 lines done... A little moment more and everything will be done! :)\n",
      "Prediction complete!\n",
      "LOADING RESOURCES...\n",
      "PREPARING EVALUATION DATA FOR PREDICTION...\n",
      "1,173 sentences extracted...\n",
      "Evaluation data extraction completed\n",
      "Predicting (line by line) and writing to file... This may take a little while...\n",
      "100/1173 lines done... A little moment more and everything will be done! :)\n",
      "200/1173 lines done... A little moment more and everything will be done! :)\n",
      "300/1173 lines done... A little moment more and everything will be done! :)\n",
      "400/1173 lines done... A little moment more and everything will be done! :)\n",
      "500/1173 lines done... A little moment more and everything will be done! :)\n",
      "600/1173 lines done... A little moment more and everything will be done! :)\n",
      "700/1173 lines done... A little moment more and everything will be done! :)\n",
      "800/1173 lines done... A little moment more and everything will be done! :)\n",
      "900/1173 lines done... A little moment more and everything will be done! :)\n",
      "1000/1173 lines done... A little moment more and everything will be done! :)\n",
      "1100/1173 lines done... A little moment more and everything will be done! :)\n",
      "Prediction complete!\n",
      "LOADING RESOURCES...\n",
      "PREPARING EVALUATION DATA FOR PREDICTION...\n",
      "1,173 sentences extracted...\n",
      "Evaluation data extraction completed\n",
      "Predicting (line by line) and writing to file... This may take a little while...\n",
      "100/1173 lines done... A little moment more and everything will be done! :)\n",
      "200/1173 lines done... A little moment more and everything will be done! :)\n",
      "300/1173 lines done... A little moment more and everything will be done! :)\n",
      "400/1173 lines done... A little moment more and everything will be done! :)\n",
      "500/1173 lines done... A little moment more and everything will be done! :)\n",
      "600/1173 lines done... A little moment more and everything will be done! :)\n",
      "700/1173 lines done... A little moment more and everything will be done! :)\n",
      "800/1173 lines done... A little moment more and everything will be done! :)\n",
      "900/1173 lines done... A little moment more and everything will be done! :)\n",
      "1000/1173 lines done... A little moment more and everything will be done! :)\n",
      "1100/1173 lines done... A little moment more and everything will be done! :)\n",
      "Prediction complete!\n"
     ]
    }
   ],
   "source": [
    "#run_tests1(\"../resources/scores.csv\")\n",
    "#run_tests2(\"../resources/scores.csv\")\n",
    "run_tests3(\"../resources/scores.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UCRLDwd0SVtX"
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 976,
     "status": "ok",
     "timestamp": 1568342191890,
     "user": {
      "displayName": "Sayo Makinwa",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mB01ONwXql1XhFq6AK_2PjHkUfJA3VDpx1AVsLimg=s64",
      "userId": "02832581640686072550"
     },
     "user_tz": -120
    },
    "id": "Uzvkm0_WtRyd",
    "outputId": "7588f2b6-6370-4510-b0e2-01f3b89df55b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Babelnet</th>\n",
       "      <th>Lex</th>\n",
       "      <th>Domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SE2</th>\n",
       "      <td>58.76</td>\n",
       "      <td>75.85</td>\n",
       "      <td>86.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SE3_(Dev)</th>\n",
       "      <td>60.65</td>\n",
       "      <td>75.14</td>\n",
       "      <td>84.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SE07</th>\n",
       "      <td>49.89</td>\n",
       "      <td>67.25</td>\n",
       "      <td>86.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SE13</th>\n",
       "      <td>58.58</td>\n",
       "      <td>69.65</td>\n",
       "      <td>75.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SE15</th>\n",
       "      <td>52.25</td>\n",
       "      <td>69.08</td>\n",
       "      <td>79.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALL</th>\n",
       "      <td>57.73</td>\n",
       "      <td>72.77</td>\n",
       "      <td>82.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Babelnet    Lex  Domain\n",
       "SE2           58.76  75.85   86.90\n",
       "SE3_(Dev)     60.65  75.14   84.81\n",
       "SE07          49.89  67.25   86.15\n",
       "SE13          58.58  69.65   75.55\n",
       "SE15          52.25  69.08   79.84\n",
       "ALL           57.73  72.77   82.75"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "all_scores_pd = pd.read_csv('../resources/scores.csv', names=['Babelnet', 'Lex', 'Domain'])\n",
    "all_scores_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "320gADzoUcaT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "200k_data_baseline_model_A.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
