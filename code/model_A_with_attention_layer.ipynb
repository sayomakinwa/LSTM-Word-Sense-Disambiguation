{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"colab":{"name":"model_A_with_attention_layer.ipynb","version":"0.3.2","provenance":[{"file_id":"1ztcJt_LgDIH_Umi5lGsDDBDB86ju8P36","timestamp":1567821932880},{"file_id":"1UCBYAjzB3OqMVVwR6-gJ23VgMpVvN6Ql","timestamp":1567817661516}],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"xg-wSpUBU7gx","colab_type":"text"},"source":["## Mount Drive and Imports"]},{"cell_type":"code","metadata":{"id":"NKtsqriiKxYd","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nRTrAorEKyu5","colab_type":"code","outputId":"57f61e0a-8ced-4af4-ef02-1276acd91265","executionInfo":{"status":"ok","timestamp":1568336588657,"user_tz":-120,"elapsed":1115,"user":{"displayName":"Sayo Makinwa","photoUrl":"","userId":"02017936482338602411"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%cd /content/drive/My Drive/nlp_hw3/code"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/nlp_hw3/code\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OxdkmYCxEybs","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"8ac0fac6-f2c7-4dc4-89be-7c55d93b1ea1","executionInfo":{"status":"ok","timestamp":1568336590876,"user_tz":-120,"elapsed":2275,"user":{"displayName":"Sayo Makinwa","photoUrl":"","userId":"02017936482338602411"}}},"source":["import os\n","import numpy as np\n","from typing import Tuple, List, Dict\n","\n","import tensorflow as tf\n","import keras\n","import keras as K\n","from keras.layers import *\n","from keras.models import *\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.callbacks import TensorBoard\n","from keras.regularizers import l2\n","from keras.optimizers import Adam\n","from tensorflow.python.eager import context\n","\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","import json\n","import pandas as pd\n","\n","import my_utils as utils\n","import corpora"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"s2DfqlkrVuTu","colab_type":"text"},"source":["## Preprocessing"]},{"cell_type":"code","metadata":{"id":"042xGwFcEyb1","colab_type":"code","colab":{}},"source":["def load_train_dataset(input_path: str, y_path: str) -> Tuple[List[str], List[str]]:\n","    \"\"\"\n","    :param input_path; Path to the input dataset\n","    :param label_path; Path to the file containing the corresponding labels for the input dataset\n","    :return sentences; List of sentences in input_file\n","    :return labels; List of corresponding word segment codes in label_path. Same len as sentences\n","    \"\"\"\n","    sentences = []\n","    k = 0\n","    with open(input_path, \"r\", encoding=\"utf-8-sig\") as file:\n","        for line in file:\n","            k += 1\n","            sentences.append(line.strip())\n","#             if (k >= 4000):\n","#                 break\n","\n","    y = []\n","    k = 0\n","    with open(y_path, \"r\", encoding=\"utf-8-sig\") as file:\n","        for line in file:\n","            k += 1\n","            y.append(line.strip())\n","#             if (k >= 4000):\n","#                 break\n","\n","    return sentences, y\n","\n","\n","def make_X_vocab(sentences: List[str]) -> Dict[str, int]:\n","    '''\n","    :param sentences; List of input sentences from the dataset\n","    :return unigrams_vocab; Dictionary from unigram to int\n","    :return bigrams_vocab; Dictionary from bigram to int\n","    '''\n","    vocab = {\"UNK\": 0}\n","\n","    for sentence in sentences:\n","        for word in sentence.split():\n","            if word not in vocab:\n","                vocab[word] = len(vocab)\n","    \n","    return vocab\n","\n","\n","def make_Y_vocab(y: List[str]) -> Dict[str, int]:\n","    \"\"\"\n","    :param labels; List of label codes\n","    :return labels_vocab; Dictionary from label code to int \n","    \"\"\"\n","    y_vocab = {\"UNK\": 0}\n","    \n","    for y_line in y:\n","        for y_word in y_line.split():\n","            if y_word not in y_vocab:\n","                y_vocab[y_word] = len(y_vocab)\n","                \n","    return y_vocab\n","\n","def make_Y(output: List[str], output_vocab: Dict[str, int]) -> np.ndarray:\n","    \"\"\"\n","    :param labels; List of word segment codes, line by line\n","    :param labels_vocab; Label codes vocab\n","    :return y; Vector of label code indices\n","    \"\"\"\n","    y = []\n","    for output_line in output:\n","        y_temp = []\n","        for single_output in output_line.split():\n","            if single_output in output_vocab:\n","                y_temp.append( output_vocab[single_output])\n","            else:\n","                y_temp.append( output_vocab[\"OTHERS\"])\n","        y.append(np.array(y_temp))\n","    \n","    return np.array(y)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nUaRjng4F7pp","colab_type":"code","colab":{}},"source":["train_data_path = '../data/Training_Corpora/semcor'\n","test_data_path = '../data/Evaluation_Datasets/senseval3'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nDelCEWdWFxz","colab_type":"code","colab":{}},"source":["#Parse training data\n","corpora_xml_path = train_data_path + '/semcor.data.xml'\n","gold_mapping_path = train_data_path + '/semcor.gold.key.txt'\n","resources_path = '../resources/'\n","\n","corpora.extract_training_data(corpora_xml_path, gold_mapping_path, resources_path, train_data_path)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KSpdbW_QWFkd","colab_type":"code","colab":{}},"source":["#Parse validation data\n","corpora_xml_path = test_data_path + '/senseval3.data.xml'\n","gold_mapping_path = test_data_path + '/senseval3.gold.key.txt'\n","resources_path = '../resources/'\n","\n","corpora.extract_training_data(corpora_xml_path, gold_mapping_path, resources_path, test_data_path)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yVV_9dxlEyb4","colab_type":"code","colab":{}},"source":["sentences, y = load_train_dataset(train_data_path+\"/trainX.txt\", train_data_path+\"/trainy.txt\")\n","test_sentences, test_y = load_train_dataset(test_data_path+\"/trainX.txt\", test_data_path+\"/trainy.txt\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-EJ1fHSzEyb7","colab_type":"code","outputId":"8202b320-239b-4967-f2a5-472f0f6caad7","executionInfo":{"status":"ok","timestamp":1568322326857,"user_tz":-120,"elapsed":981,"user":{"displayName":"Sayo Makinwa","photoUrl":"","userId":"02017936482338602411"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["print(len(sentences))\n","print(sentences[0])"],"execution_count":7,"outputs":[{"output_type":"stream","text":["37168\n","How long has it been since you reviewed the objectives of your benefit and service program\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"t1fD1tgDEycC","colab_type":"code","outputId":"32ec3fd9-cc2c-4c3c-ff7b-35001833ba36","executionInfo":{"status":"ok","timestamp":1568322326858,"user_tz":-120,"elapsed":573,"user":{"displayName":"Sayo Makinwa","photoUrl":"","userId":"02017936482338602411"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["print(len(y))\n","print(y[0])"],"execution_count":8,"outputs":[{"output_type":"stream","text":["37168\n","how bn:00106124a have it bn:00083181v since you bn:00092618v the bn:00002179n of you bn:00009904n and bn:00070654n bn:00064646n\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HgCzdmbLEycF","colab_type":"code","outputId":"5b30c43e-679e-41e2-fe22-f893d84d2c8c","executionInfo":{"status":"ok","timestamp":1568322328391,"user_tz":-120,"elapsed":1246,"user":{"displayName":"Sayo Makinwa","photoUrl":"","userId":"02017936482338602411"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["vocab = make_X_vocab(sentences + test_sentences)\n","print(len(vocab))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["48989\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LPRR9TA7EycH","colab_type":"code","outputId":"a69cf186-2c24-419d-c70d-a4968215f624","executionInfo":{"status":"ok","timestamp":1568322328928,"user_tz":-120,"elapsed":1244,"user":{"displayName":"Sayo Makinwa","photoUrl":"","userId":"02017936482338602411"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["y_vocab = make_Y_vocab(y + test_y)\n","print(len(y_vocab))"],"execution_count":10,"outputs":[{"output_type":"stream","text":["49224\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iD6TeqJREycR","colab_type":"code","colab":{}},"source":["X = utils.make_X(sentences, vocab)\n","y_array = make_Y(y, y_vocab)\n","\n","X_test = utils.make_X(test_sentences, vocab)\n","y_test_array = make_Y(test_y, y_vocab)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DF5m68nmEycT","colab_type":"code","outputId":"a1e27115-8885-4576-d496-707007a22eeb","executionInfo":{"status":"ok","timestamp":1568322330074,"user_tz":-120,"elapsed":1313,"user":{"displayName":"Sayo Makinwa","photoUrl":"","userId":"02017936482338602411"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["X.shape"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(37168,)"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"j7fg-usHEycZ","colab_type":"code","colab":{}},"source":["train_x = pad_sequences(X, truncating='pre', padding='post', maxlen=30)\n","train_y = pad_sequences(y_array, truncating='pre', padding='post', maxlen=30)\n","\n","dev_x = pad_sequences(X_test, truncating='pre', padding='post', maxlen=30)\n","dev_y = pad_sequences(y_test_array, truncating='pre', padding='post', maxlen=30)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NmXFcuk4Eycc","colab_type":"code","outputId":"193497c7-ea2f-4902-a6b4-34f9a5ca968a","executionInfo":{"status":"ok","timestamp":1568322330438,"user_tz":-120,"elapsed":541,"user":{"displayName":"Sayo Makinwa","photoUrl":"","userId":"02017936482338602411"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["print(train_x.shape)\n","print(train_y.shape)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["(37168, 30)\n","(37168, 30)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YIojgrqREycg","colab_type":"code","outputId":"502e9d3d-9706-4d8f-86db-bc6bfb41c3fd","executionInfo":{"status":"ok","timestamp":1568322331603,"user_tz":-120,"elapsed":822,"user":{"displayName":"Sayo Makinwa","photoUrl":"","userId":"02017936482338602411"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["train_y = train_y.reshape((*train_y.shape, 1))\n","dev_y = dev_y.reshape((*dev_y.shape, 1))\n","print(train_y.shape)\n","print(dev_y.shape)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["(37168, 30, 1)\n","(352, 30, 1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ABjb_KPbEyc1","colab_type":"code","outputId":"a6e46b57-a8c1-4364-e2c9-c0dd242255ee","executionInfo":{"status":"ok","timestamp":1568322332788,"user_tz":-120,"elapsed":1049,"user":{"displayName":"Sayo Makinwa","photoUrl":"","userId":"02017936482338602411"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["print(train_x.shape)\n","print(train_y.shape)\n","print(dev_x.shape)\n","print(dev_y.shape)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["(37168, 30)\n","(37168, 30, 1)\n","(352, 30)\n","(352, 30, 1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MkMfJ1HTYD9f","colab_type":"text"},"source":["## Building the model"]},{"cell_type":"code","metadata":{"id":"F_DjnIsqLHbR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":190},"outputId":"8cd27f23-ca05-4172-fcdc-9d458372faf3","executionInfo":{"status":"ok","timestamp":1568322338363,"user_tz":-120,"elapsed":4978,"user":{"displayName":"Sayo Makinwa","photoUrl":"","userId":"02017936482338602411"}}},"source":["pip install keras-self-attention"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: keras-self-attention in /usr/local/lib/python3.6/dist-packages (0.42.0)\n","Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-self-attention) (2.2.5)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-self-attention) (1.16.5)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.3.1)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.12.0)\n","Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.0.8)\n","Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.1.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (2.8.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (3.13)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YL8G9kFGLXHh","colab_type":"code","colab":{}},"source":["from keras_self_attention import SeqSelfAttention"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gd-bBhRDEyc-","colab_type":"code","colab":{}},"source":["vocab_size = len(vocab)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9uF52K_8EydA","colab_type":"code","colab":{}},"source":["#This class helps with logging\n","\n","class TrainValTensorBoard(TensorBoard):\n","    def __init__(self, log_dir='./logs', **kwargs):\n","        self.val_log_dir = os.path.join(log_dir, 'A_attention/validation')\n","        training_log_dir = os.path.join(log_dir, 'A_attention/training')\n","        super(TrainValTensorBoard, self).__init__(training_log_dir, **kwargs)\n","\n","    def set_model(self, model):\n","        if context.executing_eagerly():\n","            self.val_writer = tf.contrib.summary.create_file_writer(self.val_log_dir)\n","        else:\n","            self.val_writer = tf.summary.FileWriter(self.val_log_dir)\n","        super(TrainValTensorBoard, self).set_model(model)\n","\n","    def _write_custom_summaries(self, step, logs=None):\n","        logs = logs or {}\n","        val_logs = {k.replace('val_', ''): v for k, v in logs.items() if 'val_' in k}\n","        if context.executing_eagerly():\n","            with self.val_writer.as_default(), tf.contrib.summary.always_record_summaries():\n","                for name, value in val_logs.items():\n","                    tf.contrib.summary.scalar(name, value.item(), step=step)\n","        else:\n","            for name, value in val_logs.items():\n","                summary = tf.Summary()\n","                summary_value = summary.value.add()\n","                summary_value.simple_value = value.item()\n","                summary_value.tag = name\n","                self.val_writer.add_summary(summary, step)\n","        self.val_writer.flush()\n","\n","        logs = {k: v for k, v in logs.items() if not 'val_' in k}\n","        super(TrainValTensorBoard, self)._write_custom_summaries(step, logs)\n","\n","    def on_train_end(self, logs=None):\n","        super(TrainValTensorBoard, self).on_train_end(logs)\n","        self.val_writer.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fXhtH-HhEydE","colab_type":"code","colab":{}},"source":["#Please take note that most of this part was extracted from class exercises, with some additions\n","      \n","def create_keras_model(vocab_size, y_size, embedding_size=128, hidden_size=512):\n","    print(\"Creating KERAS model\")\n","    \n","    model = Sequential()\n","    model.add(Embedding(vocab_size, embedding_size, mask_zero=True))\n","    \n","    model.add(Bidirectional(LSTM(hidden_size, dropout=0.3, recurrent_dropout=0.3, return_sequences=True), merge_mode='concat'))\n","    model.add(Bidirectional(LSTM(hidden_size, dropout=0.3, recurrent_dropout=0.3, return_sequences=True), merge_mode='concat'))\n","    \n","    model.add(\n","        SeqSelfAttention(\n","            attention_width=15,\n","            attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n","            attention_activation='softmax',\n","            kernel_regularizer= l2(1e-6),\n","            use_attention_bias=False,\n","            name='Attention',\n","        )\n","    )\n","    \n","    model.add(TimeDistributed(Dense(y_size, activation='softmax')))\n","    optimizer = Adam()\n","    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['acc'])\n","\n","    return model\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PMqneG5WEydI","colab_type":"code","colab":{}},"source":["resource_path = \"../resources/\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pFd4-nloEydL","colab_type":"code","outputId":"98a434a5-10be-4f7d-e528-500a2356cc3b","executionInfo":{"status":"error","timestamp":1568333194770,"user_tz":-120,"elapsed":6877844,"user":{"displayName":"Sayo Makinwa","photoUrl":"","userId":"02017936482338602411"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["batch_size = 64\n","epochs = 20\n","model_name = resource_path+\"modelA_att.hdf5\"\n","\n","#checks if the FINAL model was saved and loads it instead of creating a new one\n","if os.path.exists(model_name):\n","    model = load_model(model_name)\n","    print(\"Using a pre-saved model\")\n","    model.summary()\n","    \n","else:\n","    model = create_keras_model(vocab_size, len(y_vocab))\n","    print(\"Training a new model\")\n","    model.summary()\n","    \n","    filepath = resource_path+\"models/model_A_attention.hdf5\"\n","    checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n","    #callbacks_list = [checkpoint]\n","    tb_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\", histogram_freq=0, write_graph=False, write_images=True)\n","    callbacks_list = [TrainValTensorBoard(write_graph=False), checkpoint]\n","    \n","    print(\"\\nStarting training...\")\n","    model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size,\n","              shuffle=True, validation_data=(dev_x, dev_y), callbacks=callbacks_list) \n","    print(\"Training complete.\\n\")\n","    "],"execution_count":46,"outputs":[{"output_type":"stream","text":["Creating KERAS model\n","Training a new model\n","Model: \"sequential_6\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_6 (Embedding)      (None, None, 128)         6270592   \n","_________________________________________________________________\n","bidirectional_11 (Bidirectio (None, None, 1024)        2625536   \n","_________________________________________________________________\n","bidirectional_12 (Bidirectio (None, None, 1024)        6295552   \n","_________________________________________________________________\n","Attention (SeqSelfAttention) (None, None, 1024)        1048576   \n","_________________________________________________________________\n","time_distributed_3 (TimeDist (None, None, 49224)       50454600  \n","=================================================================\n","Total params: 66,694,856\n","Trainable params: 66,694,856\n","Non-trainable params: 0\n","_________________________________________________________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n","\n","Starting training...\n","Train on 37168 samples, validate on 352 samples\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1122: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1128: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n","\n","Epoch 1/20\n","37168/37168 [==============================] - 524s 14ms/step - loss: 7.5540 - acc: 0.0715 - val_loss: 7.2922 - val_acc: 0.0911\n","Epoch 2/20\n","37168/37168 [==============================] - 522s 14ms/step - loss: 6.4032 - acc: 0.1745 - val_loss: 6.3390 - val_acc: 0.2610\n","Epoch 3/20\n","37168/37168 [==============================] - 522s 14ms/step - loss: 5.3658 - acc: 0.3352 - val_loss: 5.7223 - val_acc: 0.3676\n","Epoch 4/20\n","37168/37168 [==============================] - 522s 14ms/step - loss: 4.7057 - acc: 0.4007 - val_loss: 5.4541 - val_acc: 0.4017\n","Epoch 5/20\n","37168/37168 [==============================] - 523s 14ms/step - loss: 4.2025 - acc: 0.4344 - val_loss: 5.2824 - val_acc: 0.4266\n","Epoch 6/20\n","37168/37168 [==============================] - 522s 14ms/step - loss: 3.7238 - acc: 0.4649 - val_loss: 5.1349 - val_acc: 0.4540\n","Epoch 7/20\n","37168/37168 [==============================] - 520s 14ms/step - loss: 3.2563 - acc: 0.4974 - val_loss: 5.0354 - val_acc: 0.4715\n","Epoch 8/20\n","37168/37168 [==============================] - 526s 14ms/step - loss: 2.8050 - acc: 0.5427 - val_loss: 4.9559 - val_acc: 0.4769\n","Epoch 9/20\n","37168/37168 [==============================] - 521s 14ms/step - loss: 2.4174 - acc: 0.5854 - val_loss: 4.9239 - val_acc: 0.4891\n","Epoch 10/20\n","37168/37168 [==============================] - 521s 14ms/step - loss: 2.0798 - acc: 0.6232 - val_loss: 4.8486 - val_acc: 0.4903\n","Epoch 11/20\n","37168/37168 [==============================] - 520s 14ms/step - loss: 1.8076 - acc: 0.6583 - val_loss: 4.7958 - val_acc: 0.4949\n","Epoch 12/20\n","37168/37168 [==============================] - 520s 14ms/step - loss: 1.5906 - acc: 0.6871 - val_loss: 4.7782 - val_acc: 0.5017\n","Epoch 13/20\n","37168/37168 [==============================] - 515s 14ms/step - loss: 1.3799 - acc: 0.7204 - val_loss: 4.7145 - val_acc: 0.5090\n","Epoch 14/20\n","37168/37168 [==============================] - 515s 14ms/step - loss: 1.1983 - acc: 0.7514 - val_loss: 4.6713 - val_acc: 0.5157\n","Epoch 15/20\n","37168/37168 [==============================] - 512s 14ms/step - loss: 1.0490 - acc: 0.7783 - val_loss: 4.6442 - val_acc: 0.5179\n","Epoch 16/20\n","37168/37168 [==============================] - 511s 14ms/step - loss: 0.9133 - acc: 0.8051 - val_loss: 4.6144 - val_acc: 0.5234\n","Epoch 17/20\n","37168/37168 [==============================] - 510s 14ms/step - loss: 0.8055 - acc: 0.8269 - val_loss: 4.6112 - val_acc: 0.5218\n","Epoch 18/20\n","37168/37168 [==============================] - 510s 14ms/step - loss: 0.7106 - acc: 0.8465 - val_loss: 4.6049 - val_acc: 0.5244\n","Epoch 19/20\n","37168/37168 [==============================] - 514s 14ms/step - loss: 0.6266 - acc: 0.8642 - val_loss: 4.5923 - val_acc: 0.5240\n","Epoch 20/20\n","37168/37168 [==============================] - 512s 14ms/step - loss: 0.5676 - acc: 0.8779 - val_loss: 4.5644 - val_acc: 0.5289\n"],"name":"stdout"},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-46-17765313006f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nStarting training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size,\n\u001b[0;32m---> 24\u001b[0;31m               shuffle=True, validation_data=(dev_x, dev_y), callbacks=callbacks_list) \n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training complete.\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_end_hook\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Helper function for on_{train|test|predict}_end methods.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_TRAIN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_TEST\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_end\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \"\"\"\n\u001b[1;32m    228\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_test_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_end\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    938\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ckpt_saved_epoch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m       \u001b[0;31m# Make `_ckpt_saved_epoch` attribute `None` at the end of training as it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m       \u001b[0;31m# is only used during the training. Currently it is decided not to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute '_ckpt_saved_epoch'"]}]},{"cell_type":"code","metadata":{"id":"8N18junO9AVV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"7f51a1c9-5766-44ed-87b5-e78ef461de94","executionInfo":{"status":"ok","timestamp":1568333403163,"user_tz":-120,"elapsed":8012,"user":{"displayName":"Sayo Makinwa","photoUrl":"","userId":"02017936482338602411"}}},"source":["#Save the FINAL model for later reuse\n","model.save(model_name)\n","print(\"Trained model saved for later use\")\n","\n","print(\"\\nEvaluating test...\")\n","loss_acc = model.evaluate(dev_x, dev_y, verbose=0)\n","print(\"Test data: loss = %0.6f  accuracy = %0.2f%% \" % (loss_acc[0], loss_acc[1]*100))"],"execution_count":48,"outputs":[{"output_type":"stream","text":["Trained model saved for later use\n","\n","Evaluating test...\n","Test data: loss = 4.543067  accuracy = 53.47% \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ztDgLLxJEydO","colab_type":"code","colab":{}},"source":["#Writing the vocabularies to file\n","\n","with open(resource_path+\"A_att_x_vocab.txt\", \"w\") as file:\n","    file.write(json.dumps(vocab))\n","    \n","with open(resource_path+\"A_att_y_vocab.txt\", \"w\") as file:\n","    file.write(json.dumps(y_vocab))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oqYFyp-bYJpR","colab_type":"text"},"source":["## Running Predictions"]},{"cell_type":"code","metadata":{"id":"FqZyKhGwEydR","colab_type":"code","colab":{}},"source":["from predict_att import *\n","from score import *"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7DjVFy75Cr1J","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":190},"outputId":"bfcc1028-abf5-45d0-cc9e-578c6cf68f1f","executionInfo":{"status":"ok","timestamp":1568336601708,"user_tz":-120,"elapsed":4437,"user":{"displayName":"Sayo Makinwa","photoUrl":"","userId":"02017936482338602411"}}},"source":["pip install keras-self-attention"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: keras-self-attention in /usr/local/lib/python3.6/dist-packages (0.42.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-self-attention) (1.16.5)\n","Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-self-attention) (2.2.5)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.3.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (2.8.0)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.12.0)\n","Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.0.8)\n","Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.1.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (3.13)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"T4DYUwkGnSt-","colab_type":"code","outputId":"0073887b-fa4c-4745-eaa7-cf7557095784","executionInfo":{"status":"ok","timestamp":1568336601710,"user_tz":-120,"elapsed":3834,"user":{"displayName":"Sayo Makinwa","photoUrl":"","userId":"02017936482338602411"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["import nltk\n","nltk.download(\"wordnet\")"],"execution_count":6,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"K6-_p-mLCuEW","colab_type":"code","colab":{}},"source":["from keras_self_attention import SeqSelfAttention"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"p9G98b-X4Uzz","colab_type":"code","colab":{}},"source":["def run_tests1(score_file):\n","    \n","    resources_path = '../resources'\n","    bn2wn_mapping_file = '../resources/babelnet2wordnet.tsv'\n","    \n","    all_scores = []\n","    if (os.path.isfile(score_file)):\n","        with open(score_file, 'r') as file:\n","            for line in file:\n","                all_scores.append(line.split(\",\"))\n","        \n","    print(\"PREDICTING FOR SE2...\")\n","    se2_scores = ['SE2']\n","\n","    input_path = '../data/Evaluation_Datasets/senseval2/senseval2.data.xml'\n","    output_path = '../data/Evaluation_Datasets/senseval2'\n","    gold_file =  '../data/Evaluation_Datasets/senseval2/senseval2.gold.key.txt'\n","\n","    pred_babelnet = '../data/Evaluation_Datasets/senseval2/pred_babelnet.txt'\n","    pred_lex = '../data/Evaluation_Datasets/senseval2/pred_lex.txt'\n","    pred_domains = '../data/Evaluation_Datasets/senseval2/pred_domains.txt'\n","\n","    predict_babelnet(input_path, output_path, resources_path)\n","    score = score_predict1(pred_babelnet, gold_file, bn2wn_mapping_file)\n","    se2_scores.append(score*100)\n","\n","    predict_lexicographer(input_path, output_path, resources_path)\n","    score = score_predict_lex(pred_lex, gold_file, resources_path)\n","    se2_scores.append(score*100)\n","\n","    predict_wordnet_domains(input_path, output_path, resources_path)\n","    score = score_predict_dom(pred_domains, gold_file, resources_path)\n","    se2_scores.append(score*100)\n","\n","    all_scores.append(se2_scores)\n","    \n","\n","    #########################################################\n","\n","    print(\"\\n\\nPREDICTING FOR SE3...\")\n","    se3_scores = ['SE3_(Dev)']\n","\n","    input_path = '../data/Evaluation_Datasets/senseval3/senseval3.data.xml'\n","    output_path = '../data/Evaluation_Datasets/senseval3'\n","    gold_file =  '../data/Evaluation_Datasets/senseval3/senseval3.gold.key.txt'\n","\n","    pred_babelnet = '../data/Evaluation_Datasets/senseval3/pred_babelnet.txt'\n","    pred_lex = '../data/Evaluation_Datasets/senseval3/pred_lex.txt'\n","    pred_domains = '../data/Evaluation_Datasets/senseval3/pred_domains.txt'\n","\n","    predict_babelnet(input_path, output_path, resources_path)\n","    score = score_predict1(pred_babelnet, gold_file, bn2wn_mapping_file)\n","    se3_scores.append(score*100)\n","\n","    predict_lexicographer(input_path, output_path, resources_path)\n","    score = score_predict_lex(pred_lex, gold_file, resources_path)\n","    se3_scores.append(score*100)\n","\n","    predict_wordnet_domains(input_path, output_path, resources_path)\n","    score = score_predict_dom(pred_domains, gold_file, resources_path)\n","    se3_scores.append(score*100)\n","\n","    all_scores.append(se3_scores)\n","    \n","    \n","    ###########################################################\n","        \n","    with open(score_file, \"w\") as file:\n","        for scores in all_scores:\n","            file.write(\"{},{:.2f},{:.2f},{:.2f}\\n\".format(scores[0], float(scores[1]), float(scores[2]), float(scores[3])))\n","\n","\n","\n","def run_tests2(score_file):\n","    \n","    resources_path = '../resources'\n","    bn2wn_mapping_file = '../resources/babelnet2wordnet.tsv'\n","    \n","    all_scores = []\n","    if (os.path.isfile(score_file)):\n","        with open(score_file, 'r') as file:\n","            for line in file:\n","                all_scores.append(line.split(\",\"))\n","                \n","    \n","    print(\"\\n\\nPREDICTING FOR SE07...\")\n","    se07_scores = ['SE07']\n","    \n","    input_path = '../data/Evaluation_Datasets/semeval2007/semeval2007.data.xml'\n","    output_path = '../data/Evaluation_Datasets/semeval2007'\n","    gold_file =  '../data/Evaluation_Datasets/semeval2007/semeval2007.gold.key.txt'\n","    \n","    pred_babelnet = '../data/Evaluation_Datasets/semeval2007/pred_babelnet.txt'\n","    pred_lex = '../data/Evaluation_Datasets/semeval2007/pred_lex.txt'\n","    pred_domains = '../data/Evaluation_Datasets/semeval2007/pred_domains.txt'\n","\n","    predict_babelnet(input_path, output_path, resources_path)\n","    score = score_predict1(pred_babelnet, gold_file, bn2wn_mapping_file)\n","    se07_scores.append(score*100)\n","    \n","    predict_lexicographer(input_path, output_path, resources_path)\n","    score = score_predict_lex(pred_lex, gold_file, resources_path)\n","    se07_scores.append(score*100)\n","    \n","    predict_wordnet_domains(input_path, output_path, resources_path)\n","    score = score_predict_dom(pred_domains, gold_file, resources_path)\n","    se07_scores.append(score*100)\n","    \n","    all_scores.append(se07_scores)\n","    \n","    #########################################################\n","    \n","    print(\"\\n\\nPREDICTING FOR SE13...\")\n","    se13_scores = ['SE13']\n","    \n","    input_path = '../data/Evaluation_Datasets/semeval2013/semeval2013.data.xml'\n","    output_path = '../data/Evaluation_Datasets/semeval2013'\n","    gold_file =  '../data/Evaluation_Datasets/semeval2013/semeval2013.gold.key.txt'\n","    \n","    pred_babelnet = '../data/Evaluation_Datasets/semeval2013/pred_babelnet.txt'\n","    pred_lex = '../data/Evaluation_Datasets/semeval2013/pred_lex.txt'\n","    pred_domains = '../data/Evaluation_Datasets/semeval2013/pred_domains.txt'\n","\n","    predict_babelnet(input_path, output_path, resources_path)\n","    score = score_predict1(pred_babelnet, gold_file, bn2wn_mapping_file)\n","    se13_scores.append(score*100)\n","    \n","    predict_lexicographer(input_path, output_path, resources_path)\n","    score = score_predict_lex(pred_lex, gold_file, resources_path)\n","    se13_scores.append(score*100)\n","    \n","    predict_wordnet_domains(input_path, output_path, resources_path)\n","    score = score_predict_dom(pred_domains, gold_file, resources_path)\n","    se13_scores.append(score*100)\n","    \n","    all_scores.append(se13_scores)\n","    \n","    #########################################################\n","    \n","    \n","    with open(score_file, \"w\") as file:\n","        for scores in all_scores:\n","            file.write(\"{},{:.2f},{:.2f},{:.2f}\\n\".format(scores[0], float(scores[1]), float(scores[2]), float(scores[3])))\n","            \n","            \n","            \n","    \n","    \n","\n","def run_tests3(score_file):\n","    \n","    resources_path = '../resources'\n","    bn2wn_mapping_file = '../resources/babelnet2wordnet.tsv'\n","    \n","    all_scores = []\n","    if (os.path.isfile(score_file)):\n","        with open(score_file, 'r') as file:\n","            for line in file:\n","                all_scores.append(line.split(\",\"))\n","                \n","    \n","    print(\"\\n\\nPREDICTING FOR SE15...\")\n","    se15_scores = ['SE15']\n","    \n","    input_path = '../data/Evaluation_Datasets/semeval2015/semeval2015.data.xml'\n","    output_path = '../data/Evaluation_Datasets/semeval2015'\n","    gold_file =  '../data/Evaluation_Datasets/semeval2015/semeval2015.gold.key.txt'\n","    \n","    pred_babelnet = '../data/Evaluation_Datasets/semeval2015/pred_babelnet.txt'\n","    pred_lex = '../data/Evaluation_Datasets/semeval2015/pred_lex.txt'\n","    pred_domains = '../data/Evaluation_Datasets/semeval2015/pred_domains.txt'\n","\n","    predict_babelnet(input_path, output_path, resources_path)\n","    score = score_predict1(pred_babelnet, gold_file, bn2wn_mapping_file)\n","    se15_scores.append(score*100)\n","    \n","    predict_lexicographer(input_path, output_path, resources_path)\n","    score = score_predict_lex(pred_lex, gold_file, resources_path)\n","    se15_scores.append(score*100)\n","    \n","    predict_wordnet_domains(input_path, output_path, resources_path)\n","    score = score_predict_dom(pred_domains, gold_file, resources_path)\n","    se15_scores.append(score*100)\n","    \n","    all_scores.append(se15_scores)\n","    \n","    \n","    #####################################################################\n","    \n","    \n","    print(\"\\n\\nPREDICTING FOR ALL...\")\n","    se_ALL_scores = ['ALL']\n","    \n","    input_path = '../data/Evaluation_Datasets/ALL/ALL.data.xml'\n","    output_path = '../data/Evaluation_Datasets/ALL'\n","    gold_file =  '../data/Evaluation_Datasets/ALL/ALL.gold.key.txt'\n","    \n","    pred_babelnet = '../data/Evaluation_Datasets/ALL/pred_babelnet.txt'\n","    pred_lex = '../data/Evaluation_Datasets/ALL/pred_lex.txt'\n","    pred_domains = '../data/Evaluation_Datasets/ALL/pred_domains.txt'\n","\n","    predict_babelnet(input_path, output_path, resources_path)\n","    score = score_predict1(pred_babelnet, gold_file, bn2wn_mapping_file)\n","    se_ALL_scores.append(score*100)\n","    \n","    predict_lexicographer(input_path, output_path, resources_path)\n","    score = score_predict_lex(pred_lex, gold_file, resources_path)\n","    se_ALL_scores.append(score*100)\n","    \n","    predict_wordnet_domains(input_path, output_path, resources_path)\n","    score = score_predict_dom(pred_domains, gold_file, resources_path)\n","    se_ALL_scores.append(score*100)\n","    \n","    all_scores.append(se_ALL_scores)\n","    \n","    \n","    with open(score_file, \"w\") as file:\n","        for scores in all_scores:\n","            file.write(\"{},{:.2f},{:.2f},{:.2f}\\n\".format(scores[0], float(scores[1]), float(scores[2]), float(scores[3])))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NNeyRlWL3gVH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"b1a0a43d-4309-4750-8d76-433d85759f9c","executionInfo":{"status":"ok","timestamp":1568336984490,"user_tz":-120,"elapsed":379882,"user":{"displayName":"Sayo Makinwa","photoUrl":"","userId":"02017936482338602411"}}},"source":["#all_scores_pd = run_tests1(\"../resources/scores.csv\")\n","#all_scores_pd = run_tests2(\"../resources/scores.csv\")\n","all_scores_pd = run_tests3(\"../resources/scores.csv\")"],"execution_count":9,"outputs":[{"output_type":"stream","text":["\n","\n","PREDICTING FOR SE15...\n","LOADING RESOURCES...\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3239: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","PREPARING EVALUATION DATA FOR PREDICTION...\n","138 sentences extracted...\n","Evaluation data extraction completed\n","Predicting (line by line) and writing to file... This may take a little while...\n","100/138 lines done... A little moment more and everything will be done! :)\n","Prediction complete!\n","LOADING RESOURCES...\n","PREPARING EVALUATION DATA FOR PREDICTION...\n","138 sentences extracted...\n","Evaluation data extraction completed\n","Predicting (line by line) and writing to file... This may take a little while...\n","100/138 lines done... A little moment more and everything will be done! :)\n","Prediction complete!\n","LOADING RESOURCES...\n","PREPARING EVALUATION DATA FOR PREDICTION...\n","138 sentences extracted...\n","Evaluation data extraction completed\n","Predicting (line by line) and writing to file... This may take a little while...\n","100/138 lines done... A little moment more and everything will be done! :)\n","Prediction complete!\n","\n","\n","PREDICTING FOR ALL...\n","LOADING RESOURCES...\n","PREPARING EVALUATION DATA FOR PREDICTION...\n","1,173 sentences extracted...\n","Evaluation data extraction completed\n","Predicting (line by line) and writing to file... This may take a little while...\n","100/1173 lines done... A little moment more and everything will be done! :)\n","200/1173 lines done... A little moment more and everything will be done! :)\n","300/1173 lines done... A little moment more and everything will be done! :)\n","400/1173 lines done... A little moment more and everything will be done! :)\n","500/1173 lines done... A little moment more and everything will be done! :)\n","600/1173 lines done... A little moment more and everything will be done! :)\n","700/1173 lines done... A little moment more and everything will be done! :)\n","800/1173 lines done... A little moment more and everything will be done! :)\n","900/1173 lines done... A little moment more and everything will be done! :)\n","1000/1173 lines done... A little moment more and everything will be done! :)\n","1100/1173 lines done... A little moment more and everything will be done! :)\n","Prediction complete!\n","LOADING RESOURCES...\n","PREPARING EVALUATION DATA FOR PREDICTION...\n","1,173 sentences extracted...\n","Evaluation data extraction completed\n","Predicting (line by line) and writing to file... This may take a little while...\n","100/1173 lines done... A little moment more and everything will be done! :)\n","200/1173 lines done... A little moment more and everything will be done! :)\n","300/1173 lines done... A little moment more and everything will be done! :)\n","400/1173 lines done... A little moment more and everything will be done! :)\n","500/1173 lines done... A little moment more and everything will be done! :)\n","600/1173 lines done... A little moment more and everything will be done! :)\n","700/1173 lines done... A little moment more and everything will be done! :)\n","800/1173 lines done... A little moment more and everything will be done! :)\n","900/1173 lines done... A little moment more and everything will be done! :)\n","1000/1173 lines done... A little moment more and everything will be done! :)\n","1100/1173 lines done... A little moment more and everything will be done! :)\n","Prediction complete!\n","LOADING RESOURCES...\n","PREPARING EVALUATION DATA FOR PREDICTION...\n","1,173 sentences extracted...\n","Evaluation data extraction completed\n","Predicting (line by line) and writing to file... This may take a little while...\n","100/1173 lines done... A little moment more and everything will be done! :)\n","200/1173 lines done... A little moment more and everything will be done! :)\n","300/1173 lines done... A little moment more and everything will be done! :)\n","400/1173 lines done... A little moment more and everything will be done! :)\n","500/1173 lines done... A little moment more and everything will be done! :)\n","600/1173 lines done... A little moment more and everything will be done! :)\n","700/1173 lines done... A little moment more and everything will be done! :)\n","800/1173 lines done... A little moment more and everything will be done! :)\n","900/1173 lines done... A little moment more and everything will be done! :)\n","1000/1173 lines done... A little moment more and everything will be done! :)\n","1100/1173 lines done... A little moment more and everything will be done! :)\n","Prediction complete!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Jd7iZT-GYTCa","colab_type":"text"},"source":["##Results"]},{"cell_type":"code","metadata":{"id":"kViXUjmsEydk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":235},"outputId":"185e79dc-6563-47d8-c90b-8e2b0cbcf81e","executionInfo":{"status":"ok","timestamp":1568336984494,"user_tz":-120,"elapsed":65030,"user":{"displayName":"Sayo Makinwa","photoUrl":"","userId":"02017936482338602411"}}},"source":["all_scores_pd = pd.read_csv('../resources/scores.csv', names=['Babelnet', 'Lex', 'Domain'])\n","all_scores_pd"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Babelnet</th>\n","      <th>Lex</th>\n","      <th>Domain</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>SE2</th>\n","      <td>59.25</td>\n","      <td>76.12</td>\n","      <td>87.47</td>\n","    </tr>\n","    <tr>\n","      <th>SE3_(Dev)</th>\n","      <td>60.97</td>\n","      <td>74.59</td>\n","      <td>85.14</td>\n","    </tr>\n","    <tr>\n","      <th>SE07</th>\n","      <td>50.11</td>\n","      <td>65.71</td>\n","      <td>85.49</td>\n","    </tr>\n","    <tr>\n","      <th>SE13</th>\n","      <td>58.27</td>\n","      <td>68.49</td>\n","      <td>74.15</td>\n","    </tr>\n","    <tr>\n","      <th>SE15</th>\n","      <td>52.84</td>\n","      <td>69.37</td>\n","      <td>79.26</td>\n","    </tr>\n","    <tr>\n","      <th>ALL</th>\n","      <td>57.99</td>\n","      <td>72.40</td>\n","      <td>82.57</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["           Babelnet    Lex  Domain\n","SE2           59.25  76.12   87.47\n","SE3_(Dev)     60.97  74.59   85.14\n","SE07          50.11  65.71   85.49\n","SE13          58.27  68.49   74.15\n","SE15          52.84  69.37   79.26\n","ALL           57.99  72.40   82.57"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"PI9o3iYaKjsJ","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}