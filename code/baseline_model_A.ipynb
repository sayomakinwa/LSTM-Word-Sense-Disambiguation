{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"colab":{"name":"baseline_model_A.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"P00PH5WB6Cdn","colab_type":"text"},"source":["## Mount Drive and Imports"]},{"cell_type":"code","metadata":{"id":"NKtsqriiKxYd","colab_type":"code","outputId":"cd733947-bf30-4e07-f471-e1e2576a4df1","executionInfo":{"status":"ok","timestamp":1568221481213,"user_tz":-120,"elapsed":20483,"user":{"displayName":"Sayo Makinwa","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB01ONwXql1XhFq6AK_2PjHkUfJA3VDpx1AVsLimg=s64","userId":"02832581640686072550"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nRTrAorEKyu5","colab_type":"code","outputId":"c9aa533b-ac05-41b6-c093-bee968863afe","executionInfo":{"status":"ok","timestamp":1568221481450,"user_tz":-120,"elapsed":672,"user":{"displayName":"Sayo Makinwa","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB01ONwXql1XhFq6AK_2PjHkUfJA3VDpx1AVsLimg=s64","userId":"02832581640686072550"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%cd /content/drive/My Drive/Colab Notebooks/nlp_hw3/code"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks/nlp_hw3/code\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OxdkmYCxEybs","colab_type":"code","colab":{}},"source":["import os\n","import numpy as np\n","from typing import Tuple, List, Dict\n","\n","import tensorflow as tf\n","import tensorflow.keras as K\n","from tensorflow.keras.layers import *\n","from tensorflow.keras.models import load_model\n","\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","from tensorflow.keras.callbacks import TensorBoard\n","from tensorflow.python.eager import context\n","import json\n","import pandas as pd\n","\n","#from numpy import array\n","#from numpy import argmax\n","#from sklearn.preprocessing import LabelEncoder\n","#from sklearn.preprocessing import OneHotEncoder\n","\n","import my_utils as utils\n","import corpora"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TYLN4Lf-6RO_","colab_type":"text"},"source":["## Preprocessing"]},{"cell_type":"code","metadata":{"id":"042xGwFcEyb1","colab_type":"code","colab":{}},"source":["def load_train_dataset(input_path: str, y_path: str) -> Tuple[List[str], List[str]]:\n","    \"\"\"\n","    :param input_path; Path to the input dataset\n","    :param label_path; Path to the file containing the corresponding labels for the input dataset\n","    :return sentences; List of sentences in input_file\n","    :return labels; List of corresponding word segment codes in label_path. Same len as sentences\n","    \"\"\"\n","    sentences = []\n","    k = 0\n","    with open(input_path, \"r\", encoding=\"utf-8-sig\") as file:\n","        for line in file:\n","            k += 1\n","            sentences.append(line.strip())\n","#             if (k >= 4000):\n","#                 break\n","\n","    y = []\n","    k = 0\n","    with open(y_path, \"r\", encoding=\"utf-8-sig\") as file:\n","        for line in file:\n","            k += 1\n","            y.append(line.strip())\n","#             if (k >= 4000):\n","#                 break\n","\n","    return sentences, y\n","\n","\n","def make_X_vocab(sentences: List[str]) -> Dict[str, int]:\n","    '''\n","    :param sentences; List of input sentences from the dataset\n","    :return unigrams_vocab; Dictionary from unigram to int\n","    :return bigrams_vocab; Dictionary from bigram to int\n","    '''\n","    vocab = {\"UNK\": 0}\n","\n","    for sentence in sentences:\n","        for word in sentence.split():\n","            if word not in vocab:\n","                vocab[word] = len(vocab)\n","    \n","    return vocab\n","\n","\n","def make_Y_vocab(y: List[str]) -> Dict[str, int]:\n","    \"\"\"\n","    :param labels; List of label codes\n","    :return labels_vocab; Dictionary from label code to int \n","    \"\"\"\n","    y_vocab = {\"UNK\": 0}\n","    \n","    for y_line in y:\n","        for y_word in y_line.split():\n","            if y_word not in y_vocab:\n","                y_vocab[y_word] = len(y_vocab)\n","                \n","    return y_vocab\n","\n","def make_Y(output: List[str], output_vocab: Dict[str, int]) -> np.ndarray:\n","    \"\"\"\n","    :param labels; List of word segment codes, line by line\n","    :param labels_vocab; Label codes vocab\n","    :return y; Vector of label code indices\n","    \"\"\"\n","    y = []\n","    for output_line in output:\n","        y_temp = []\n","        for single_output in output_line.split():\n","            if single_output in output_vocab:\n","                y_temp.append( output_vocab[single_output])\n","            else:\n","                y_temp.append( output_vocab[\"UNK\"])\n","        y.append(np.array(y_temp))\n","    \n","    return np.array(y)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6kiV4oP3-r-Y","colab_type":"code","colab":{}},"source":["train_data_path = '../data/Training_Corpora/semcor'\n","test_data_path = '../data/Evaluation_Datasets/senseval3'\n","\n","#Parse training data\n","corpora_xml_path = train_data_path + '/semcor.data.xml'\n","gold_mapping_path = train_data_path + '/semcor.gold.key.txt'\n","resources_path = '../resources/'\n","\n","corpora.extract_training_data(corpora_xml_path, gold_mapping_path, resources_path, train_data_path)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eBT3apRV_Bdn","colab_type":"code","colab":{}},"source":["#Parse validation data\n","corpora_xml_path = test_data_path + '/senseval3.data.xml'\n","gold_mapping_path = test_data_path + '/senseval3.gold.key.txt'\n","resources_path = '../resources/'\n","\n","corpora.extract_training_data(corpora_xml_path, gold_mapping_path, resources_path, test_data_path)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yVV_9dxlEyb4","colab_type":"code","colab":{}},"source":["sentences, y = load_train_dataset(train_data_path+\"/trainX.txt\", train_data_path+\"/trainy.txt\")\n","test_sentences, test_y = load_train_dataset(test_data_path+\"/trainX.txt\", test_data_path+\"/trainy.txt\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-EJ1fHSzEyb7","colab_type":"code","outputId":"8ea6b5b0-4264-4488-b56c-0b7f49ade095","executionInfo":{"status":"ok","timestamp":1568200789352,"user_tz":-120,"elapsed":630,"user":{"displayName":"Sayo Makinwa","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB01ONwXql1XhFq6AK_2PjHkUfJA3VDpx1AVsLimg=s64","userId":"02832581640686072550"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["print(len(sentences))\n","print(sentences[0])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["37168\n","How long has it been since you reviewed the objectives of your benefit and service program\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"t1fD1tgDEycC","colab_type":"code","outputId":"ca04a65f-215a-46da-e775-6286f7884d34","executionInfo":{"status":"ok","timestamp":1568200790463,"user_tz":-120,"elapsed":1227,"user":{"displayName":"Sayo Makinwa","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB01ONwXql1XhFq6AK_2PjHkUfJA3VDpx1AVsLimg=s64","userId":"02832581640686072550"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["print(len(y))\n","print(y[0])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["37168\n","how bn:00106124a have it bn:00083181v since you bn:00092618v the bn:00002179n of you bn:00009904n and bn:00070654n bn:00064646n\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HgCzdmbLEycF","colab_type":"code","outputId":"2fccd9ea-74e5-4292-be52-2c4764311e90","executionInfo":{"status":"ok","timestamp":1568200791858,"user_tz":-120,"elapsed":897,"user":{"displayName":"Sayo Makinwa","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB01ONwXql1XhFq6AK_2PjHkUfJA3VDpx1AVsLimg=s64","userId":"02832581640686072550"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["vocab = make_X_vocab(sentences + test_sentences)\n","print(len(vocab))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["48989\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LPRR9TA7EycH","colab_type":"code","outputId":"dd8ccc23-3d9f-4c78-ea47-8836d57f4ea4","executionInfo":{"status":"ok","timestamp":1568200794812,"user_tz":-120,"elapsed":1125,"user":{"displayName":"Sayo Makinwa","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB01ONwXql1XhFq6AK_2PjHkUfJA3VDpx1AVsLimg=s64","userId":"02832581640686072550"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["y_vocab = make_Y_vocab(y + test_y)\n","print(len(y_vocab))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["49224\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iD6TeqJREycR","colab_type":"code","colab":{}},"source":["X = utils.make_X(sentences, vocab)\n","y_array = make_Y(y, y_vocab)\n","\n","X_test = utils.make_X(test_sentences, vocab)\n","y_test_array = make_Y(test_y, y_vocab)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DF5m68nmEycT","colab_type":"code","outputId":"5d3fd940-b8c8-4ab3-89a6-469d5c2fbed3","executionInfo":{"status":"ok","timestamp":1568200898491,"user_tz":-120,"elapsed":925,"user":{"displayName":"Sayo Makinwa","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB01ONwXql1XhFq6AK_2PjHkUfJA3VDpx1AVsLimg=s64","userId":"02832581640686072550"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["print(X.shape)\n","print(y_array.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(37168,)\n","(37168,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"j7fg-usHEycZ","colab_type":"code","colab":{}},"source":["train_x = pad_sequences(X, truncating='pre', padding='post', maxlen=30)\n","train_y = pad_sequences(y_array, truncating='pre', padding='post', maxlen=30)\n","\n","dev_x = pad_sequences(X_test, truncating='pre', padding='post', maxlen=30)\n","dev_y = pad_sequences(y_test_array, truncating='pre', padding='post', maxlen=30)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NmXFcuk4Eycc","colab_type":"code","outputId":"b3ba4cef-5834-444a-8494-683006b35b47","executionInfo":{"status":"ok","timestamp":1568200906611,"user_tz":-120,"elapsed":927,"user":{"displayName":"Sayo Makinwa","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB01ONwXql1XhFq6AK_2PjHkUfJA3VDpx1AVsLimg=s64","userId":"02832581640686072550"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["print(train_x.shape)\n","print(train_y.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(37168, 30)\n","(37168, 30)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YIojgrqREycg","colab_type":"code","outputId":"580fcad8-74f6-421f-902b-e05641442fe8","executionInfo":{"status":"ok","timestamp":1568200994152,"user_tz":-120,"elapsed":909,"user":{"displayName":"Sayo Makinwa","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB01ONwXql1XhFq6AK_2PjHkUfJA3VDpx1AVsLimg=s64","userId":"02832581640686072550"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["train_y = train_y.reshape((*train_y.shape, 1))\n","dev_y = dev_y.reshape((*dev_y.shape, 1))\n","print(train_y.shape)\n","print(dev_y.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(37168, 30, 1)\n","(352, 30, 1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ABjb_KPbEyc1","colab_type":"code","outputId":"88f7fb9c-e4ed-478a-e6db-f3de82ee5a3f","executionInfo":{"status":"ok","timestamp":1568201043897,"user_tz":-120,"elapsed":748,"user":{"displayName":"Sayo Makinwa","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB01ONwXql1XhFq6AK_2PjHkUfJA3VDpx1AVsLimg=s64","userId":"02832581640686072550"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["print(train_x.shape)\n","print(train_y.shape)\n","print(dev_x.shape)\n","print(dev_y.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(37168, 30)\n","(37168, 30, 1)\n","(352, 30)\n","(352, 30, 1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IwhKREoe_vx_","colab_type":"text"},"source":["## Building the model"]},{"cell_type":"code","metadata":{"id":"gd-bBhRDEyc-","colab_type":"code","colab":{}},"source":["vocab_size = len(vocab)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9uF52K_8EydA","colab_type":"code","colab":{}},"source":["#This class helps with logging\n","class TrainValTensorBoard(TensorBoard):\n","    def __init__(self, log_dir='./logs', **kwargs):\n","        self.val_log_dir = os.path.join(log_dir, 'model_A/validation')\n","        training_log_dir = os.path.join(log_dir, 'model_A/training')\n","        super(TrainValTensorBoard, self).__init__(training_log_dir, **kwargs)\n","\n","    def set_model(self, model):\n","        if context.executing_eagerly():\n","            self.val_writer = tf.contrib.summary.create_file_writer(self.val_log_dir)\n","        else:\n","            self.val_writer = tf.summary.FileWriter(self.val_log_dir)\n","        super(TrainValTensorBoard, self).set_model(model)\n","\n","    def _write_custom_summaries(self, step, logs=None):\n","        logs = logs or {}\n","        val_logs = {k.replace('val_', ''): v for k, v in logs.items() if 'val_' in k}\n","        if context.executing_eagerly():\n","            with self.val_writer.as_default(), tf.contrib.summary.always_record_summaries():\n","                for name, value in val_logs.items():\n","                    tf.contrib.summary.scalar(name, value.item(), step=step)\n","        else:\n","            for name, value in val_logs.items():\n","                summary = tf.Summary()\n","                summary_value = summary.value.add()\n","                summary_value.simple_value = value.item()\n","                summary_value.tag = name\n","                self.val_writer.add_summary(summary, step)\n","        self.val_writer.flush()\n","\n","        logs = {k: v for k, v in logs.items() if not 'val_' in k}\n","        super(TrainValTensorBoard, self)._write_custom_summaries(step, logs)\n","\n","    def on_train_end(self, logs=None):\n","        super(TrainValTensorBoard, self).on_train_end(logs)\n","        self.val_writer.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fXhtH-HhEydE","colab_type":"code","colab":{}},"source":["#Please take note that most of this part was extracted from class exercises, with some additions\n","\n","def create_keras_model(vocab_size, y_size, embedding_size=128, hidden_size=512):\n","    print(\"Creating KERAS model\")\n","    \n","    model = K.models.Sequential()\n","    model.add(Embedding(vocab_size, embedding_size, mask_zero=True))\n","    \n","    model.add(Bidirectional(LSTM(hidden_size, dropout=0.3, recurrent_dropout=0.3, return_sequences=True), merge_mode='concat'))\n","    model.add(Bidirectional(LSTM(hidden_size, dropout=0.3, recurrent_dropout=0.3, return_sequences=True), merge_mode='concat'))\n","    \n","    model.add(TimeDistributed(Dense(y_size, activation='softmax')))\n","    optimizer = K.optimizers.Adam()\n","    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['acc'])\n","\n","    return model\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PMqneG5WEydI","colab_type":"code","colab":{}},"source":["resource_path = \"../resources/\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"872HdkkrTJi9","colab_type":"text"},"source":["## Training"]},{"cell_type":"code","metadata":{"id":"pFd4-nloEydL","colab_type":"code","outputId":"bace702c-bbc2-4ade-c5a3-f3ea561abbfe","executionInfo":{"status":"ok","timestamp":1568214089923,"user_tz":-120,"elapsed":13029528,"user":{"displayName":"Sayo Makinwa","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB01ONwXql1XhFq6AK_2PjHkUfJA3VDpx1AVsLimg=s64","userId":"02832581640686072550"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["batch_size = 64\n","epochs = 20\n","model_name = resource_path+\"model.hdf5\"\n","\n","#checks if the FINAL model was saved and loads it instead of creating a new one\n","if os.path.exists(model_name):\n","    model = load_model(model_name)\n","    print(\"Using a pre-saved model\")\n","    model.summary()\n","    \n","else:\n","    model = create_keras_model(vocab_size, len(y_vocab))\n","    print(\"Training a new model\")\n","    model.summary()\n","    \n","    filepath = resource_path+\"models/model_A.hdf5\"\n","    checkpoint = K.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n","    #callbacks_list = [checkpoint]\n","    tb_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\", histogram_freq=0, write_graph=False, write_images=True)\n","    callbacks_list = [TrainValTensorBoard(write_graph=False), checkpoint]\n","    \n","    print(\"\\nStarting training...\")\n","    model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size,\n","              shuffle=True, validation_data=(dev_x, dev_y), callbacks=callbacks_list) \n","    print(\"Training complete.\\n\")\n","    \n","    #Save the FINAL model for later reuse\n","    model.save(model_name)\n","    print(\"Trained model saved for later use\")\n","\n","    print(\"\\nEvaluating test...\")\n","    loss_acc = model.evaluate(dev_x, dev_y, verbose=0)\n","    print(\"Test data: loss = %0.6f  accuracy = %0.2f%% \" % (loss_acc[0], loss_acc[1]*100))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Creating KERAS model\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py:3794: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py:3794: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n"],"name":"stderr"},{"output_type":"stream","text":["Training a new model\n","Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding (Embedding)        (None, None, 128)         6270592   \n","_________________________________________________________________\n","bidirectional (Bidirectional (None, None, 1024)        2625536   \n","_________________________________________________________________\n","bidirectional_1 (Bidirection (None, None, 1024)        6295552   \n","_________________________________________________________________\n","time_distributed (TimeDistri (None, None, 49224)       50454600  \n","=================================================================\n","Total params: 65,646,280\n","Trainable params: 65,646,280\n","Non-trainable params: 0\n","_________________________________________________________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Starting training...\n","Train on 37168 samples, validate on 352 samples\n","Epoch 1/20\n","37168/37168 [==============================] - 652s 18ms/sample - loss: 3.8682 - acc: 0.1276 - val_loss: 2.5288 - val_acc: 0.3184\n","Epoch 2/20\n","37168/37168 [==============================] - 651s 18ms/sample - loss: 2.7528 - acc: 0.3789 - val_loss: 2.1508 - val_acc: 0.4205\n","Epoch 3/20\n","37168/37168 [==============================] - 652s 18ms/sample - loss: 2.2793 - acc: 0.4638 - val_loss: 1.9494 - val_acc: 0.4815\n","Epoch 4/20\n","37168/37168 [==============================] - 652s 18ms/sample - loss: 1.9257 - acc: 0.5138 - val_loss: 1.8087 - val_acc: 0.5053\n","Epoch 5/20\n","37168/37168 [==============================] - 652s 18ms/sample - loss: 1.6739 - acc: 0.5473 - val_loss: 1.7115 - val_acc: 0.5249\n","Epoch 6/20\n","37168/37168 [==============================] - 651s 18ms/sample - loss: 1.4770 - acc: 0.5738 - val_loss: 1.6540 - val_acc: 0.5335\n","Epoch 7/20\n","37168/37168 [==============================] - 651s 18ms/sample - loss: 1.2989 - acc: 0.6013 - val_loss: 1.5821 - val_acc: 0.5473\n","Epoch 8/20\n","37168/37168 [==============================] - 650s 17ms/sample - loss: 1.1291 - acc: 0.6298 - val_loss: 1.5385 - val_acc: 0.5592\n","Epoch 9/20\n","37168/37168 [==============================] - 650s 17ms/sample - loss: 0.9864 - acc: 0.6546 - val_loss: 1.5200 - val_acc: 0.5586\n","Epoch 10/20\n","37168/37168 [==============================] - 652s 18ms/sample - loss: 0.8561 - acc: 0.6815 - val_loss: 1.4886 - val_acc: 0.5641\n","Epoch 11/20\n","37168/37168 [==============================] - 651s 18ms/sample - loss: 0.7380 - acc: 0.7107 - val_loss: 1.4805 - val_acc: 0.5669\n","Epoch 12/20\n","37168/37168 [==============================] - 650s 17ms/sample - loss: 0.6326 - acc: 0.7418 - val_loss: 1.4510 - val_acc: 0.5810\n","Epoch 13/20\n","37168/37168 [==============================] - 651s 18ms/sample - loss: 0.5415 - acc: 0.7711 - val_loss: 1.4386 - val_acc: 0.5870\n","Epoch 14/20\n","37168/37168 [==============================] - 647s 17ms/sample - loss: 0.4656 - acc: 0.7963 - val_loss: 1.4377 - val_acc: 0.5828\n","Epoch 15/20\n","37168/37168 [==============================] - 650s 17ms/sample - loss: 0.4039 - acc: 0.8183 - val_loss: 1.4336 - val_acc: 0.5885\n","Epoch 16/20\n","37168/37168 [==============================] - 649s 17ms/sample - loss: 0.3556 - acc: 0.8355 - val_loss: 1.4335 - val_acc: 0.5907\n","Epoch 17/20\n","37168/37168 [==============================] - 649s 17ms/sample - loss: 0.3158 - acc: 0.8497 - val_loss: 1.4372 - val_acc: 0.5889\n","Epoch 18/20\n","37168/37168 [==============================] - 652s 18ms/sample - loss: 0.2846 - acc: 0.8613 - val_loss: 1.4337 - val_acc: 0.5923\n","Epoch 19/20\n","37168/37168 [==============================] - 652s 18ms/sample - loss: 0.2582 - acc: 0.8711 - val_loss: 1.4389 - val_acc: 0.5929\n","Epoch 20/20\n","37168/37168 [==============================] - 647s 17ms/sample - loss: 0.2368 - acc: 0.8795 - val_loss: 1.4306 - val_acc: 0.5947\n","Training complete.\n","\n","Trained model saved for later use\n","\n","Evaluating test...\n","Test data: loss = 1.430603  accuracy = 59.47% \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ztDgLLxJEydO","colab_type":"code","colab":{}},"source":["del model\n","\n","#Writing the vocabularies to file\n","\n","with open(resource_path+\"x_vocab.txt\", \"w\") as file:\n","    file.write(json.dumps(vocab))\n","    \n","with open(resource_path+\"y_vocab.txt\", \"w\") as file:\n","    file.write(json.dumps(y_vocab))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"viA96Aw5TOF8","colab_type":"text"},"source":["## Running Predictions"]},{"cell_type":"code","metadata":{"id":"FqZyKhGwEydR","colab_type":"code","colab":{}},"source":["from predict import *\n","from score import *"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"T4DYUwkGnSt-","colab_type":"code","outputId":"26f17b8b-52eb-492c-dbe7-37ddda996354","executionInfo":{"status":"ok","timestamp":1568221497493,"user_tz":-120,"elapsed":1159,"user":{"displayName":"Sayo Makinwa","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB01ONwXql1XhFq6AK_2PjHkUfJA3VDpx1AVsLimg=s64","userId":"02832581640686072550"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["import nltk\n","nltk.download(\"wordnet\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"N-zJujtZEydT","colab_type":"code","colab":{}},"source":["def run_tests1(score_file):\n","    \n","    resources_path = '../resources'\n","    bn2wn_mapping_file = '../resources/babelnet2wordnet.tsv'\n","    \n","    all_scores = []\n","    if (os.path.isfile(score_file)):\n","        with open(score_file, 'r') as file:\n","            for line in file:\n","                all_scores.append(line.split(\",\"))\n","        \n","    print(\"PREDICTING FOR SE2...\")\n","    se2_scores = ['SE2']\n","\n","    input_path = '../data/Evaluation_Datasets/senseval2/senseval2.data.xml'\n","    output_path = '../data/Evaluation_Datasets/senseval2'\n","    gold_file =  '../data/Evaluation_Datasets/senseval2/senseval2.gold.key.txt'\n","\n","    pred_babelnet = '../data/Evaluation_Datasets/senseval2/pred_babelnet.txt'\n","    pred_lex = '../data/Evaluation_Datasets/senseval2/pred_lex.txt'\n","    pred_domains = '../data/Evaluation_Datasets/senseval2/pred_domains.txt'\n","\n","    predict_babelnet(input_path, output_path, resources_path)\n","    score = score_predict1(pred_babelnet, gold_file, bn2wn_mapping_file)\n","    se2_scores.append(score*100)\n","\n","    predict_lexicographer(input_path, output_path, resources_path)\n","    score = score_predict_lex(pred_lex, gold_file, resources_path)\n","    se2_scores.append(score*100)\n","\n","    predict_wordnet_domains(input_path, output_path, resources_path)\n","    score = score_predict_dom(pred_domains, gold_file, resources_path)\n","    se2_scores.append(score*100)\n","\n","    all_scores.append(se2_scores)\n","    \n","\n","    #########################################################\n","\n","    print(\"\\n\\nPREDICTING FOR SE3...\")\n","    se3_scores = ['SE3_(Dev)']\n","\n","    input_path = '../data/Evaluation_Datasets/senseval3/senseval3.data.xml'\n","    output_path = '../data/Evaluation_Datasets/senseval3'\n","    gold_file =  '../data/Evaluation_Datasets/senseval3/senseval3.gold.key.txt'\n","\n","    pred_babelnet = '../data/Evaluation_Datasets/senseval3/pred_babelnet.txt'\n","    pred_lex = '../data/Evaluation_Datasets/senseval3/pred_lex.txt'\n","    pred_domains = '../data/Evaluation_Datasets/senseval3/pred_domains.txt'\n","\n","    predict_babelnet(input_path, output_path, resources_path)\n","    score = score_predict1(pred_babelnet, gold_file, bn2wn_mapping_file)\n","    se3_scores.append(score*100)\n","\n","    predict_lexicographer(input_path, output_path, resources_path)\n","    score = score_predict_lex(pred_lex, gold_file, resources_path)\n","    se3_scores.append(score*100)\n","\n","    predict_wordnet_domains(input_path, output_path, resources_path)\n","    score = score_predict_dom(pred_domains, gold_file, resources_path)\n","    se3_scores.append(score*100)\n","\n","    all_scores.append(se3_scores)\n","    \n","    \n","    ###########################################################\n","        \n","    with open(score_file, \"w\") as file:\n","        for scores in all_scores:\n","            file.write(\"{},{:.2f},{:.2f},{:.2f}\\n\".format(scores[0], float(scores[1]), float(scores[2]), float(scores[3])))\n","\n","\n","\n","def run_tests2(score_file):\n","    \n","    resources_path = '../resources'\n","    bn2wn_mapping_file = '../resources/babelnet2wordnet.tsv'\n","    \n","    all_scores = []\n","    if (os.path.isfile(score_file)):\n","        with open(score_file, 'r') as file:\n","            for line in file:\n","                all_scores.append(line.split(\",\"))\n","                \n","    \n","    print(\"\\n\\nPREDICTING FOR SE07...\")\n","    se07_scores = ['SE07']\n","    \n","    input_path = '../data/Evaluation_Datasets/semeval2007/semeval2007.data.xml'\n","    output_path = '../data/Evaluation_Datasets/semeval2007'\n","    gold_file =  '../data/Evaluation_Datasets/semeval2007/semeval2007.gold.key.txt'\n","    \n","    pred_babelnet = '../data/Evaluation_Datasets/semeval2007/pred_babelnet.txt'\n","    pred_lex = '../data/Evaluation_Datasets/semeval2007/pred_lex.txt'\n","    pred_domains = '../data/Evaluation_Datasets/semeval2007/pred_domains.txt'\n","\n","    predict_babelnet(input_path, output_path, resources_path)\n","    score = score_predict1(pred_babelnet, gold_file, bn2wn_mapping_file)\n","    se07_scores.append(score*100)\n","    \n","    predict_lexicographer(input_path, output_path, resources_path)\n","    score = score_predict_lex(pred_lex, gold_file, resources_path)\n","    se07_scores.append(score*100)\n","    \n","    predict_wordnet_domains(input_path, output_path, resources_path)\n","    score = score_predict_dom(pred_domains, gold_file, resources_path)\n","    se07_scores.append(score*100)\n","    \n","    all_scores.append(se07_scores)\n","    \n","    #########################################################\n","    \n","    print(\"\\n\\nPREDICTING FOR SE13...\")\n","    se13_scores = ['SE13']\n","    \n","    input_path = '../data/Evaluation_Datasets/semeval2013/semeval2013.data.xml'\n","    output_path = '../data/Evaluation_Datasets/semeval2013'\n","    gold_file =  '../data/Evaluation_Datasets/semeval2013/semeval2013.gold.key.txt'\n","    \n","    pred_babelnet = '../data/Evaluation_Datasets/semeval2013/pred_babelnet.txt'\n","    pred_lex = '../data/Evaluation_Datasets/semeval2013/pred_lex.txt'\n","    pred_domains = '../data/Evaluation_Datasets/semeval2013/pred_domains.txt'\n","\n","    predict_babelnet(input_path, output_path, resources_path)\n","    score = score_predict1(pred_babelnet, gold_file, bn2wn_mapping_file)\n","    se13_scores.append(score*100)\n","    \n","    predict_lexicographer(input_path, output_path, resources_path)\n","    score = score_predict_lex(pred_lex, gold_file, resources_path)\n","    se13_scores.append(score*100)\n","    \n","    predict_wordnet_domains(input_path, output_path, resources_path)\n","    score = score_predict_dom(pred_domains, gold_file, resources_path)\n","    se13_scores.append(score*100)\n","    \n","    all_scores.append(se13_scores)\n","    \n","    #########################################################\n","    \n","    \n","    with open(score_file, \"w\") as file:\n","        for scores in all_scores:\n","            file.write(\"{},{:.2f},{:.2f},{:.2f}\\n\".format(scores[0], float(scores[1]), float(scores[2]), float(scores[3])))\n","            \n","            \n","            \n","    \n","    \n","\n","def run_tests3(score_file):\n","    \n","    resources_path = '../resources'\n","    bn2wn_mapping_file = '../resources/babelnet2wordnet.tsv'\n","    \n","    all_scores = []\n","    if (os.path.isfile(score_file)):\n","        with open(score_file, 'r') as file:\n","            for line in file:\n","                all_scores.append(line.split(\",\"))\n","                \n","    \n","    print(\"\\n\\nPREDICTING FOR SE15...\")\n","    se15_scores = ['SE15']\n","    \n","    input_path = '../data/Evaluation_Datasets/semeval2015/semeval2015.data.xml'\n","    output_path = '../data/Evaluation_Datasets/semeval2015'\n","    gold_file =  '../data/Evaluation_Datasets/semeval2015/semeval2015.gold.key.txt'\n","    \n","    pred_babelnet = '../data/Evaluation_Datasets/semeval2015/pred_babelnet.txt'\n","    pred_lex = '../data/Evaluation_Datasets/semeval2015/pred_lex.txt'\n","    pred_domains = '../data/Evaluation_Datasets/semeval2015/pred_domains.txt'\n","\n","    predict_babelnet(input_path, output_path, resources_path)\n","    score = score_predict1(pred_babelnet, gold_file, bn2wn_mapping_file)\n","    se15_scores.append(score*100)\n","    \n","    predict_lexicographer(input_path, output_path, resources_path)\n","    score = score_predict_lex(pred_lex, gold_file, resources_path)\n","    se15_scores.append(score*100)\n","    \n","    predict_wordnet_domains(input_path, output_path, resources_path)\n","    score = score_predict_dom(pred_domains, gold_file, resources_path)\n","    se15_scores.append(score*100)\n","    \n","    all_scores.append(se15_scores)\n","    \n","    \n","    #####################################################################\n","    \n","    \n","    print(\"\\n\\nPREDICTING FOR ALL...\")\n","    se_ALL_scores = ['ALL']\n","    \n","    input_path = '../data/Evaluation_Datasets/ALL/ALL.data.xml'\n","    output_path = '../data/Evaluation_Datasets/ALL'\n","    gold_file =  '../data/Evaluation_Datasets/ALL/ALL.gold.key.txt'\n","    \n","    pred_babelnet = '../data/Evaluation_Datasets/ALL/pred_babelnet.txt'\n","    pred_lex = '../data/Evaluation_Datasets/ALL/pred_lex.txt'\n","    pred_domains = '../data/Evaluation_Datasets/ALL/pred_domains.txt'\n","\n","    predict_babelnet(input_path, output_path, resources_path)\n","    score = score_predict1(pred_babelnet, gold_file, bn2wn_mapping_file)\n","    se_ALL_scores.append(score*100)\n","    \n","    predict_lexicographer(input_path, output_path, resources_path)\n","    score = score_predict_lex(pred_lex, gold_file, resources_path)\n","    se_ALL_scores.append(score*100)\n","    \n","    predict_wordnet_domains(input_path, output_path, resources_path)\n","    score = score_predict_dom(pred_domains, gold_file, resources_path)\n","    se_ALL_scores.append(score*100)\n","    \n","    all_scores.append(se_ALL_scores)\n","    \n","    \n","    with open(score_file, \"w\") as file:\n","        for scores in all_scores:\n","            file.write(\"{},{:.2f},{:.2f},{:.2f}\\n\".format(scores[0], float(scores[1]), float(scores[2]), float(scores[3])))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kViXUjmsEydk","colab_type":"code","outputId":"bfd618fa-21ad-4cce-fc6a-ee9d4c3e0a4d","executionInfo":{"status":"ok","timestamp":1568222709727,"user_tz":-120,"elapsed":36652,"user":{"displayName":"Sayo Makinwa","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB01ONwXql1XhFq6AK_2PjHkUfJA3VDpx1AVsLimg=s64","userId":"02832581640686072550"}},"colab":{"base_uri":"https://localhost:8080/","height":289}},"source":["utils.all_scores_pd = run_tests1(\"../resources/scores.csv\")\n","utils.all_scores_pd = run_tests2(\"../resources/scores.csv\")\n","utils.all_scores_pd = run_tests3(\"../resources/scores.csv\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["PREDICTING FOR SE2...\n","Scoring 0/2,282 instances...\n","\n","PREDICTING FOR SE3...\n","\n","\n","PREDICTING FOR SE07...\n","Scoring 0/455 instances...\n","\n","PREDICTING FOR SE13...\n","Scoring 0/1,644 instances...\n","\n","PREDICTING FOR SE15...\n","Scoring 0/1,022 instances...\n","\n","PREDICTING FOR ALL...\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UCRLDwd0SVtX","colab_type":"text"},"source":["## Results"]},{"cell_type":"code","metadata":{"id":"Uzvkm0_WtRyd","colab_type":"code","outputId":"c26296c6-b41b-4f3f-babf-43700c693090","executionInfo":{"status":"ok","timestamp":1568222750990,"user_tz":-120,"elapsed":564,"user":{"displayName":"Sayo Makinwa","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB01ONwXql1XhFq6AK_2PjHkUfJA3VDpx1AVsLimg=s64","userId":"02832581640686072550"}},"colab":{"base_uri":"https://localhost:8080/","height":235}},"source":["import pandas as pd\n","\n","all_scores_pd = pd.read_csv('../resources/scores.csv', names=['Babelnet', 'Lex', 'Domain'])\n","all_scores_pd"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Babelnet</th>\n","      <th>Lex</th>\n","      <th>Domain</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>SE2</th>\n","      <td>61.74</td>\n","      <td>77.43</td>\n","      <td>87.95</td>\n","    </tr>\n","    <tr>\n","      <th>SE3_(Dev)</th>\n","      <td>63.73</td>\n","      <td>78.11</td>\n","      <td>87.08</td>\n","    </tr>\n","    <tr>\n","      <th>SE07</th>\n","      <td>56.04</td>\n","      <td>70.33</td>\n","      <td>86.81</td>\n","    </tr>\n","    <tr>\n","      <th>SE13</th>\n","      <td>59.49</td>\n","      <td>70.92</td>\n","      <td>74.88</td>\n","    </tr>\n","    <tr>\n","      <th>SE15</th>\n","      <td>56.65</td>\n","      <td>72.70</td>\n","      <td>81.31</td>\n","    </tr>\n","    <tr>\n","      <th>ALL</th>\n","      <td>60.66</td>\n","      <td>75.02</td>\n","      <td>83.76</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["           Babelnet    Lex  Domain\n","SE2           61.74  77.43   87.95\n","SE3_(Dev)     63.73  78.11   87.08\n","SE07          56.04  70.33   86.81\n","SE13          59.49  70.92   74.88\n","SE15          56.65  72.70   81.31\n","ALL           60.66  75.02   83.76"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"320gADzoUcaT","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}